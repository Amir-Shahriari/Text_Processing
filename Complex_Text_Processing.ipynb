{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FAkdkebwAXO"
   },
   "source": [
    "# Overall Task Review\n",
    "\n",
    "I will work on a task of \"query-focused summarisation\" on medical questions where the goal is, given a medical question and a list of sentences extracted from relevant medical publications, to determine which of these sentences from the list can be used as part of the answer to the question.\n",
    "\n",
    "I will use data that has been derived from the **BioASQ challenge** (http://www.bioasq.org/), after some data manipulation to make it easier to process for this practice. The BioASQ challenge organises several \"shared tasks\", including a task on biomedical semantic question answering which we are using here. The data are in the file `bioasq10_labelled.csv`, which is part of the zip file provided. Each row of the file has a question, a sentence text, and a label that indicates whether the sentence text is part of the answer to the question (1) or not (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cjAFYJIwAXO"
   },
   "source": [
    "# Data Review\n",
    "\n",
    "The following code uses pandas to store the file `bioasq10_labelled.csv` in a data frame and show the first rows of data. For this code to run, first you need to unzip the file `data.zip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKRP5VWmwAXP"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Specify the path to the zip file\n",
    "zip_file_path = 'data.zip'\n",
    "\n",
    "# Specify the directory where you want to extract the contents\n",
    "extracted_dir = 'data/'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(extracted_dir):\n",
    "    os.makedirs(extracted_dir)\n",
    "\n",
    "# Extract the contents of the zip file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huIA0duZwAXP",
    "outputId": "f4bdb4b0-1bb2-41ba-f54e-1d8ae029f212"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>sentid</th>\n",
       "      <th>question</th>\n",
       "      <th>sentence text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>Hirschsprung disease (HSCR) is a multifactoria...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>In this study, we review the identification of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>The majority of the identified genes are relat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>The non-Mendelian inheritance of sporadic non-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>Coding sequence mutations in e.g.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qid  sentid                                           question  \\\n",
       "0    0       0  Is Hirschsprung disease a mendelian or a multi...   \n",
       "1    0       1  Is Hirschsprung disease a mendelian or a multi...   \n",
       "2    0       2  Is Hirschsprung disease a mendelian or a multi...   \n",
       "3    0       3  Is Hirschsprung disease a mendelian or a multi...   \n",
       "4    0       4  Is Hirschsprung disease a mendelian or a multi...   \n",
       "\n",
       "                                       sentence text  label  \n",
       "0  Hirschsprung disease (HSCR) is a multifactoria...      0  \n",
       "1  In this study, we review the identification of...      1  \n",
       "2  The majority of the identified genes are relat...      1  \n",
       "3  The non-Mendelian inheritance of sporadic non-...      1  \n",
       "4                  Coding sequence mutations in e.g.      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"bioasq10b_labelled.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsjI4EojwAXQ"
   },
   "source": [
    "The columns of the CSV file are:\n",
    "\n",
    "* `qid`: an ID for a question. Several rows may have the same question ID, as we can see above.\n",
    "* `sentid`: an ID for a sentence.\n",
    "* `question`: The text of the question. In the above example, the first rows all have the same question: \"Is Hirschsprung disease a mendelian or a multifactorial disorder?\"\n",
    "* `sentence text`: The text of the sentence.\n",
    "* `label`: 1 if the sentence is a part of the answer, 0 if the sentence is not part of the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOCkVY_JwAXQ"
   },
   "source": [
    "# Now Let's get started for the next step\n",
    "\n",
    "I'll use the provided files `training.csv`, `dev_test.csv`, and `test.csv` in the data.zip file for all the tasks below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VTTgRnN0dC4"
   },
   "source": [
    "# Simple Siamese NN\n",
    "\n",
    "I will Implement a simple TensorFlow-Keras neural model that has the following sequence of layers:\n",
    "\n",
    "1. An input layer that will accept the tf.idf of triplet data. The input of Siamese network is a triplet, consisting of anchor (i.e., the question), positive answer, negative answer.\n",
    "2. 3 hidden layers and a relu activation function. You need to determine the size of the hidden layers.\n",
    "3. Implement a class that serves as a distance layer. It returns the squared Euclidean distance between anchor and positive answer, as well as that between anchor and negative answer\n",
    "4. Implement a function that prepares raw data in csv files into triplets. Note that it is important to keep the similar number of positive pairs and negative pairs. For example, if a question has 10 anwsers, then we at most can have 10 positive pairs and it is good to associate this question with 10~20 negative sentences.\n",
    "\n",
    "\n",
    "Then I will train the model with the training data and use the `dev_test` set to determine a good size of the hidden layer.\n",
    "\n",
    "With the model that you have trained, implement a summariser that returns the $n$ sentences with highest predicted score. Use the following function signature:\n",
    "\n",
    "```{python}\n",
    "def nn_summariser(csvfile, questionids, n=1):\n",
    "   \"\"\"Return the IDs of the n sentences that have the highest predicted score.\n",
    "      The input questionids is a list of question ids.\n",
    "      The output is a list of lists of sentence ids\n",
    "   \"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "Then I'll report the final results using the test set. Remember: use the test set to report the final results of the best system only.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgAfp9skwAXQ"
   },
   "source": [
    "#### Data set for this practice was too big for my system's computing power so i had to split it in half and only use half of the data for the first two models and for the last model specifically i could only use a quarter of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-4CKtELwAXQ"
   },
   "source": [
    "### All the library and modules imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fMsRXJQIwAXQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LANlFc_GwAXQ"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "K3eP3gaHwAXQ",
    "outputId": "992bd624-219d-4c53-9b85-1db70157651e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(max_features=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(max_features=1000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('training.csv')\n",
    "dev_test_df = pd.read_csv('dev_test.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "#function to sample a fraction of the dataset\n",
    "def sample_fraction(df, fraction):\n",
    "    return df.sample(frac=fraction, random_state=42)\n",
    "\n",
    "#Sampling 50% of each dataset\n",
    "fraction = 0.5\n",
    "train_df = sample_fraction(train_df, fraction)\n",
    "dev_test_df = sample_fraction(dev_test_df, fraction)\n",
    "test_df = sample_fraction(test_df, fraction)\n",
    "\n",
    "#function to prepare triplets\n",
    "def prepare_triplets(df):\n",
    "    triplets = []\n",
    "    labels = []\n",
    "    questions = df['question'].unique()\n",
    "    for q in questions:\n",
    "        sub_df = df[df['question'] == q]\n",
    "        positives = sub_df[sub_df['label'] == 1]['sentence text'].tolist()\n",
    "        negatives = sub_df[sub_df['label'] == 0]['sentence text'].tolist()\n",
    "\n",
    "        for pos in positives:\n",
    "            for neg in negatives:\n",
    "                triplets.append((q, pos, neg))\n",
    "                labels.append(1)  #positive pair\n",
    "                labels.append(0)  #negative pair\n",
    "    return triplets, labels\n",
    "\n",
    "#preparing triplets for training and dev_test datasets\n",
    "train_triplets, train_labels = prepare_triplets(train_df)\n",
    "dev_test_triplets, dev_test_labels = prepare_triplets(dev_test_df)\n",
    "test_triplets, test_labels = prepare_triplets(test_df)\n",
    "#preparing tf-idf vectors\n",
    "all_text = train_df['question'].tolist() + train_df['sentence text'].tolist()\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  #reduced dimensionality\n",
    "vectorizer.fit(all_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTkOsvxPwAXQ"
   },
   "source": [
    "in the cell above i prepared the triplets by grouping each record by the qid and then adding a pair of sentence texts with the 1 and 0 labels to each triplet then i used TfidfVectorizer from sklearn, i also needed to reduce the max features of the vectorizer to 1000 to prevent the memory crash issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jT1qlIEywAXR"
   },
   "outputs": [],
   "source": [
    "def vectorize_triplets(triplets):\n",
    "    vectorized_triplets = []\n",
    "    for anchor, pos, neg in triplets:\n",
    "        anchor_vec = vectorizer.transform([anchor]).toarray()\n",
    "        pos_vec = vectorizer.transform([pos]).toarray()\n",
    "        neg_vec = vectorizer.transform([neg]).toarray()\n",
    "        vectorized_triplets.append((anchor_vec, pos_vec, neg_vec))\n",
    "    return np.array(vectorized_triplets, dtype=object)\n",
    "\n",
    "train_triplets_vec = vectorize_triplets(train_triplets)\n",
    "dev_test_triplets_vec = vectorize_triplets(dev_test_triplets)\n",
    "test_triplets_vec = vectorize_triplets(test_triplets)\n",
    "#convert to np.float32 to ensure compatibility with TensorFlow\n",
    "train_triplets_vec = np.array(train_triplets_vec.tolist(), dtype=np.float32)\n",
    "dev_test_triplets_vec = np.array(dev_test_triplets_vec.tolist(), dtype=np.float32)\n",
    "test_triplets_vec = np.array(test_triplets_vec.tolist(), dtype=np.float32)\n",
    "#distance layer\n",
    "class DistanceLayer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        anchor, positive, negative = inputs\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), axis=1, keepdims=True)\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), axis=1, keepdims=True)\n",
    "        return ap_distance, an_distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybSDlagZKB2J"
   },
   "source": [
    "the cell above contains the function definition of the triplet preparation by utilizing the vectorizer that i've imported from sklearn, then i used numpy library to conver the vectorized triplets into numpy arrays and converted the data types to numpy float 32 so they would be compatible with Tensorflow. Moreover, i've defined the class DistanceLayer that serves as a distance layer. It returns the squared Euclidean distance between anchor and positive answer, as well as that between anchor and negative answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvc1BvBUNPyY"
   },
   "source": [
    "### Model creation and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfwddldfwAXR"
   },
   "source": [
    "I've defined the model structure in the cell below, which has an input layer that accepts triplets, it consists of 3 hidden layers and a relu activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "D4d7YPNswAXR"
   },
   "outputs": [],
   "source": [
    "#siamese network\n",
    "def build_siamese_model(input_shape, hidden_size):\n",
    "    input = layers.Input(shape=input_shape)\n",
    "\n",
    "    #shared network\n",
    "    x = layers.Dense(hidden_size, activation='relu')(input)\n",
    "    x = layers.Dense(hidden_size, activation='relu')(x)\n",
    "    x = layers.Dense(hidden_size, activation='relu')(x)\n",
    "    shared_network = tf.keras.Model(inputs=input, outputs=x)\n",
    "\n",
    "    #inputs for anchor, positive and negative\n",
    "    anchor_input = layers.Input(shape=input_shape, name='anchor_input')\n",
    "    positive_input = layers.Input(shape=input_shape, name='positive_input')\n",
    "    negative_input = layers.Input(shape=input_shape, name='negative_input')\n",
    "\n",
    "    #shared embeddings\n",
    "    anchor_embedding = shared_network(anchor_input)\n",
    "    positive_embedding = shared_network(positive_input)\n",
    "    negative_embedding = shared_network(negative_input)\n",
    "\n",
    "    #distance layer\n",
    "    ap_distance, an_distance = DistanceLayer()([anchor_embedding, positive_embedding, negative_embedding])\n",
    "\n",
    "    model = tf.keras.Model(inputs=[anchor_input, positive_input, negative_input], outputs=[ap_distance, an_distance])\n",
    "    return model\n",
    "\n",
    "#loss function\n",
    "def loss(margin=0.1):\n",
    "    def loss_function(y_true, y_pred):\n",
    "        ap_distance, an_distance = y_pred\n",
    "        return tf.maximum(ap_distance - an_distance + margin, 0)\n",
    "    return loss_function\n",
    "\n",
    "#converting triplet data into arrays for training\n",
    "anchor_train = np.vstack(train_triplets_vec[:, 0])\n",
    "positive_train = np.vstack(train_triplets_vec[:, 1])\n",
    "negative_train = np.vstack(train_triplets_vec[:, 2])\n",
    "\n",
    "anchor_dev = np.vstack(dev_test_triplets_vec[:, 0])\n",
    "positive_dev = np.vstack(dev_test_triplets_vec[:, 1])\n",
    "negative_dev = np.vstack(dev_test_triplets_vec[:, 2])\n",
    "\n",
    "anchor_test = np.vstack(test_triplets_vec[:, 0])\n",
    "positive_test = np.vstack(test_triplets_vec[:, 1])\n",
    "negative_test = np.vstack(test_triplets_vec[:, 2])\n",
    "\n",
    "#splitting the data for cross-validation\n",
    "anchor_train, anchor_val, positive_train, positive_val, negative_train, negative_val = train_test_split(\n",
    "    anchor_train, positive_train, negative_train, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xuXgsiyOpEH"
   },
   "source": [
    "I've also implemented the custom loss function in the cell above and did manual trial and error to experiment the optimal margin value, then i converted the triplets into arrays for training. The loss function is triplet loss, which helps to train the Siamese network by minimizing the distance between the anchor and positive examples while maximizing the distance between the anchor and negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UIl7qLU5wAXR",
    "outputId": "ed39072a-3061-4db9-cf60-65f877e7cb05",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "Validation loss: 0.8756504654884338\n",
      "Start of epoch 1\n",
      "Validation loss: 0.2092791348695755\n",
      "Start of epoch 2\n",
      "Validation loss: 0.11120724678039551\n",
      "Start of epoch 3\n",
      "Validation loss: 0.15317168831825256\n",
      "Start of epoch 4\n",
      "Validation loss: 0.2040964961051941\n",
      "Start of epoch 5\n",
      "Validation loss: 0.16112138330936432\n",
      "Start of epoch 6\n",
      "Validation loss: 0.139619380235672\n",
      "Start of epoch 7\n",
      "Validation loss: 0.13301801681518555\n",
      "Start of epoch 8\n",
      "Validation loss: 0.13285623490810394\n",
      "Start of epoch 9\n",
      "Validation loss: 0.1328524351119995\n"
     ]
    }
   ],
   "source": [
    "#function to compute predictions\n",
    "def compute_predictions(anchor, positive, negative):\n",
    "    ap_distance, an_distance = siamese_model([anchor, positive, negative], training=False)\n",
    "    predictions = (ap_distance < an_distance).numpy().astype(int)\n",
    "    return predictions\n",
    "\n",
    "#building and compile the model\n",
    "siamese_model = build_siamese_model((1000,), 128)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "siamese_model.compile(optimizer=optimizer, loss=loss(0.1))\n",
    "\n",
    "#converting the data into TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((anchor_train, positive_train, negative_train)).batch(32)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((anchor_val, positive_val, negative_val)).batch(32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((anchor_test, positive_test, negative_test)).batch(32)\n",
    "#custom training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch}')\n",
    "\n",
    "    #training\n",
    "    for step, (anchor_batch, positive_batch, negative_batch) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            ap_distance, an_distance = siamese_model([anchor_batch, positive_batch, negative_batch], training=True)\n",
    "            loss_value = loss(0.1)(None, [ap_distance, an_distance])\n",
    "        grads = tape.gradient(loss_value, siamese_model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, siamese_model.trainable_weights))\n",
    "\n",
    "    #validation\n",
    "    val_losses = []\n",
    "    for step, (anchor_batch, positive_batch, negative_batch) in enumerate(val_dataset):\n",
    "        ap_distance, an_distance = siamese_model([anchor_batch, positive_batch, negative_batch], training=False)\n",
    "        val_loss_value = loss(0.1)(None, [ap_distance, an_distance])\n",
    "        val_losses.append(val_loss_value.numpy().flatten()[0])\n",
    "    val_loss = np.mean(val_losses)\n",
    "    print(f'Validation loss: {val_loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ErjNLNvOrmN"
   },
   "source": [
    "In the cell above i've define my custom training loop and fed the model the optimal hyper parameters that i've acquired through manual experiment, the reason i didn't use keras tuner is because it makes the computation of the model much more time consuming and my computer does not have that computing power to do it efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09Wzdd4NKB2K",
    "outputId": "b2916b5e-3c74-42ae-8611-6994c8c78fb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Precision: 0.5009970854425525\n",
      "validation Recall: 0.9778443113772455\n",
      "validation F1 Score: 0.662541839943199\n"
     ]
    }
   ],
   "source": [
    "#evaluating on the validation set\n",
    "val_predictions = compute_predictions(anchor_val, positive_val, negative_val).flatten()\n",
    "val_y_true = dev_test_labels[:len(val_predictions)]\n",
    "\n",
    "precision = precision_score(val_y_true, val_predictions)\n",
    "recall = recall_score(val_y_true, val_predictions)\n",
    "f1 = f1_score(val_y_true, val_predictions)\n",
    "\n",
    "print(f\"validation Precision: {precision}\")\n",
    "print(f\"validation Recall: {recall}\")\n",
    "print(f\"validation F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExMmDSpSOv7H"
   },
   "source": [
    "the results of the model evaluation on the validation set is the following:\n",
    "\n",
    "validation Precision: 0.5009970854425525\n",
    "\n",
    "validation Recall: 0.9778443113772455\n",
    "\n",
    "validation F1 Score: 0.662541839943199\n",
    "\n",
    "results are not great but is expected to have this sort of F1 score since the datasets are not adquate and large enough for the training and the labels 0s and 1s are not equally distributed, there are much more 0s than 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nW-S6mNkK5Hp",
    "outputId": "5b37e990-2679-4347-c98f-b2e0e9425d69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision: 0.5002120141342756\n",
      "Test Recall: 0.5953903095558546\n",
      "Test F1 Score: 0.5436669483063216\n"
     ]
    }
   ],
   "source": [
    "#evaluating on the test set\n",
    "test_predictions = compute_predictions(anchor_test, positive_test, negative_test).flatten()\n",
    "test_y_true = test_labels[:len(test_predictions)]\n",
    "\n",
    "precision = precision_score(test_y_true, test_predictions)\n",
    "recall = recall_score(test_y_true, test_predictions)\n",
    "f1 = f1_score(test_y_true, test_predictions)\n",
    "\n",
    "print(f\"Test Precision: {precision}\")\n",
    "print(f\"Test Recall: {recall}\")\n",
    "print(f\"Test F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVrPEuaPO8Ko"
   },
   "source": [
    "The model's performance on the test set is lower than the validation set which is completely normal:\n",
    "\n",
    "Test Precision: 0.5002120141342756\n",
    "\n",
    "Test Recall: 0.5953903095558546\n",
    "\n",
    "Test F1 Score: 0.5436669483063216\n",
    "\n",
    "however an F1 score of 0.5334 could be still considered as an acceptable performance due to inadequacy of the data volume and label distribution imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSdriuV3PPcP"
   },
   "source": [
    "### nn_summariser function\n",
    "In the cell below i've defined the function that returns the IDs of the n sentences that have the highest predicted score. The input questionids is a list of question ids. The output is a list of lists of sentence ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2N7TzYtSwAXR",
    "outputId": "6ad9e46c-80d3-47cb-b172-d02c6b17bbbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question ID: 4109, Top Sentence IDs: [20]\n",
      "Question ID: 584, Top Sentence IDs: [20]\n",
      "Question ID: 1644, Top Sentence IDs: [52]\n",
      "Question ID: 3764, Top Sentence IDs: [8]\n",
      "Question ID: 3508, Top Sentence IDs: [3]\n",
      "Question ID: 991, Top Sentence IDs: [33]\n",
      "Question ID: 2401, Top Sentence IDs: [11]\n",
      "Question ID: 1999, Top Sentence IDs: [4]\n",
      "Question ID: 937, Top Sentence IDs: [0]\n",
      "Question ID: 2125, Top Sentence IDs: [8]\n",
      "Question ID: 1606, Top Sentence IDs: [33]\n",
      "Question ID: 2531, Top Sentence IDs: [6]\n",
      "Question ID: 2938, Top Sentence IDs: [0]\n",
      "Question ID: 671, Top Sentence IDs: [34]\n",
      "Question ID: 1922, Top Sentence IDs: [6]\n",
      "Question ID: 1754, Top Sentence IDs: [13]\n",
      "Question ID: 1449, Top Sentence IDs: [6]\n",
      "Question ID: 1852, Top Sentence IDs: [15]\n",
      "Question ID: 2092, Top Sentence IDs: [9]\n",
      "Question ID: 3905, Top Sentence IDs: [6]\n",
      "Question ID: 643, Top Sentence IDs: [21]\n",
      "Question ID: 3987, Top Sentence IDs: [23]\n",
      "Question ID: 520, Top Sentence IDs: [4]\n",
      "Question ID: 1014, Top Sentence IDs: [20]\n",
      "Question ID: 1253, Top Sentence IDs: [11]\n",
      "Question ID: 1907, Top Sentence IDs: [35]\n",
      "Question ID: 605, Top Sentence IDs: [0]\n",
      "Question ID: 1480, Top Sentence IDs: [0]\n",
      "Question ID: 2772, Top Sentence IDs: [8]\n",
      "Question ID: 290, Top Sentence IDs: [6]\n",
      "Question ID: 329, Top Sentence IDs: [31]\n",
      "Question ID: 3719, Top Sentence IDs: [0]\n",
      "Question ID: 1766, Top Sentence IDs: [18]\n",
      "Question ID: 1537, Top Sentence IDs: [7]\n",
      "Question ID: 72, Top Sentence IDs: [29]\n",
      "Question ID: 4185, Top Sentence IDs: [4]\n",
      "Question ID: 1177, Top Sentence IDs: [3]\n",
      "Question ID: 978, Top Sentence IDs: [3]\n",
      "Question ID: 2786, Top Sentence IDs: [6]\n",
      "Question ID: 259, Top Sentence IDs: [1]\n",
      "Question ID: 1417, Top Sentence IDs: [24]\n",
      "Question ID: 396, Top Sentence IDs: [31]\n",
      "Question ID: 273, Top Sentence IDs: [24]\n",
      "Question ID: 1022, Top Sentence IDs: [17]\n",
      "Question ID: 3748, Top Sentence IDs: [16]\n",
      "Question ID: 806, Top Sentence IDs: [10]\n",
      "Question ID: 1527, Top Sentence IDs: [3]\n",
      "Question ID: 10, Top Sentence IDs: [6]\n",
      "Question ID: 4107, Top Sentence IDs: [0]\n",
      "Question ID: 1083, Top Sentence IDs: [30]\n",
      "Question ID: 371, Top Sentence IDs: [11]\n",
      "Question ID: 300, Top Sentence IDs: [3]\n",
      "Question ID: 785, Top Sentence IDs: [5]\n",
      "Question ID: 3401, Top Sentence IDs: [0]\n",
      "Question ID: 972, Top Sentence IDs: [32]\n",
      "Question ID: 1001, Top Sentence IDs: [13]\n",
      "Question ID: 3850, Top Sentence IDs: [10]\n",
      "Question ID: 2971, Top Sentence IDs: [22]\n",
      "Question ID: 4133, Top Sentence IDs: [10]\n",
      "Question ID: 1539, Top Sentence IDs: [118]\n",
      "Question ID: 3307, Top Sentence IDs: [4]\n",
      "Question ID: 854, Top Sentence IDs: [12]\n",
      "Question ID: 3943, Top Sentence IDs: [4]\n",
      "Question ID: 2021, Top Sentence IDs: [45]\n",
      "Question ID: 441, Top Sentence IDs: [2]\n",
      "Question ID: 1431, Top Sentence IDs: [0]\n",
      "Question ID: 3168, Top Sentence IDs: [5]\n",
      "Question ID: 1119, Top Sentence IDs: [5]\n",
      "Question ID: 1511, Top Sentence IDs: [3]\n",
      "Question ID: 1614, Top Sentence IDs: [11]\n",
      "Question ID: 3182, Top Sentence IDs: [7]\n",
      "Question ID: 1811, Top Sentence IDs: [23]\n",
      "Question ID: 2683, Top Sentence IDs: [8]\n",
      "Question ID: 1520, Top Sentence IDs: [35]\n",
      "Question ID: 1308, Top Sentence IDs: [4]\n",
      "Question ID: 129, Top Sentence IDs: [8]\n",
      "Question ID: 672, Top Sentence IDs: [19]\n",
      "Question ID: 1202, Top Sentence IDs: [10]\n",
      "Question ID: 895, Top Sentence IDs: [15]\n",
      "Question ID: 4085, Top Sentence IDs: [12]\n",
      "Question ID: 1112, Top Sentence IDs: [22]\n",
      "Question ID: 2966, Top Sentence IDs: [8]\n",
      "Question ID: 2472, Top Sentence IDs: [9]\n",
      "Question ID: 2501, Top Sentence IDs: [15]\n",
      "Question ID: 4127, Top Sentence IDs: [9]\n",
      "Question ID: 443, Top Sentence IDs: [2]\n",
      "Question ID: 505, Top Sentence IDs: [12]\n",
      "Question ID: 1282, Top Sentence IDs: [6]\n",
      "Question ID: 1373, Top Sentence IDs: [2]\n",
      "Question ID: 456, Top Sentence IDs: [18]\n",
      "Question ID: 4121, Top Sentence IDs: [2]\n",
      "Question ID: 2009, Top Sentence IDs: [11]\n",
      "Question ID: 2226, Top Sentence IDs: [1]\n",
      "Question ID: 82, Top Sentence IDs: [10]\n",
      "Question ID: 1296, Top Sentence IDs: [16]\n",
      "Question ID: 1716, Top Sentence IDs: [52]\n",
      "Question ID: 2253, Top Sentence IDs: [3]\n",
      "Question ID: 3565, Top Sentence IDs: [2]\n",
      "Question ID: 1603, Top Sentence IDs: [4]\n",
      "Question ID: 2478, Top Sentence IDs: [18]\n",
      "Question ID: 351, Top Sentence IDs: [4]\n",
      "Question ID: 3809, Top Sentence IDs: [27]\n",
      "Question ID: 1594, Top Sentence IDs: [0]\n",
      "Question ID: 2933, Top Sentence IDs: [0]\n",
      "Question ID: 2055, Top Sentence IDs: [0]\n",
      "Question ID: 1761, Top Sentence IDs: [23]\n",
      "Question ID: 3237, Top Sentence IDs: [2]\n",
      "Question ID: 1697, Top Sentence IDs: [0]\n",
      "Question ID: 3572, Top Sentence IDs: [4]\n",
      "Question ID: 1294, Top Sentence IDs: [19]\n",
      "Question ID: 384, Top Sentence IDs: [9]\n",
      "Question ID: 1665, Top Sentence IDs: [14]\n",
      "Question ID: 952, Top Sentence IDs: [0]\n",
      "Question ID: 3659, Top Sentence IDs: [5]\n",
      "Question ID: 1729, Top Sentence IDs: [9]\n",
      "Question ID: 135, Top Sentence IDs: [2]\n",
      "Question ID: 2552, Top Sentence IDs: [8]\n",
      "Question ID: 2474, Top Sentence IDs: [10]\n",
      "Question ID: 1828, Top Sentence IDs: [8]\n",
      "Question ID: 700, Top Sentence IDs: [1]\n",
      "Question ID: 47, Top Sentence IDs: [3]\n",
      "Question ID: 2386, Top Sentence IDs: [2]\n",
      "Question ID: 3160, Top Sentence IDs: [18]\n",
      "Question ID: 771, Top Sentence IDs: [7]\n",
      "Question ID: 1117, Top Sentence IDs: [2]\n",
      "Question ID: 1368, Top Sentence IDs: [34]\n",
      "Question ID: 3938, Top Sentence IDs: [21]\n",
      "Question ID: 2632, Top Sentence IDs: [10]\n",
      "Question ID: 1916, Top Sentence IDs: [25]\n",
      "Question ID: 3729, Top Sentence IDs: [0]\n",
      "Question ID: 2626, Top Sentence IDs: [1]\n",
      "Question ID: 2045, Top Sentence IDs: [1]\n",
      "Question ID: 1223, Top Sentence IDs: [10]\n",
      "Question ID: 3150, Top Sentence IDs: [2]\n",
      "Question ID: 4210, Top Sentence IDs: [20]\n",
      "Question ID: 2452, Top Sentence IDs: [0]\n",
      "Question ID: 2763, Top Sentence IDs: [4]\n",
      "Question ID: 2041, Top Sentence IDs: [6]\n",
      "Question ID: 285, Top Sentence IDs: [14]\n",
      "Question ID: 1823, Top Sentence IDs: [35]\n",
      "Question ID: 2229, Top Sentence IDs: [29]\n",
      "Question ID: 1222, Top Sentence IDs: [12]\n",
      "Question ID: 693, Top Sentence IDs: [23]\n",
      "Question ID: 713, Top Sentence IDs: [5]\n",
      "Question ID: 1486, Top Sentence IDs: [2]\n",
      "Question ID: 3596, Top Sentence IDs: [0]\n",
      "Question ID: 957, Top Sentence IDs: [0]\n",
      "Question ID: 686, Top Sentence IDs: [26]\n",
      "Question ID: 2342, Top Sentence IDs: [2]\n",
      "Question ID: 1568, Top Sentence IDs: [29]\n",
      "Question ID: 3249, Top Sentence IDs: [27]\n",
      "Question ID: 3433, Top Sentence IDs: [6]\n",
      "Question ID: 596, Top Sentence IDs: [15]\n",
      "Question ID: 145, Top Sentence IDs: [6]\n",
      "Question ID: 1203, Top Sentence IDs: [4]\n",
      "Question ID: 3969, Top Sentence IDs: [6]\n",
      "Question ID: 1337, Top Sentence IDs: [18]\n",
      "Question ID: 617, Top Sentence IDs: [3]\n",
      "Question ID: 2462, Top Sentence IDs: [17]\n",
      "Question ID: 593, Top Sentence IDs: [9]\n",
      "Question ID: 1429, Top Sentence IDs: [0]\n",
      "Question ID: 868, Top Sentence IDs: [13]\n",
      "Question ID: 2596, Top Sentence IDs: [4]\n",
      "Question ID: 3126, Top Sentence IDs: [19]\n",
      "Question ID: 2201, Top Sentence IDs: [6]\n",
      "Question ID: 4004, Top Sentence IDs: [9]\n",
      "Question ID: 1817, Top Sentence IDs: [26]\n",
      "Question ID: 1281, Top Sentence IDs: [5]\n",
      "Question ID: 720, Top Sentence IDs: [21]\n",
      "Question ID: 4132, Top Sentence IDs: [0]\n",
      "Question ID: 829, Top Sentence IDs: [0]\n",
      "Question ID: 408, Top Sentence IDs: [9]\n",
      "Question ID: 3714, Top Sentence IDs: [0]\n",
      "Question ID: 1323, Top Sentence IDs: [26]\n",
      "Question ID: 6, Top Sentence IDs: [16]\n",
      "Question ID: 1933, Top Sentence IDs: [4]\n",
      "Question ID: 3399, Top Sentence IDs: [0]\n",
      "Question ID: 2675, Top Sentence IDs: [8]\n",
      "Question ID: 2840, Top Sentence IDs: [7]\n",
      "Question ID: 2191, Top Sentence IDs: [1]\n",
      "Question ID: 1765, Top Sentence IDs: [9]\n",
      "Question ID: 115, Top Sentence IDs: [24]\n",
      "Question ID: 1218, Top Sentence IDs: [4]\n",
      "Question ID: 2448, Top Sentence IDs: [31]\n",
      "Question ID: 2480, Top Sentence IDs: [11]\n",
      "Question ID: 1617, Top Sentence IDs: [0]\n",
      "Question ID: 707, Top Sentence IDs: [8]\n",
      "Question ID: 3823, Top Sentence IDs: [6]\n",
      "Question ID: 1375, Top Sentence IDs: [0]\n",
      "Question ID: 1506, Top Sentence IDs: [14]\n",
      "Question ID: 876, Top Sentence IDs: [42]\n",
      "Question ID: 3845, Top Sentence IDs: [0]\n",
      "Question ID: 324, Top Sentence IDs: [17]\n",
      "Question ID: 2304, Top Sentence IDs: [15]\n",
      "Question ID: 119, Top Sentence IDs: [11]\n",
      "Question ID: 1340, Top Sentence IDs: [10]\n",
      "Question ID: 3772, Top Sentence IDs: [4]\n",
      "Question ID: 4167, Top Sentence IDs: [29]\n",
      "Question ID: 245, Top Sentence IDs: [0]\n",
      "Question ID: 110, Top Sentence IDs: [3]\n",
      "Question ID: 2245, Top Sentence IDs: [0]\n",
      "Question ID: 74, Top Sentence IDs: [32]\n",
      "Question ID: 1966, Top Sentence IDs: [13]\n",
      "Question ID: 2647, Top Sentence IDs: [2]\n",
      "Question ID: 1138, Top Sentence IDs: [11]\n",
      "Question ID: 677, Top Sentence IDs: [17]\n",
      "Question ID: 1418, Top Sentence IDs: [0]\n",
      "Question ID: 3390, Top Sentence IDs: [6]\n",
      "Question ID: 2090, Top Sentence IDs: [22]\n",
      "Question ID: 2496, Top Sentence IDs: [7]\n",
      "Question ID: 2375, Top Sentence IDs: [5]\n",
      "Question ID: 1200, Top Sentence IDs: [0]\n",
      "Question ID: 3805, Top Sentence IDs: [0]\n",
      "Question ID: 1653, Top Sentence IDs: [24]\n",
      "Question ID: 158, Top Sentence IDs: [18]\n",
      "Question ID: 3667, Top Sentence IDs: [18]\n",
      "Question ID: 1172, Top Sentence IDs: [1]\n",
      "Question ID: 1797, Top Sentence IDs: [7]\n",
      "Question ID: 1676, Top Sentence IDs: [2]\n",
      "Question ID: 2153, Top Sentence IDs: [4]\n",
      "Question ID: 215, Top Sentence IDs: [4]\n",
      "Question ID: 796, Top Sentence IDs: [18]\n",
      "Question ID: 996, Top Sentence IDs: [7]\n",
      "Question ID: 69, Top Sentence IDs: [10]\n",
      "Question ID: 3442, Top Sentence IDs: [1]\n",
      "Question ID: 1640, Top Sentence IDs: [21]\n",
      "Question ID: 2149, Top Sentence IDs: [24]\n",
      "Question ID: 4143, Top Sentence IDs: [14]\n",
      "Question ID: 2499, Top Sentence IDs: [13]\n",
      "Question ID: 2805, Top Sentence IDs: [1]\n",
      "Question ID: 2573, Top Sentence IDs: [0]\n",
      "Question ID: 116, Top Sentence IDs: [21]\n",
      "Question ID: 489, Top Sentence IDs: [29]\n",
      "Question ID: 3900, Top Sentence IDs: [4]\n",
      "Question ID: 760, Top Sentence IDs: [9]\n",
      "Question ID: 4119, Top Sentence IDs: [33]\n",
      "Question ID: 1728, Top Sentence IDs: [26]\n",
      "Question ID: 3574, Top Sentence IDs: [27]\n",
      "Question ID: 968, Top Sentence IDs: [3]\n",
      "Question ID: 601, Top Sentence IDs: [22]\n",
      "Question ID: 1672, Top Sentence IDs: [37]\n",
      "Question ID: 647, Top Sentence IDs: [35]\n",
      "Question ID: 252, Top Sentence IDs: [12]\n",
      "Question ID: 3254, Top Sentence IDs: [24]\n",
      "Question ID: 639, Top Sentence IDs: [11]\n",
      "Question ID: 2959, Top Sentence IDs: [0]\n",
      "Question ID: 1991, Top Sentence IDs: [2]\n",
      "Question ID: 1186, Top Sentence IDs: [8]\n",
      "Question ID: 2326, Top Sentence IDs: [2]\n",
      "Question ID: 3554, Top Sentence IDs: [8]\n",
      "Question ID: 3837, Top Sentence IDs: [16]\n",
      "Question ID: 2941, Top Sentence IDs: [2]\n",
      "Question ID: 2861, Top Sentence IDs: [12]\n",
      "Question ID: 3873, Top Sentence IDs: [3]\n",
      "Question ID: 3683, Top Sentence IDs: [18]\n",
      "Question ID: 1332, Top Sentence IDs: [31]\n",
      "Question ID: 1021, Top Sentence IDs: [1]\n",
      "Question ID: 3348, Top Sentence IDs: [0]\n",
      "Question ID: 3428, Top Sentence IDs: [0]\n",
      "Question ID: 2737, Top Sentence IDs: [18]\n",
      "Question ID: 3100, Top Sentence IDs: [11]\n",
      "Question ID: 36, Top Sentence IDs: [26]\n",
      "Question ID: 2123, Top Sentence IDs: [12]\n",
      "Question ID: 1727, Top Sentence IDs: [9]\n",
      "Question ID: 747, Top Sentence IDs: [24]\n",
      "Question ID: 4062, Top Sentence IDs: [9]\n",
      "Question ID: 823, Top Sentence IDs: [0]\n",
      "Question ID: 99, Top Sentence IDs: [15]\n",
      "Question ID: 2244, Top Sentence IDs: [13]\n",
      "Question ID: 1776, Top Sentence IDs: [49]\n",
      "Question ID: 687, Top Sentence IDs: [20]\n",
      "Question ID: 2139, Top Sentence IDs: [17]\n",
      "Question ID: 4150, Top Sentence IDs: [12]\n",
      "Question ID: 3392, Top Sentence IDs: [4]\n",
      "Question ID: 2267, Top Sentence IDs: [3]\n",
      "Question ID: 2237, Top Sentence IDs: [0]\n",
      "Question ID: 2426, Top Sentence IDs: [19]\n",
      "Question ID: 222, Top Sentence IDs: [2]\n",
      "Question ID: 61, Top Sentence IDs: [36]\n",
      "Question ID: 1392, Top Sentence IDs: [7]\n",
      "Question ID: 1835, Top Sentence IDs: [4]\n",
      "Question ID: 420, Top Sentence IDs: [2]\n",
      "Question ID: 899, Top Sentence IDs: [8]\n",
      "Question ID: 1352, Top Sentence IDs: [26]\n",
      "Question ID: 2236, Top Sentence IDs: [14]\n",
      "Question ID: 1100, Top Sentence IDs: [0]\n",
      "Question ID: 2992, Top Sentence IDs: [8]\n",
      "Question ID: 959, Top Sentence IDs: [1]\n",
      "Question ID: 3806, Top Sentence IDs: [11]\n",
      "Question ID: 692, Top Sentence IDs: [7]\n",
      "Question ID: 1199, Top Sentence IDs: [0]\n",
      "Question ID: 3513, Top Sentence IDs: [3]\n",
      "Question ID: 1803, Top Sentence IDs: [5]\n",
      "Question ID: 2187, Top Sentence IDs: [13]\n",
      "Question ID: 2374, Top Sentence IDs: [7]\n",
      "Question ID: 646, Top Sentence IDs: [0]\n",
      "Question ID: 1876, Top Sentence IDs: [0]\n",
      "Question ID: 2761, Top Sentence IDs: [0]\n",
      "Question ID: 1499, Top Sentence IDs: [15]\n",
      "Question ID: 1699, Top Sentence IDs: [2]\n",
      "Question ID: 1487, Top Sentence IDs: [6]\n",
      "Question ID: 2299, Top Sentence IDs: [0]\n",
      "Question ID: 3375, Top Sentence IDs: [5]\n",
      "Question ID: 2233, Top Sentence IDs: [21]\n",
      "Question ID: 1319, Top Sentence IDs: [14]\n",
      "Question ID: 3945, Top Sentence IDs: [13]\n",
      "Question ID: 286, Top Sentence IDs: [16]\n",
      "Question ID: 2000, Top Sentence IDs: [5]\n",
      "Question ID: 1234, Top Sentence IDs: [12]\n",
      "Question ID: 2434, Top Sentence IDs: [17]\n",
      "Question ID: 1509, Top Sentence IDs: [10]\n",
      "Question ID: 336, Top Sentence IDs: [18]\n",
      "Question ID: 182, Top Sentence IDs: [4]\n",
      "Question ID: 3494, Top Sentence IDs: [7]\n",
      "Question ID: 7, Top Sentence IDs: [7]\n",
      "Question ID: 2910, Top Sentence IDs: [8]\n",
      "Question ID: 2806, Top Sentence IDs: [1]\n",
      "Question ID: 1746, Top Sentence IDs: [1]\n",
      "Question ID: 1988, Top Sentence IDs: [1]\n",
      "Question ID: 634, Top Sentence IDs: [32]\n",
      "Question ID: 712, Top Sentence IDs: [0]\n",
      "Question ID: 4130, Top Sentence IDs: [1]\n",
      "Question ID: 2459, Top Sentence IDs: [2]\n",
      "Question ID: 979, Top Sentence IDs: [1]\n",
      "Question ID: 1216, Top Sentence IDs: [11]\n",
      "Question ID: 305, Top Sentence IDs: [10]\n",
      "Question ID: 1555, Top Sentence IDs: [10]\n",
      "Question ID: 3332, Top Sentence IDs: [3]\n",
      "Question ID: 3308, Top Sentence IDs: [0]\n",
      "Question ID: 1937, Top Sentence IDs: [1]\n",
      "Question ID: 1070, Top Sentence IDs: [35]\n",
      "Question ID: 4036, Top Sentence IDs: [2]\n",
      "Question ID: 4189, Top Sentence IDs: [11]\n",
      "Question ID: 1306, Top Sentence IDs: [0]\n",
      "Question ID: 1935, Top Sentence IDs: [3]\n",
      "Question ID: 804, Top Sentence IDs: [4]\n",
      "Question ID: 2120, Top Sentence IDs: [1]\n",
      "Question ID: 1656, Top Sentence IDs: [3]\n",
      "Question ID: 1156, Top Sentence IDs: [14]\n",
      "Question ID: 3638, Top Sentence IDs: [21]\n",
      "Question ID: 3370, Top Sentence IDs: [1]\n",
      "Question ID: 2208, Top Sentence IDs: [11]\n",
      "Question ID: 1779, Top Sentence IDs: [12]\n",
      "Question ID: 607, Top Sentence IDs: [2]\n",
      "Question ID: 3090, Top Sentence IDs: [16]\n",
      "Question ID: 4179, Top Sentence IDs: [13]\n",
      "Question ID: 1043, Top Sentence IDs: [17]\n",
      "Question ID: 3589, Top Sentence IDs: [2]\n",
      "Question ID: 1964, Top Sentence IDs: [4]\n",
      "Question ID: 3279, Top Sentence IDs: [7]\n",
      "Question ID: 2415, Top Sentence IDs: [7]\n",
      "Question ID: 1522, Top Sentence IDs: [5]\n",
      "Question ID: 2506, Top Sentence IDs: [14]\n",
      "Question ID: 4220, Top Sentence IDs: [4]\n",
      "Question ID: 2835, Top Sentence IDs: [8]\n",
      "Question ID: 4198, Top Sentence IDs: [5]\n",
      "Question ID: 3797, Top Sentence IDs: [5]\n",
      "Question ID: 1460, Top Sentence IDs: [8]\n",
      "Question ID: 122, Top Sentence IDs: [2]\n",
      "Question ID: 3533, Top Sentence IDs: [10]\n",
      "Question ID: 493, Top Sentence IDs: [22]\n",
      "Question ID: 725, Top Sentence IDs: [52]\n",
      "Question ID: 681, Top Sentence IDs: [0]\n",
      "Question ID: 1556, Top Sentence IDs: [11]\n",
      "Question ID: 1873, Top Sentence IDs: [0]\n",
      "Question ID: 45, Top Sentence IDs: [22]\n",
      "Question ID: 1957, Top Sentence IDs: [6]\n",
      "Question ID: 3469, Top Sentence IDs: [1]\n",
      "Question ID: 1358, Top Sentence IDs: [13]\n",
      "Question ID: 2764, Top Sentence IDs: [2]\n",
      "Question ID: 1141, Top Sentence IDs: [1]\n",
      "Question ID: 263, Top Sentence IDs: [9]\n",
      "Question ID: 2360, Top Sentence IDs: [5]\n",
      "Question ID: 2485, Top Sentence IDs: [12]\n",
      "Question ID: 2082, Top Sentence IDs: [23]\n",
      "Question ID: 4015, Top Sentence IDs: [9]\n",
      "Question ID: 2321, Top Sentence IDs: [11]\n",
      "Question ID: 1063, Top Sentence IDs: [28]\n",
      "Question ID: 3543, Top Sentence IDs: [5]\n",
      "Question ID: 3042, Top Sentence IDs: [12]\n",
      "Question ID: 2101, Top Sentence IDs: [15]\n",
      "Question ID: 3154, Top Sentence IDs: [2]\n",
      "Question ID: 270, Top Sentence IDs: [7]\n",
      "Question ID: 624, Top Sentence IDs: [4]\n",
      "Question ID: 848, Top Sentence IDs: [13]\n",
      "Question ID: 1287, Top Sentence IDs: [7]\n",
      "Question ID: 2836, Top Sentence IDs: [2]\n",
      "Question ID: 718, Top Sentence IDs: [8]\n",
      "Question ID: 2319, Top Sentence IDs: [8]\n",
      "Question ID: 2181, Top Sentence IDs: [24]\n",
      "Question ID: 1343, Top Sentence IDs: [6]\n",
      "Question ID: 1152, Top Sentence IDs: [3]\n",
      "Question ID: 1799, Top Sentence IDs: [7]\n",
      "Question ID: 2214, Top Sentence IDs: [2]\n",
      "Question ID: 3014, Top Sentence IDs: [13]\n",
      "Question ID: 163, Top Sentence IDs: [44]\n",
      "Question ID: 1162, Top Sentence IDs: [1]\n",
      "Question ID: 465, Top Sentence IDs: [0]\n",
      "Question ID: 180, Top Sentence IDs: [13]\n",
      "Question ID: 3878, Top Sentence IDs: [8]\n",
      "Question ID: 1736, Top Sentence IDs: [7]\n",
      "Question ID: 1513, Top Sentence IDs: [0]\n",
      "Question ID: 228, Top Sentence IDs: [23]\n",
      "Question ID: 2846, Top Sentence IDs: [1]\n",
      "Question ID: 2431, Top Sentence IDs: [6]\n",
      "Question ID: 2667, Top Sentence IDs: [13]\n",
      "Question ID: 1292, Top Sentence IDs: [4]\n",
      "Question ID: 1125, Top Sentence IDs: [7]\n",
      "Question ID: 2538, Top Sentence IDs: [9]\n",
      "Question ID: 888, Top Sentence IDs: [9]\n",
      "Question ID: 3971, Top Sentence IDs: [0]\n",
      "Question ID: 2783, Top Sentence IDs: [0]\n",
      "Question ID: 933, Top Sentence IDs: [2]\n",
      "Question ID: 2135, Top Sentence IDs: [3]\n",
      "Question ID: 4080, Top Sentence IDs: [42]\n",
      "Question ID: 3593, Top Sentence IDs: [0]\n",
      "Question ID: 1184, Top Sentence IDs: [0]\n",
      "Question ID: 4000, Top Sentence IDs: [3]\n",
      "Question ID: 376, Top Sentence IDs: [11]\n",
      "Question ID: 3725, Top Sentence IDs: [1]\n",
      "Question ID: 1268, Top Sentence IDs: [13]\n",
      "Question ID: 3044, Top Sentence IDs: [0]\n",
      "Question ID: 3321, Top Sentence IDs: [0]\n",
      "Question ID: 1624, Top Sentence IDs: [7]\n",
      "Question ID: 573, Top Sentence IDs: [7]\n",
      "Question ID: 3218, Top Sentence IDs: [14]\n",
      "Question ID: 2252, Top Sentence IDs: [15]\n",
      "Question ID: 2034, Top Sentence IDs: [3]\n",
      "Question ID: 2225, Top Sentence IDs: [9]\n",
      "Question ID: 811, Top Sentence IDs: [10]\n",
      "Question ID: 4035, Top Sentence IDs: [22]\n",
      "Question ID: 3289, Top Sentence IDs: [17]\n",
      "Question ID: 817, Top Sentence IDs: [7]\n",
      "Question ID: 106, Top Sentence IDs: [0]\n",
      "Question ID: 2656, Top Sentence IDs: [1]\n",
      "Question ID: 3450, Top Sentence IDs: [2]\n",
      "Question ID: 281, Top Sentence IDs: [14]\n",
      "Question ID: 3752, Top Sentence IDs: [18]\n",
      "Question ID: 3980, Top Sentence IDs: [16]\n",
      "Question ID: 2127, Top Sentence IDs: [1]\n",
      "Question ID: 4162, Top Sentence IDs: [5]\n",
      "Question ID: 783, Top Sentence IDs: [13]\n",
      "Question ID: 3410, Top Sentence IDs: [0]\n",
      "Question ID: 1682, Top Sentence IDs: [9]\n",
      "Question ID: 2532, Top Sentence IDs: [1]\n",
      "Question ID: 1286, Top Sentence IDs: [9]\n",
      "Question ID: 1290, Top Sentence IDs: [17]\n",
      "Question ID: 540, Top Sentence IDs: [21]\n",
      "Question ID: 1453, Top Sentence IDs: [22]\n",
      "Question ID: 1098, Top Sentence IDs: [16]\n",
      "Question ID: 2498, Top Sentence IDs: [3]\n",
      "Question ID: 1917, Top Sentence IDs: [4]\n",
      "Question ID: 2791, Top Sentence IDs: [0]\n",
      "Question ID: 3940, Top Sentence IDs: [0]\n",
      "Question ID: 1147, Top Sentence IDs: [20]\n",
      "Question ID: 2454, Top Sentence IDs: [8]\n",
      "Question ID: 35, Top Sentence IDs: [5]\n",
      "Question ID: 1424, Top Sentence IDs: [17]\n",
      "Question ID: 1523, Top Sentence IDs: [5]\n",
      "Question ID: 3409, Top Sentence IDs: [1]\n",
      "Question ID: 1692, Top Sentence IDs: [6]\n",
      "Question ID: 808, Top Sentence IDs: [5]\n",
      "Question ID: 1855, Top Sentence IDs: [0]\n",
      "Question ID: 1631, Top Sentence IDs: [8]\n",
      "Question ID: 3275, Top Sentence IDs: [0]\n",
      "Question ID: 1089, Top Sentence IDs: [6]\n",
      "Question ID: 296, Top Sentence IDs: [6]\n",
      "Question ID: 3317, Top Sentence IDs: [5]\n",
      "Question ID: 1092, Top Sentence IDs: [0]\n",
      "Question ID: 1871, Top Sentence IDs: [32]\n",
      "Question ID: 3870, Top Sentence IDs: [20]\n",
      "Question ID: 2422, Top Sentence IDs: [12]\n",
      "Question ID: 1354, Top Sentence IDs: [3]\n",
      "Question ID: 202, Top Sentence IDs: [3]\n",
      "Question ID: 2138, Top Sentence IDs: [11]\n",
      "Question ID: 3884, Top Sentence IDs: [5]\n",
      "Question ID: 3274, Top Sentence IDs: [22]\n",
      "Question ID: 2466, Top Sentence IDs: [16]\n",
      "Question ID: 1501, Top Sentence IDs: [21]\n",
      "Question ID: 1455, Top Sentence IDs: [3]\n",
      "Question ID: 2822, Top Sentence IDs: [4]\n",
      "Question ID: 3808, Top Sentence IDs: [5]\n",
      "Question ID: 258, Top Sentence IDs: [13]\n",
      "Question ID: 2339, Top Sentence IDs: [9]\n",
      "Question ID: 3492, Top Sentence IDs: [2]\n",
      "Question ID: 2182, Top Sentence IDs: [5]\n",
      "Question ID: 358, Top Sentence IDs: [8]\n",
      "Question ID: 2295, Top Sentence IDs: [7]\n",
      "Question ID: 4092, Top Sentence IDs: [1]\n",
      "Question ID: 2396, Top Sentence IDs: [0]\n",
      "Question ID: 1168, Top Sentence IDs: [32]\n",
      "Question ID: 2366, Top Sentence IDs: [23]\n",
      "Question ID: 3773, Top Sentence IDs: [7]\n",
      "Question ID: 2489, Top Sentence IDs: [5]\n",
      "Question ID: 1261, Top Sentence IDs: [8]\n",
      "Question ID: 3630, Top Sentence IDs: [1]\n",
      "Question ID: 2887, Top Sentence IDs: [1]\n",
      "Question ID: 2403, Top Sentence IDs: [10]\n",
      "Question ID: 2242, Top Sentence IDs: [2]\n",
      "Question ID: 3674, Top Sentence IDs: [3]\n",
      "Question ID: 2457, Top Sentence IDs: [0]\n",
      "Question ID: 2284, Top Sentence IDs: [0]\n",
      "Question ID: 393, Top Sentence IDs: [4]\n",
      "Question ID: 494, Top Sentence IDs: [4]\n",
      "Question ID: 3898, Top Sentence IDs: [0]\n",
      "Question ID: 2863, Top Sentence IDs: [17]\n",
      "Question ID: 1690, Top Sentence IDs: [14]\n",
      "Question ID: 1142, Top Sentence IDs: [2]\n",
      "Question ID: 2443, Top Sentence IDs: [1]\n",
      "Question ID: 3192, Top Sentence IDs: [1]\n",
      "Question ID: 547, Top Sentence IDs: [9]\n",
      "Question ID: 4018, Top Sentence IDs: [25]\n",
      "Question ID: 1561, Top Sentence IDs: [13]\n",
      "Question ID: 4155, Top Sentence IDs: [3]\n",
      "Question ID: 1251, Top Sentence IDs: [2]\n",
      "Question ID: 185, Top Sentence IDs: [7]\n",
      "Question ID: 3801, Top Sentence IDs: [0]\n",
      "Question ID: 3452, Top Sentence IDs: [5]\n",
      "Question ID: 1181, Top Sentence IDs: [2]\n",
      "Question ID: 167, Top Sentence IDs: [2]\n",
      "Question ID: 516, Top Sentence IDs: [14]\n",
      "Question ID: 2300, Top Sentence IDs: [5]\n",
      "Question ID: 3335, Top Sentence IDs: [8]\n",
      "Question ID: 1419, Top Sentence IDs: [3]\n",
      "Question ID: 1003, Top Sentence IDs: [6]\n",
      "Question ID: 1547, Top Sentence IDs: [0]\n",
      "Question ID: 1660, Top Sentence IDs: [8]\n",
      "Question ID: 3458, Top Sentence IDs: [1]\n",
      "Question ID: 434, Top Sentence IDs: [6]\n",
      "Question ID: 1920, Top Sentence IDs: [13]\n",
      "Question ID: 1393, Top Sentence IDs: [11]\n",
      "Question ID: 2180, Top Sentence IDs: [1]\n",
      "Question ID: 598, Top Sentence IDs: [16]\n",
      "Question ID: 916, Top Sentence IDs: [9]\n",
      "Question ID: 1391, Top Sentence IDs: [0]\n",
      "Question ID: 3790, Top Sentence IDs: [8]\n",
      "Question ID: 2110, Top Sentence IDs: [17]\n",
      "Question ID: 3607, Top Sentence IDs: [4]\n",
      "Question ID: 1053, Top Sentence IDs: [1]\n",
      "Question ID: 2405, Top Sentence IDs: [0]\n",
      "Question ID: 3782, Top Sentence IDs: [4]\n",
      "Question ID: 3304, Top Sentence IDs: [6]\n",
      "Question ID: 2046, Top Sentence IDs: [3]\n",
      "Question ID: 4067, Top Sentence IDs: [15]\n",
      "Question ID: 283, Top Sentence IDs: [1]\n",
      "Question ID: 3175, Top Sentence IDs: [2]\n",
      "Question ID: 3415, Top Sentence IDs: [6]\n",
      "Question ID: 3104, Top Sentence IDs: [1]\n",
      "Question ID: 579, Top Sentence IDs: [3]\n",
      "Question ID: 1072, Top Sentence IDs: [0]\n",
      "Question ID: 2734, Top Sentence IDs: [2]\n",
      "Question ID: 1123, Top Sentence IDs: [4]\n",
      "Question ID: 2065, Top Sentence IDs: [1]\n",
      "Question ID: 339, Top Sentence IDs: [5]\n",
      "Question ID: 1804, Top Sentence IDs: [0]\n",
      "Question ID: 2710, Top Sentence IDs: [4]\n",
      "Question ID: 3244, Top Sentence IDs: [5]\n",
      "Question ID: 1671, Top Sentence IDs: [2]\n",
      "Question ID: 2665, Top Sentence IDs: [2]\n",
      "Question ID: 2873, Top Sentence IDs: [5]\n",
      "Question ID: 2701, Top Sentence IDs: [2]\n",
      "Question ID: 3062, Top Sentence IDs: [0]\n",
      "Question ID: 2991, Top Sentence IDs: [4]\n",
      "Question ID: 739, Top Sentence IDs: [22]\n",
      "Question ID: 3737, Top Sentence IDs: [7]\n",
      "Question ID: 729, Top Sentence IDs: [10]\n",
      "Question ID: 3292, Top Sentence IDs: [5]\n",
      "Question ID: 3243, Top Sentence IDs: [3]\n",
      "Question ID: 3702, Top Sentence IDs: [0]\n",
      "Question ID: 3819, Top Sentence IDs: [1]\n",
      "Question ID: 3017, Top Sentence IDs: [4]\n",
      "Question ID: 1939, Top Sentence IDs: [7]\n",
      "Question ID: 3920, Top Sentence IDs: [0]\n",
      "Question ID: 141, Top Sentence IDs: [5]\n",
      "Question ID: 2406, Top Sentence IDs: [3]\n",
      "Question ID: 1403, Top Sentence IDs: [18]\n",
      "Question ID: 3743, Top Sentence IDs: [8]\n",
      "Question ID: 620, Top Sentence IDs: [2]\n",
      "Question ID: 1943, Top Sentence IDs: [7]\n",
      "Question ID: 2362, Top Sentence IDs: [0]\n",
      "Question ID: 578, Top Sentence IDs: [3]\n",
      "Question ID: 3595, Top Sentence IDs: [0]\n",
      "Question ID: 2541, Top Sentence IDs: [2]\n",
      "Question ID: 3688, Top Sentence IDs: [0]\n",
      "Question ID: 3111, Top Sentence IDs: [6]\n",
      "Question ID: 4010, Top Sentence IDs: [19]\n",
      "Question ID: 3497, Top Sentence IDs: [2]\n",
      "Question ID: 742, Top Sentence IDs: [4]\n",
      "Question ID: 3109, Top Sentence IDs: [1]\n",
      "Question ID: 4084, Top Sentence IDs: [1]\n",
      "Question ID: 3471, Top Sentence IDs: [0]\n",
      "Question ID: 3587, Top Sentence IDs: [0]\n",
      "Question ID: 3718, Top Sentence IDs: [18]\n",
      "Question ID: 2227, Top Sentence IDs: [6]\n",
      "Question ID: 2085, Top Sentence IDs: [0]\n",
      "Question ID: 2977, Top Sentence IDs: [9]\n",
      "Question ID: 3874, Top Sentence IDs: [1]\n",
      "Question ID: 4046, Top Sentence IDs: [1]\n",
      "Question ID: 2171, Top Sentence IDs: [2]\n",
      "Question ID: 1239, Top Sentence IDs: [10]\n",
      "Question ID: 380, Top Sentence IDs: [4]\n",
      "Question ID: 3610, Top Sentence IDs: [2]\n",
      "Question ID: 3966, Top Sentence IDs: [9]\n",
      "Question ID: 2477, Top Sentence IDs: [0]\n",
      "Question ID: 1196, Top Sentence IDs: [12]\n",
      "Question ID: 753, Top Sentence IDs: [2]\n",
      "Question ID: 2167, Top Sentence IDs: [0]\n",
      "Question ID: 1240, Top Sentence IDs: [12]\n",
      "Question ID: 2160, Top Sentence IDs: [4]\n",
      "Question ID: 4089, Top Sentence IDs: [3]\n",
      "Question ID: 2266, Top Sentence IDs: [0]\n",
      "Question ID: 120, Top Sentence IDs: [0]\n",
      "Question ID: 142, Top Sentence IDs: [1]\n",
      "Question ID: 3581, Top Sentence IDs: [3]\n",
      "Question ID: 279, Top Sentence IDs: [1]\n",
      "Question ID: 3728, Top Sentence IDs: [1]\n",
      "Question ID: 3722, Top Sentence IDs: [11]\n",
      "Question ID: 2838, Top Sentence IDs: [3]\n",
      "Question ID: 2388, Top Sentence IDs: [0]\n",
      "Question ID: 1413, Top Sentence IDs: [10]\n",
      "Question ID: 3418, Top Sentence IDs: [2]\n",
      "Question ID: 3327, Top Sentence IDs: [0]\n",
      "Question ID: 3633, Top Sentence IDs: [2]\n",
      "Question ID: 347, Top Sentence IDs: [3]\n",
      "Question ID: 4050, Top Sentence IDs: [18]\n",
      "Question ID: 3299, Top Sentence IDs: [0]\n",
      "Question ID: 4066, Top Sentence IDs: [0]\n",
      "Question ID: 2419, Top Sentence IDs: [6]\n",
      "Question ID: 3974, Top Sentence IDs: [6]\n",
      "Question ID: 665, Top Sentence IDs: [2]\n",
      "Question ID: 2018, Top Sentence IDs: [0]\n",
      "Question ID: 2767, Top Sentence IDs: [1]\n",
      "Question ID: 153, Top Sentence IDs: [4]\n",
      "Question ID: 4181, Top Sentence IDs: [0]\n",
      "Question ID: 2503, Top Sentence IDs: [1]\n",
      "Question ID: 542, Top Sentence IDs: [1]\n",
      "Question ID: 3996, Top Sentence IDs: [1]\n",
      "Question ID: 3482, Top Sentence IDs: [0]\n",
      "Question ID: 3835, Top Sentence IDs: [4]\n",
      "Question ID: 2106, Top Sentence IDs: [2]\n",
      "Question ID: 1048, Top Sentence IDs: [5]\n",
      "Question ID: 3152, Top Sentence IDs: [1]\n",
      "Question ID: 1452, Top Sentence IDs: [3]\n",
      "Question ID: 2052, Top Sentence IDs: [6]\n",
      "Question ID: 3807, Top Sentence IDs: [2]\n",
      "Question ID: 3514, Top Sentence IDs: [6]\n",
      "Question ID: 2650, Top Sentence IDs: [6]\n",
      "Question ID: 3282, Top Sentence IDs: [0]\n",
      "Question ID: 2159, Top Sentence IDs: [4]\n",
      "Question ID: 3463, Top Sentence IDs: [4]\n",
      "Question ID: 1521, Top Sentence IDs: [3]\n",
      "Question ID: 13, Top Sentence IDs: [1]\n",
      "Question ID: 892, Top Sentence IDs: [18]\n",
      "Question ID: 3933, Top Sentence IDs: [2]\n",
      "Question ID: 2730, Top Sentence IDs: [8]\n",
      "Question ID: 2794, Top Sentence IDs: [0]\n",
      "Question ID: 282, Top Sentence IDs: [1]\n",
      "Question ID: 1970, Top Sentence IDs: [0]\n",
      "Question ID: 560, Top Sentence IDs: [12]\n",
      "Question ID: 3086, Top Sentence IDs: [1]\n",
      "Question ID: 2206, Top Sentence IDs: [10]\n",
      "Question ID: 908, Top Sentence IDs: [2]\n",
      "Question ID: 2176, Top Sentence IDs: [3]\n",
      "Question ID: 1618, Top Sentence IDs: [9]\n",
      "Question ID: 835, Top Sentence IDs: [3]\n",
      "Question ID: 1782, Top Sentence IDs: [1]\n",
      "Question ID: 2808, Top Sentence IDs: [2]\n",
      "Question ID: 2588, Top Sentence IDs: [2]\n",
      "Question ID: 1851, Top Sentence IDs: [5]\n",
      "Question ID: 2702, Top Sentence IDs: [2]\n",
      "Question ID: 1710, Top Sentence IDs: [7]\n",
      "Question ID: 2305, Top Sentence IDs: [4]\n",
      "Question ID: 3281, Top Sentence IDs: [4]\n",
      "Question ID: 2142, Top Sentence IDs: [2]\n",
      "Question ID: 3163, Top Sentence IDs: [0]\n",
      "Question ID: 3468, Top Sentence IDs: [0]\n",
      "Question ID: 3381, Top Sentence IDs: [2]\n",
      "Question ID: 2722, Top Sentence IDs: [2]\n",
      "Question ID: 1247, Top Sentence IDs: [5]\n",
      "Question ID: 645, Top Sentence IDs: [2]\n",
      "Question ID: 3708, Top Sentence IDs: [0]\n",
      "Question ID: 3291, Top Sentence IDs: [0]\n",
      "Question ID: 3646, Top Sentence IDs: [4]\n",
      "Question ID: 2882, Top Sentence IDs: [1]\n",
      "Question ID: 2578, Top Sentence IDs: [9]\n",
      "Question ID: 1680, Top Sentence IDs: [0]\n",
      "Question ID: 2619, Top Sentence IDs: [0]\n",
      "Question ID: 3391, Top Sentence IDs: [3]\n",
      "Question ID: 1610, Top Sentence IDs: [9]\n",
      "Question ID: 3038, Top Sentence IDs: [1]\n",
      "Question ID: 3946, Top Sentence IDs: [0]\n",
      "Question ID: 3775, Top Sentence IDs: [0]\n",
      "Question ID: 3384, Top Sentence IDs: [1]\n",
      "Question ID: 2413, Top Sentence IDs: [2]\n",
      "Question ID: 1136, Top Sentence IDs: [3]\n",
      "Question ID: 1756, Top Sentence IDs: [2]\n",
      "Question ID: 3601, Top Sentence IDs: [2]\n",
      "Question ID: 3857, Top Sentence IDs: [4]\n",
      "Question ID: 857, Top Sentence IDs: [2]\n",
      "Question ID: 2099, Top Sentence IDs: [4]\n",
      "Question ID: 2768, Top Sentence IDs: [3]\n",
      "Question ID: 561, Top Sentence IDs: [8]\n",
      "Question ID: 3865, Top Sentence IDs: [0]\n",
      "Question ID: 2108, Top Sentence IDs: [1]\n",
      "Question ID: 3750, Top Sentence IDs: [2]\n",
      "Question ID: 2799, Top Sentence IDs: [0]\n",
      "Question ID: 2614, Top Sentence IDs: [2]\n",
      "Question ID: 1897, Top Sentence IDs: [0]\n",
      "Question ID: 722, Top Sentence IDs: [0]\n",
      "Question ID: 795, Top Sentence IDs: [1]\n",
      "Question ID: 4168, Top Sentence IDs: [0]\n",
      "Question ID: 391, Top Sentence IDs: [7]\n",
      "Question ID: 728, Top Sentence IDs: [3]\n",
      "Question ID: 2344, Top Sentence IDs: [0]\n",
      "Question ID: 1819, Top Sentence IDs: [2]\n",
      "Question ID: 311, Top Sentence IDs: [0]\n",
      "Question ID: 2849, Top Sentence IDs: [0]\n",
      "Question ID: 1673, Top Sentence IDs: [2]\n",
      "Question ID: 328, Top Sentence IDs: [0]\n",
      "Question ID: 2831, Top Sentence IDs: [2]\n",
      "Question ID: 3605, Top Sentence IDs: [3]\n",
      "Question ID: 1762, Top Sentence IDs: [2]\n",
      "Question ID: 1369, Top Sentence IDs: [6]\n",
      "Question ID: 2408, Top Sentence IDs: [5]\n",
      "Question ID: 2795, Top Sentence IDs: [0]\n",
      "Question ID: 1019, Top Sentence IDs: [1]\n",
      "Question ID: 2952, Top Sentence IDs: [3]\n",
      "Question ID: 2900, Top Sentence IDs: [1]\n",
      "Question ID: 844, Top Sentence IDs: [0]\n",
      "Question ID: 3700, Top Sentence IDs: [0]\n",
      "Question ID: 2550, Top Sentence IDs: [1]\n",
      "Question ID: 3222, Top Sentence IDs: [1]\n",
      "Question ID: 1315, Top Sentence IDs: [0]\n",
      "Question ID: 1249, Top Sentence IDs: [9]\n",
      "Question ID: 1066, Top Sentence IDs: [1]\n",
      "Question ID: 4146, Top Sentence IDs: [0]\n",
      "Question ID: 3498, Top Sentence IDs: [1]\n",
      "Question ID: 1328, Top Sentence IDs: [0]\n",
      "Question ID: 3416, Top Sentence IDs: [0]\n",
      "Question ID: 2310, Top Sentence IDs: [2]\n",
      "Question ID: 1948, Top Sentence IDs: [2]\n",
      "Question ID: 3411, Top Sentence IDs: [0]\n",
      "Question ID: 214, Top Sentence IDs: [1]\n",
      "Question ID: 1604, Top Sentence IDs: [0]\n",
      "Question ID: 3932, Top Sentence IDs: [0]\n",
      "Question ID: 869, Top Sentence IDs: [10]\n",
      "Question ID: 1558, Top Sentence IDs: [6]\n",
      "Question ID: 1985, Top Sentence IDs: [1]\n",
      "Question ID: 3457, Top Sentence IDs: [1]\n",
      "Question ID: 2905, Top Sentence IDs: [6]\n",
      "Question ID: 3694, Top Sentence IDs: [1]\n",
      "Question ID: 1093, Top Sentence IDs: [3]\n",
      "Question ID: 2916, Top Sentence IDs: [1]\n",
      "Question ID: 2314, Top Sentence IDs: [1]\n",
      "Question ID: 3345, Top Sentence IDs: [0]\n",
      "Question ID: 3021, Top Sentence IDs: [1]\n",
      "Question ID: 1772, Top Sentence IDs: [0]\n",
      "Question ID: 886, Top Sentence IDs: [1]\n",
      "Question ID: 2914, Top Sentence IDs: [0]\n",
      "Question ID: 2923, Top Sentence IDs: [2]\n",
      "Question ID: 2200, Top Sentence IDs: [4]\n",
      "Question ID: 3196, Top Sentence IDs: [4]\n",
      "Question ID: 3124, Top Sentence IDs: [8]\n",
      "Question ID: 4005, Top Sentence IDs: [0]\n",
      "Question ID: 117, Top Sentence IDs: [1]\n",
      "Question ID: 4208, Top Sentence IDs: [1]\n",
      "Question ID: 424, Top Sentence IDs: [1]\n",
      "Question ID: 1616, Top Sentence IDs: [2]\n",
      "Question ID: 1266, Top Sentence IDs: [1]\n",
      "Question ID: 4100, Top Sentence IDs: [0]\n",
      "Question ID: 4108, Top Sentence IDs: [2]\n",
      "Question ID: 3343, Top Sentence IDs: [3]\n",
      "Question ID: 528, Top Sentence IDs: [1]\n",
      "Question ID: 249, Top Sentence IDs: [3]\n",
      "Question ID: 2859, Top Sentence IDs: [2]\n",
      "Question ID: 3897, Top Sentence IDs: [2]\n",
      "Question ID: 3419, Top Sentence IDs: [4]\n",
      "Question ID: 2922, Top Sentence IDs: [3]\n",
      "Question ID: 3195, Top Sentence IDs: [1]\n",
      "Question ID: 2927, Top Sentence IDs: [1]\n",
      "Question ID: 3092, Top Sentence IDs: [2]\n",
      "Question ID: 3091, Top Sentence IDs: [0]\n",
      "Question ID: 2074, Top Sentence IDs: [1]\n",
      "Question ID: 2732, Top Sentence IDs: [0]\n",
      "Question ID: 3341, Top Sentence IDs: [5]\n",
      "Question ID: 3301, Top Sentence IDs: [1]\n",
      "Question ID: 1133, Top Sentence IDs: [3]\n",
      "Question ID: 1377, Top Sentence IDs: [0]\n",
      "Question ID: 1356, Top Sentence IDs: [0]\n",
      "Question ID: 940, Top Sentence IDs: [0]\n",
      "Question ID: 3901, Top Sentence IDs: [2]\n",
      "Question ID: 2990, Top Sentence IDs: [2]\n",
      "Question ID: 3194, Top Sentence IDs: [2]\n",
      "Question ID: 698, Top Sentence IDs: [3]\n",
      "Question ID: 1638, Top Sentence IDs: [1]\n",
      "Question ID: 3539, Top Sentence IDs: [2]\n",
      "Question ID: 1004, Top Sentence IDs: [4]\n"
     ]
    }
   ],
   "source": [
    "def nn_summariser(csvfile, questionids, n=1):\n",
    "    df = pd.read_csv(csvfile)\n",
    "    all_text = df['question'].tolist() + df['sentence text'].tolist()\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    vectorizer.fit(all_text)\n",
    "\n",
    "    results = []\n",
    "    for qid in questionids:\n",
    "        sub_df = df[df['qid'] == qid]\n",
    "        question = sub_df['question'].iloc[0]\n",
    "        sentences = sub_df['sentence text'].tolist()\n",
    "        sent_ids = sub_df['sentid'].tolist()\n",
    "\n",
    "        question_vec = vectorizer.transform([question]).toarray()\n",
    "        sentence_vecs = vectorizer.transform(sentences).toarray()\n",
    "\n",
    "        question_vec = np.tile(question_vec, (len(sentences), 1))\n",
    "\n",
    "        ap_distance, _ = siamese_model([question_vec, sentence_vecs, sentence_vecs])\n",
    "        scores = ap_distance.numpy().flatten()\n",
    "\n",
    "        top_n_indices = np.argsort(scores)[:n]\n",
    "        top_n_ids = [sent_ids[i] for i in top_n_indices]\n",
    "        results.append(top_n_ids)\n",
    "\n",
    "    return results\n",
    "\n",
    "#reporting final results using the test set\n",
    "test_question_ids = test_df['qid'].unique()\n",
    "test_results = nn_summariser('test.csv', test_question_ids, n=1)\n",
    "\n",
    "#printing the test results\n",
    "for qid, top_ids in zip(test_question_ids, test_results):\n",
    "    print(f\"Question ID: {qid}, Top Sentence IDs: {top_ids}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Q8W-jAwAXR"
   },
   "source": [
    "IDs of the  n  sentences that have the highest prediction score in the given question are printed in the cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0NeK3gM0dC9"
   },
   "source": [
    "# Recurrent NN\n",
    "\n",
    "I'll implement a more complex Siamese neural network that is composed of the following layers:\n",
    "\n",
    "* An embedding layer that generates embedding vectors of the sentence text with 35 dimensions.\n",
    "* A LSTM layer. You need to determine the size of this LSTM layer, and the text length limit (if needed).\n",
    "* 3 hidden layers and a relu activation function. You need to determine the size of the hidden layers.\n",
    "\n",
    "I'll train the model with the training data, use the `dev_test` set to determine a good size of the LSTM layer and an appropriate length limit (if needed), and report the final results using the test set. Again, remember to use the test set only after you have determined the optimal parameters of the LSTM layer.\n",
    "\n",
    "At last based on my experiments, I will comment on whether this system is better than the systems developed in the previous tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8RRCWeQTrPl"
   },
   "outputs": [],
   "source": [
    "#loading datasets\n",
    "train_df = pd.read_csv('training.csv')\n",
    "dev_test_df = pd.read_csv('dev_test.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "#sampling 50% of each dataset\n",
    "fraction = 0.5\n",
    "train_df = train_df.sample(frac=fraction, random_state=42)\n",
    "dev_test_df = dev_test_df.sample(frac=fraction, random_state=42)\n",
    "test_df = test_df.sample(frac=fraction, random_state=42)\n",
    "\n",
    "#function to prepare triplets\n",
    "def prepare_triplets(df):\n",
    "    triplets = []\n",
    "    labels = []\n",
    "    questions = df['question'].unique()\n",
    "    for q in questions:\n",
    "        sub_df = df[df['question'] == q]\n",
    "        positives = sub_df[sub_df['label'] == 1]['sentence text'].tolist()\n",
    "        negatives = sub_df[sub_df['label'] == 0]['sentence text'].tolist()\n",
    "\n",
    "        for pos in positives:\n",
    "            for neg in negatives:\n",
    "                triplets.append((q, pos, neg))\n",
    "                labels.append(1)  #positive pair\n",
    "                labels.append(0)  #negative pair\n",
    "    return triplets, labels\n",
    "\n",
    "#preparing triplets for training and dev_test datasets\n",
    "train_triplets, train_labels = prepare_triplets(train_df)\n",
    "dev_test_triplets, dev_test_labels = prepare_triplets(dev_test_df)\n",
    "test_triplets, test_labels = prepare_triplets(test_df)\n",
    "#preparing tokenizer and sequences\n",
    "all_text = train_df['question'].tolist() + train_df['sentence text'].tolist()\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above is completely the same as the first task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufNcap7IwAXR"
   },
   "outputs": [],
   "source": [
    "max_length = 150\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "def tokenize_and_pad(triplets):\n",
    "    anchor_texts = [triplet[0] for triplet in triplets]\n",
    "    positive_texts = [triplet[1] for triplet in triplets]\n",
    "    negative_texts = [triplet[2] for triplet in triplets]\n",
    "\n",
    "    anchor_seqs = tokenizer.texts_to_sequences(anchor_texts)\n",
    "    positive_seqs = tokenizer.texts_to_sequences(positive_texts)\n",
    "    negative_seqs = tokenizer.texts_to_sequences(negative_texts)\n",
    "\n",
    "    anchor_padded = pad_sequences(anchor_seqs, maxlen=max_length, padding='post')\n",
    "    positive_padded = pad_sequences(positive_seqs, maxlen=max_length, padding='post')\n",
    "    negative_padded = pad_sequences(negative_seqs, maxlen=max_length, padding='post')\n",
    "\n",
    "    return anchor_padded, positive_padded, negative_padded\n",
    "\n",
    "anchor_train, positive_train, negative_train = tokenize_and_pad(train_triplets)\n",
    "anchor_dev, positive_dev, negative_dev = tokenize_and_pad(dev_test_triplets)\n",
    "anchor_test, positive_test, negative_test = tokenize_and_pad(test_triplets)\n",
    "#splitting the data for cross-validation\n",
    "anchor_train, anchor_val, positive_train, positive_val, negative_train, negative_val = train_test_split(\n",
    "    anchor_train, positive_train, negative_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the cell above i've defined the fucntion to tokenize and pad the triplets i've set the length limit to be 150 after few manual trial and errors and it seems to be the optimal value, after that i've prepared the three datasets for model training and evaluation by tokenization and then padding the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i've defiend a Siamese network with an embedding layer and LSTM layers in teh cell below.\n",
    "- **Input Layer:** Defines the input shape for sequences.\n",
    "- **Embedding Layer:** Embeds input sequences into dense vectors of fixed size.\n",
    "- **LSTM Layer:** Processes the embedded sequences to capture temporal dependencies.\n",
    "- **Dense Layers:** Further process the LSTM outputs through fully connected layers with ReLU activations to form the shared network. This network is used to generate embeddings for anchor, positive, and negative inputs.\n",
    "- **Shared Network:** The shared network processes the anchor, positive, and negative inputs to produce their embeddings.\n",
    "- **Distance Calculation:** The custom `DistanceLayer` computes the distances between the embeddings of the anchor-positive and anchor-negative pairs.\n",
    "- **Model Definition:** The model takes anchor, positive, and negative inputs and outputs the distances computed by the DistanceLayer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sC1P05aZwAXS"
   },
   "outputs": [],
   "source": [
    "#distance layer\n",
    "class DistanceLayer(layers.Layer):\n",
    "    def _init_(self, **kwargs):\n",
    "        super()._init_(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        anchor, positive, negative = inputs\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), axis=1, keepdims=True)\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), axis=1, keepdims=True)\n",
    "        return ap_distance, an_distance\n",
    "\n",
    "#siamese network with embedding and LSTM layers\n",
    "def build_siamese_model(vocab_size, max_length, embedding_dim=35, lstm_units=64, hidden_size=128):\n",
    "    input = layers.Input(shape=(max_length,))\n",
    "\n",
    "    #shared network\n",
    "    x = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length)(input)\n",
    "    x = layers.LSTM(lstm_units)(x)\n",
    "    x = layers.Dense(hidden_size, activation='relu')(x)\n",
    "    x = layers.Dense(hidden_size, activation='relu')(x)\n",
    "    x = layers.Dense(hidden_size, activation='relu')(x)\n",
    "    shared_network = tf.keras.Model(inputs=input, outputs=x)\n",
    "\n",
    "    #inputs for anchor, positive and negative\n",
    "    anchor_input = layers.Input(shape=(max_length,), name='anchor_input')\n",
    "    positive_input = layers.Input(shape=(max_length,), name='positive_input')\n",
    "    negative_input = layers.Input(shape=(max_length,), name='negative_input')\n",
    "\n",
    "    #shared embeddings\n",
    "    anchor_embedding = shared_network(anchor_input)\n",
    "    positive_embedding = shared_network(positive_input)\n",
    "    negative_embedding = shared_network(negative_input)\n",
    "\n",
    "    #distance layer\n",
    "    ap_distance, an_distance = DistanceLayer()([anchor_embedding, positive_embedding, negative_embedding])\n",
    "\n",
    "    model = tf.keras.Model(inputs=[anchor_input, positive_input, negative_input], outputs=[ap_distance, an_distance])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below i've define the loss function which is same as the one used for the first model and then i prepared the data by slicing the tokenized and padded datasets into batches of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whNU4uNXwAXS",
    "outputId": "ed2468e5-dc71-446b-8fd8-17881496ecde"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "#loss function\n",
    "def loss(margin=0.1):\n",
    "    def loss_function(y_true, y_pred):\n",
    "        ap_distance, an_distance = y_pred\n",
    "        return tf.maximum(ap_distance - an_distance + margin, 0)\n",
    "    return loss_function\n",
    "\n",
    "#converting triplet data into arrays for training\n",
    "def prepare_data_for_training(anchor, positive, negative):\n",
    "    return tf.data.Dataset.from_tensor_slices((anchor, positive, negative)).batch(32)\n",
    "\n",
    "train_dataset = prepare_data_for_training(anchor_train, positive_train, negative_train)\n",
    "val_dataset = prepare_data_for_training(anchor_val, positive_val, negative_val)\n",
    "test_dataset = prepare_data_for_training(anchor_test, positive_test, negative_test)\n",
    "#building and compile the model\n",
    "siamese_model = build_siamese_model(vocab_size, max_length, embedding_dim=35, lstm_units=64, hidden_size=128)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "siamese_model.compile(optimizer=optimizer, loss=loss(0.1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the selection of hyper parameters is based on trial and error i didn't use keras tuner because i was encountering crashes so all i could do was trying different commonly used values of the hyper parameters and use the one with the best result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the cell below is a custom training loop for the model with 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZvGZM8HEwAXS",
    "outputId": "3cb5619e-6e10-4363-ad9c-6cae7025d5db",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7b3d791909d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7b3d791909d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "#custom training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch}')\n",
    "\n",
    "    #training\n",
    "    for step, (anchor_batch, positive_batch, negative_batch) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            ap_distance, an_distance = siamese_model([anchor_batch, positive_batch, negative_batch], training=True)\n",
    "            loss_value = loss(0.1)(None, [ap_distance, an_distance])\n",
    "        grads = tape.gradient(loss_value, siamese_model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, siamese_model.trainable_weights))\n",
    "\n",
    "    #validation\n",
    "    val_losses = []\n",
    "    for step, (anchor_batch, positive_batch, negative_batch) in enumerate(val_dataset):\n",
    "        ap_distance, an_distance = siamese_model([anchor_batch, positive_batch, negative_batch], training=False)\n",
    "        val_loss_value = loss(0.1)(None, [ap_distance, an_distance])\n",
    "        val_losses.append(val_loss_value.numpy().flatten()[0])\n",
    "    val_loss = np.mean(val_losses)\n",
    "    print(f'Validation loss: {val_loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the cell below i've defined the function to compute the prediction which is the same as the one used for the first model and then i've evaluated the model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHjrXYrQwAXS",
    "outputId": "07a8fe6b-9b41-481c-919e-1da4807f0d57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.5001577784790154\n",
      "Validation Recall: 0.9491017964071856\n",
      "Validation F1 Score: 0.6550940276916718\n"
     ]
    }
   ],
   "source": [
    "#function to compute predictions\n",
    "def compute_predictions(anchor, positive, negative):\n",
    "    ap_distance, an_distance = siamese_model([anchor, positive, negative], training=False)\n",
    "    predictions = (ap_distance < an_distance).numpy().astype(int)\n",
    "    return predictions\n",
    "\n",
    "#evaluating on the validation set\n",
    "val_predictions = compute_predictions(anchor_val, positive_val, negative_val).flatten()\n",
    "val_y_true = dev_test_labels[:len(val_predictions)]\n",
    "\n",
    "precision = precision_score(val_y_true, val_predictions)\n",
    "recall = recall_score(val_y_true, val_predictions)\n",
    "f1 = f1_score(val_y_true, val_predictions)\n",
    "\n",
    "print(f\"Validation Precision: {precision}\")\n",
    "print(f\"Validation Recall: {recall}\")\n",
    "print(f\"Validation F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performance of model on the validation set is the following:\n",
    "\n",
    "Validation Precision: 0.5001577784790154\n",
    "\n",
    "Validation Recall: 0.9491017964071856\n",
    "\n",
    "Validation F1 Score: 0.6550940276916718\n",
    "\n",
    "which is again due to the complexity of the model and inadequacy of proper data is normal and can be accepted as a normal F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhYK_TRFwAXR",
    "outputId": "41da8e30-56ec-4387-a10f-8e97a884311c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision: 0.5205479452054794\n",
      "Test Recall: 0.5608856088560885\n",
      "Test F1 Score: 0.5399644760213144\n"
     ]
    }
   ],
   "source": [
    "#evaluating on the test set\n",
    "test_predictions = compute_predictions(anchor_test, positive_test, negative_test).flatten()\n",
    "test_y_true = test_labels[:len(test_predictions)]\n",
    "\n",
    "precision = precision_score(test_y_true, test_predictions)\n",
    "recall = recall_score(test_y_true, test_predictions)\n",
    "f1 = f1_score(test_y_true, test_predictions)\n",
    "\n",
    "print(f\"Test Precision: {precision}\")\n",
    "print(f\"Test Recall: {recall}\")\n",
    "print(f\"Test F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the results of the model perfomance on the test set are below:\n",
    "\n",
    "Test Precision: 0.5205479452054794\n",
    "\n",
    "Test Recall: 0.5608856088560885\n",
    "\n",
    "Test F1 Score: 0.5399644760213144\n",
    "\n",
    "which is lower than the model trained in the first task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn_summariser function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8AsknAowAXS",
    "outputId": "65a9bd59-b29f-41fe-e40d-5c6f334fdba4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question ID: 4109, Top Sentence IDs: [19]\n",
      "Question ID: 584, Top Sentence IDs: [2]\n",
      "Question ID: 1644, Top Sentence IDs: [48]\n",
      "Question ID: 3764, Top Sentence IDs: [8]\n",
      "Question ID: 3508, Top Sentence IDs: [4]\n",
      "Question ID: 991, Top Sentence IDs: [30]\n",
      "Question ID: 2401, Top Sentence IDs: [7]\n",
      "Question ID: 1999, Top Sentence IDs: [3]\n",
      "Question ID: 937, Top Sentence IDs: [6]\n",
      "Question ID: 2125, Top Sentence IDs: [1]\n",
      "Question ID: 1606, Top Sentence IDs: [49]\n",
      "Question ID: 2531, Top Sentence IDs: [9]\n",
      "Question ID: 2938, Top Sentence IDs: [1]\n",
      "Question ID: 671, Top Sentence IDs: [31]\n",
      "Question ID: 1922, Top Sentence IDs: [1]\n",
      "Question ID: 1754, Top Sentence IDs: [15]\n",
      "Question ID: 1449, Top Sentence IDs: [7]\n",
      "Question ID: 1852, Top Sentence IDs: [26]\n",
      "Question ID: 2092, Top Sentence IDs: [12]\n",
      "Question ID: 3905, Top Sentence IDs: [12]\n",
      "Question ID: 643, Top Sentence IDs: [16]\n",
      "Question ID: 3987, Top Sentence IDs: [16]\n",
      "Question ID: 520, Top Sentence IDs: [5]\n",
      "Question ID: 1014, Top Sentence IDs: [12]\n",
      "Question ID: 1253, Top Sentence IDs: [11]\n",
      "Question ID: 1907, Top Sentence IDs: [0]\n",
      "Question ID: 605, Top Sentence IDs: [4]\n",
      "Question ID: 1480, Top Sentence IDs: [5]\n",
      "Question ID: 2772, Top Sentence IDs: [9]\n",
      "Question ID: 290, Top Sentence IDs: [8]\n",
      "Question ID: 329, Top Sentence IDs: [39]\n",
      "Question ID: 3719, Top Sentence IDs: [26]\n",
      "Question ID: 1766, Top Sentence IDs: [15]\n",
      "Question ID: 1537, Top Sentence IDs: [2]\n",
      "Question ID: 72, Top Sentence IDs: [4]\n",
      "Question ID: 4185, Top Sentence IDs: [22]\n",
      "Question ID: 1177, Top Sentence IDs: [2]\n",
      "Question ID: 978, Top Sentence IDs: [0]\n",
      "Question ID: 2786, Top Sentence IDs: [4]\n",
      "Question ID: 259, Top Sentence IDs: [4]\n",
      "Question ID: 1417, Top Sentence IDs: [1]\n",
      "Question ID: 396, Top Sentence IDs: [37]\n",
      "Question ID: 273, Top Sentence IDs: [7]\n",
      "Question ID: 1022, Top Sentence IDs: [18]\n",
      "Question ID: 3748, Top Sentence IDs: [16]\n",
      "Question ID: 806, Top Sentence IDs: [53]\n",
      "Question ID: 1527, Top Sentence IDs: [12]\n",
      "Question ID: 10, Top Sentence IDs: [21]\n",
      "Question ID: 4107, Top Sentence IDs: [43]\n",
      "Question ID: 1083, Top Sentence IDs: [8]\n",
      "Question ID: 371, Top Sentence IDs: [26]\n",
      "Question ID: 300, Top Sentence IDs: [5]\n",
      "Question ID: 785, Top Sentence IDs: [7]\n",
      "Question ID: 3401, Top Sentence IDs: [1]\n",
      "Question ID: 972, Top Sentence IDs: [58]\n",
      "Question ID: 1001, Top Sentence IDs: [35]\n",
      "Question ID: 3850, Top Sentence IDs: [30]\n",
      "Question ID: 2971, Top Sentence IDs: [9]\n",
      "Question ID: 4133, Top Sentence IDs: [2]\n",
      "Question ID: 1539, Top Sentence IDs: [115]\n",
      "Question ID: 3307, Top Sentence IDs: [30]\n",
      "Question ID: 854, Top Sentence IDs: [11]\n",
      "Question ID: 3943, Top Sentence IDs: [1]\n",
      "Question ID: 2021, Top Sentence IDs: [0]\n",
      "Question ID: 441, Top Sentence IDs: [4]\n",
      "Question ID: 1431, Top Sentence IDs: [1]\n",
      "Question ID: 3168, Top Sentence IDs: [1]\n",
      "Question ID: 1119, Top Sentence IDs: [5]\n",
      "Question ID: 1511, Top Sentence IDs: [2]\n",
      "Question ID: 1614, Top Sentence IDs: [11]\n",
      "Question ID: 3182, Top Sentence IDs: [0]\n",
      "Question ID: 1811, Top Sentence IDs: [5]\n",
      "Question ID: 2683, Top Sentence IDs: [0]\n",
      "Question ID: 1520, Top Sentence IDs: [43]\n",
      "Question ID: 1308, Top Sentence IDs: [3]\n",
      "Question ID: 129, Top Sentence IDs: [5]\n",
      "Question ID: 672, Top Sentence IDs: [31]\n",
      "Question ID: 1202, Top Sentence IDs: [0]\n",
      "Question ID: 895, Top Sentence IDs: [24]\n",
      "Question ID: 4085, Top Sentence IDs: [20]\n",
      "Question ID: 1112, Top Sentence IDs: [12]\n",
      "Question ID: 2966, Top Sentence IDs: [24]\n",
      "Question ID: 2472, Top Sentence IDs: [8]\n",
      "Question ID: 2501, Top Sentence IDs: [21]\n",
      "Question ID: 4127, Top Sentence IDs: [4]\n",
      "Question ID: 443, Top Sentence IDs: [8]\n",
      "Question ID: 505, Top Sentence IDs: [4]\n",
      "Question ID: 1282, Top Sentence IDs: [10]\n",
      "Question ID: 1373, Top Sentence IDs: [17]\n",
      "Question ID: 456, Top Sentence IDs: [13]\n",
      "Question ID: 4121, Top Sentence IDs: [7]\n",
      "Question ID: 2009, Top Sentence IDs: [3]\n",
      "Question ID: 2226, Top Sentence IDs: [1]\n",
      "Question ID: 82, Top Sentence IDs: [18]\n",
      "Question ID: 1296, Top Sentence IDs: [2]\n",
      "Question ID: 1716, Top Sentence IDs: [52]\n",
      "Question ID: 2253, Top Sentence IDs: [1]\n",
      "Question ID: 3565, Top Sentence IDs: [0]\n",
      "Question ID: 1603, Top Sentence IDs: [2]\n",
      "Question ID: 2478, Top Sentence IDs: [9]\n",
      "Question ID: 351, Top Sentence IDs: [7]\n",
      "Question ID: 3809, Top Sentence IDs: [43]\n",
      "Question ID: 1594, Top Sentence IDs: [9]\n",
      "Question ID: 2933, Top Sentence IDs: [3]\n",
      "Question ID: 2055, Top Sentence IDs: [11]\n",
      "Question ID: 1761, Top Sentence IDs: [31]\n",
      "Question ID: 3237, Top Sentence IDs: [1]\n",
      "Question ID: 1697, Top Sentence IDs: [3]\n",
      "Question ID: 3572, Top Sentence IDs: [2]\n",
      "Question ID: 1294, Top Sentence IDs: [8]\n",
      "Question ID: 384, Top Sentence IDs: [16]\n",
      "Question ID: 1665, Top Sentence IDs: [5]\n",
      "Question ID: 952, Top Sentence IDs: [3]\n",
      "Question ID: 3659, Top Sentence IDs: [5]\n",
      "Question ID: 1729, Top Sentence IDs: [12]\n",
      "Question ID: 135, Top Sentence IDs: [2]\n",
      "Question ID: 2552, Top Sentence IDs: [15]\n",
      "Question ID: 2474, Top Sentence IDs: [17]\n",
      "Question ID: 1828, Top Sentence IDs: [6]\n",
      "Question ID: 700, Top Sentence IDs: [10]\n",
      "Question ID: 47, Top Sentence IDs: [19]\n",
      "Question ID: 2386, Top Sentence IDs: [9]\n",
      "Question ID: 3160, Top Sentence IDs: [15]\n",
      "Question ID: 771, Top Sentence IDs: [14]\n",
      "Question ID: 1117, Top Sentence IDs: [0]\n",
      "Question ID: 1368, Top Sentence IDs: [22]\n",
      "Question ID: 3938, Top Sentence IDs: [33]\n",
      "Question ID: 2632, Top Sentence IDs: [6]\n",
      "Question ID: 1916, Top Sentence IDs: [20]\n",
      "Question ID: 3729, Top Sentence IDs: [1]\n",
      "Question ID: 2626, Top Sentence IDs: [0]\n",
      "Question ID: 2045, Top Sentence IDs: [3]\n",
      "Question ID: 1223, Top Sentence IDs: [8]\n",
      "Question ID: 3150, Top Sentence IDs: [0]\n",
      "Question ID: 4210, Top Sentence IDs: [34]\n",
      "Question ID: 2452, Top Sentence IDs: [12]\n",
      "Question ID: 2763, Top Sentence IDs: [6]\n",
      "Question ID: 2041, Top Sentence IDs: [11]\n",
      "Question ID: 285, Top Sentence IDs: [7]\n",
      "Question ID: 1823, Top Sentence IDs: [9]\n",
      "Question ID: 2229, Top Sentence IDs: [0]\n",
      "Question ID: 1222, Top Sentence IDs: [12]\n",
      "Question ID: 693, Top Sentence IDs: [23]\n",
      "Question ID: 713, Top Sentence IDs: [0]\n",
      "Question ID: 1486, Top Sentence IDs: [6]\n",
      "Question ID: 3596, Top Sentence IDs: [6]\n",
      "Question ID: 957, Top Sentence IDs: [15]\n",
      "Question ID: 686, Top Sentence IDs: [43]\n",
      "Question ID: 2342, Top Sentence IDs: [0]\n",
      "Question ID: 1568, Top Sentence IDs: [7]\n",
      "Question ID: 3249, Top Sentence IDs: [6]\n",
      "Question ID: 3433, Top Sentence IDs: [14]\n",
      "Question ID: 596, Top Sentence IDs: [11]\n",
      "Question ID: 145, Top Sentence IDs: [0]\n",
      "Question ID: 1203, Top Sentence IDs: [1]\n",
      "Question ID: 3969, Top Sentence IDs: [7]\n",
      "Question ID: 1337, Top Sentence IDs: [2]\n",
      "Question ID: 617, Top Sentence IDs: [6]\n",
      "Question ID: 2462, Top Sentence IDs: [11]\n",
      "Question ID: 593, Top Sentence IDs: [7]\n",
      "Question ID: 1429, Top Sentence IDs: [4]\n",
      "Question ID: 868, Top Sentence IDs: [4]\n",
      "Question ID: 2596, Top Sentence IDs: [1]\n",
      "Question ID: 3126, Top Sentence IDs: [22]\n",
      "Question ID: 2201, Top Sentence IDs: [14]\n",
      "Question ID: 4004, Top Sentence IDs: [38]\n",
      "Question ID: 1817, Top Sentence IDs: [40]\n",
      "Question ID: 1281, Top Sentence IDs: [13]\n",
      "Question ID: 720, Top Sentence IDs: [14]\n",
      "Question ID: 4132, Top Sentence IDs: [3]\n",
      "Question ID: 829, Top Sentence IDs: [8]\n",
      "Question ID: 408, Top Sentence IDs: [9]\n",
      "Question ID: 3714, Top Sentence IDs: [2]\n",
      "Question ID: 1323, Top Sentence IDs: [10]\n",
      "Question ID: 6, Top Sentence IDs: [6]\n",
      "Question ID: 1933, Top Sentence IDs: [21]\n",
      "Question ID: 3399, Top Sentence IDs: [1]\n",
      "Question ID: 2675, Top Sentence IDs: [13]\n",
      "Question ID: 2840, Top Sentence IDs: [9]\n",
      "Question ID: 2191, Top Sentence IDs: [0]\n",
      "Question ID: 1765, Top Sentence IDs: [5]\n",
      "Question ID: 115, Top Sentence IDs: [23]\n",
      "Question ID: 1218, Top Sentence IDs: [2]\n",
      "Question ID: 2448, Top Sentence IDs: [24]\n",
      "Question ID: 2480, Top Sentence IDs: [22]\n",
      "Question ID: 1617, Top Sentence IDs: [20]\n",
      "Question ID: 707, Top Sentence IDs: [7]\n",
      "Question ID: 3823, Top Sentence IDs: [5]\n",
      "Question ID: 1375, Top Sentence IDs: [1]\n",
      "Question ID: 1506, Top Sentence IDs: [15]\n",
      "Question ID: 876, Top Sentence IDs: [10]\n",
      "Question ID: 3845, Top Sentence IDs: [3]\n",
      "Question ID: 324, Top Sentence IDs: [16]\n",
      "Question ID: 2304, Top Sentence IDs: [0]\n",
      "Question ID: 119, Top Sentence IDs: [16]\n",
      "Question ID: 1340, Top Sentence IDs: [6]\n",
      "Question ID: 3772, Top Sentence IDs: [4]\n",
      "Question ID: 4167, Top Sentence IDs: [25]\n",
      "Question ID: 245, Top Sentence IDs: [0]\n",
      "Question ID: 110, Top Sentence IDs: [2]\n",
      "Question ID: 2245, Top Sentence IDs: [3]\n",
      "Question ID: 74, Top Sentence IDs: [5]\n",
      "Question ID: 1966, Top Sentence IDs: [6]\n",
      "Question ID: 2647, Top Sentence IDs: [0]\n",
      "Question ID: 1138, Top Sentence IDs: [21]\n",
      "Question ID: 677, Top Sentence IDs: [2]\n",
      "Question ID: 1418, Top Sentence IDs: [7]\n",
      "Question ID: 3390, Top Sentence IDs: [2]\n",
      "Question ID: 2090, Top Sentence IDs: [12]\n",
      "Question ID: 2496, Top Sentence IDs: [21]\n",
      "Question ID: 2375, Top Sentence IDs: [13]\n",
      "Question ID: 1200, Top Sentence IDs: [0]\n",
      "Question ID: 3805, Top Sentence IDs: [1]\n",
      "Question ID: 1653, Top Sentence IDs: [5]\n",
      "Question ID: 158, Top Sentence IDs: [1]\n",
      "Question ID: 3667, Top Sentence IDs: [21]\n",
      "Question ID: 1172, Top Sentence IDs: [1]\n",
      "Question ID: 1797, Top Sentence IDs: [13]\n",
      "Question ID: 1676, Top Sentence IDs: [0]\n",
      "Question ID: 2153, Top Sentence IDs: [20]\n",
      "Question ID: 215, Top Sentence IDs: [12]\n",
      "Question ID: 796, Top Sentence IDs: [0]\n",
      "Question ID: 996, Top Sentence IDs: [9]\n",
      "Question ID: 69, Top Sentence IDs: [13]\n",
      "Question ID: 3442, Top Sentence IDs: [0]\n",
      "Question ID: 1640, Top Sentence IDs: [33]\n",
      "Question ID: 2149, Top Sentence IDs: [0]\n",
      "Question ID: 4143, Top Sentence IDs: [2]\n",
      "Question ID: 2499, Top Sentence IDs: [21]\n",
      "Question ID: 2805, Top Sentence IDs: [0]\n",
      "Question ID: 2573, Top Sentence IDs: [0]\n",
      "Question ID: 116, Top Sentence IDs: [12]\n",
      "Question ID: 489, Top Sentence IDs: [14]\n",
      "Question ID: 3900, Top Sentence IDs: [3]\n",
      "Question ID: 760, Top Sentence IDs: [16]\n",
      "Question ID: 4119, Top Sentence IDs: [10]\n",
      "Question ID: 1728, Top Sentence IDs: [25]\n",
      "Question ID: 3574, Top Sentence IDs: [18]\n",
      "Question ID: 968, Top Sentence IDs: [5]\n",
      "Question ID: 601, Top Sentence IDs: [6]\n",
      "Question ID: 1672, Top Sentence IDs: [39]\n",
      "Question ID: 647, Top Sentence IDs: [43]\n",
      "Question ID: 252, Top Sentence IDs: [14]\n",
      "Question ID: 3254, Top Sentence IDs: [10]\n",
      "Question ID: 639, Top Sentence IDs: [27]\n",
      "Question ID: 2959, Top Sentence IDs: [0]\n",
      "Question ID: 1991, Top Sentence IDs: [16]\n",
      "Question ID: 1186, Top Sentence IDs: [5]\n",
      "Question ID: 2326, Top Sentence IDs: [5]\n",
      "Question ID: 3554, Top Sentence IDs: [5]\n",
      "Question ID: 3837, Top Sentence IDs: [1]\n",
      "Question ID: 2941, Top Sentence IDs: [2]\n",
      "Question ID: 2861, Top Sentence IDs: [0]\n",
      "Question ID: 3873, Top Sentence IDs: [2]\n",
      "Question ID: 3683, Top Sentence IDs: [0]\n",
      "Question ID: 1332, Top Sentence IDs: [1]\n",
      "Question ID: 1021, Top Sentence IDs: [6]\n",
      "Question ID: 3348, Top Sentence IDs: [1]\n",
      "Question ID: 3428, Top Sentence IDs: [1]\n",
      "Question ID: 2737, Top Sentence IDs: [1]\n",
      "Question ID: 3100, Top Sentence IDs: [0]\n",
      "Question ID: 36, Top Sentence IDs: [19]\n",
      "Question ID: 2123, Top Sentence IDs: [3]\n",
      "Question ID: 1727, Top Sentence IDs: [3]\n",
      "Question ID: 747, Top Sentence IDs: [13]\n",
      "Question ID: 4062, Top Sentence IDs: [9]\n",
      "Question ID: 823, Top Sentence IDs: [6]\n",
      "Question ID: 99, Top Sentence IDs: [0]\n",
      "Question ID: 2244, Top Sentence IDs: [18]\n",
      "Question ID: 1776, Top Sentence IDs: [50]\n",
      "Question ID: 687, Top Sentence IDs: [33]\n",
      "Question ID: 2139, Top Sentence IDs: [43]\n",
      "Question ID: 4150, Top Sentence IDs: [1]\n",
      "Question ID: 3392, Top Sentence IDs: [5]\n",
      "Question ID: 2267, Top Sentence IDs: [1]\n",
      "Question ID: 2237, Top Sentence IDs: [1]\n",
      "Question ID: 2426, Top Sentence IDs: [27]\n",
      "Question ID: 222, Top Sentence IDs: [0]\n",
      "Question ID: 61, Top Sentence IDs: [17]\n",
      "Question ID: 1392, Top Sentence IDs: [2]\n",
      "Question ID: 1835, Top Sentence IDs: [11]\n",
      "Question ID: 420, Top Sentence IDs: [1]\n",
      "Question ID: 899, Top Sentence IDs: [22]\n",
      "Question ID: 1352, Top Sentence IDs: [20]\n",
      "Question ID: 2236, Top Sentence IDs: [14]\n",
      "Question ID: 1100, Top Sentence IDs: [12]\n",
      "Question ID: 2992, Top Sentence IDs: [20]\n",
      "Question ID: 959, Top Sentence IDs: [10]\n",
      "Question ID: 3806, Top Sentence IDs: [32]\n",
      "Question ID: 692, Top Sentence IDs: [14]\n",
      "Question ID: 1199, Top Sentence IDs: [12]\n",
      "Question ID: 3513, Top Sentence IDs: [3]\n",
      "Question ID: 1803, Top Sentence IDs: [1]\n",
      "Question ID: 2187, Top Sentence IDs: [11]\n",
      "Question ID: 2374, Top Sentence IDs: [8]\n",
      "Question ID: 646, Top Sentence IDs: [12]\n",
      "Question ID: 1876, Top Sentence IDs: [0]\n",
      "Question ID: 2761, Top Sentence IDs: [0]\n",
      "Question ID: 1499, Top Sentence IDs: [20]\n",
      "Question ID: 1699, Top Sentence IDs: [6]\n",
      "Question ID: 1487, Top Sentence IDs: [8]\n",
      "Question ID: 2299, Top Sentence IDs: [0]\n",
      "Question ID: 3375, Top Sentence IDs: [4]\n",
      "Question ID: 2233, Top Sentence IDs: [0]\n",
      "Question ID: 1319, Top Sentence IDs: [15]\n",
      "Question ID: 3945, Top Sentence IDs: [13]\n",
      "Question ID: 286, Top Sentence IDs: [0]\n",
      "Question ID: 2000, Top Sentence IDs: [5]\n",
      "Question ID: 1234, Top Sentence IDs: [1]\n",
      "Question ID: 2434, Top Sentence IDs: [12]\n",
      "Question ID: 1509, Top Sentence IDs: [6]\n",
      "Question ID: 336, Top Sentence IDs: [21]\n",
      "Question ID: 182, Top Sentence IDs: [1]\n",
      "Question ID: 3494, Top Sentence IDs: [6]\n",
      "Question ID: 7, Top Sentence IDs: [7]\n",
      "Question ID: 2910, Top Sentence IDs: [5]\n",
      "Question ID: 2806, Top Sentence IDs: [3]\n",
      "Question ID: 1746, Top Sentence IDs: [0]\n",
      "Question ID: 1988, Top Sentence IDs: [3]\n",
      "Question ID: 634, Top Sentence IDs: [21]\n",
      "Question ID: 712, Top Sentence IDs: [0]\n",
      "Question ID: 4130, Top Sentence IDs: [3]\n",
      "Question ID: 2459, Top Sentence IDs: [0]\n",
      "Question ID: 979, Top Sentence IDs: [27]\n",
      "Question ID: 1216, Top Sentence IDs: [2]\n",
      "Question ID: 305, Top Sentence IDs: [26]\n",
      "Question ID: 1555, Top Sentence IDs: [3]\n",
      "Question ID: 3332, Top Sentence IDs: [0]\n",
      "Question ID: 3308, Top Sentence IDs: [5]\n",
      "Question ID: 1937, Top Sentence IDs: [2]\n",
      "Question ID: 1070, Top Sentence IDs: [20]\n",
      "Question ID: 4036, Top Sentence IDs: [0]\n",
      "Question ID: 4189, Top Sentence IDs: [26]\n",
      "Question ID: 1306, Top Sentence IDs: [14]\n",
      "Question ID: 1935, Top Sentence IDs: [0]\n",
      "Question ID: 804, Top Sentence IDs: [0]\n",
      "Question ID: 2120, Top Sentence IDs: [2]\n",
      "Question ID: 1656, Top Sentence IDs: [20]\n",
      "Question ID: 1156, Top Sentence IDs: [15]\n",
      "Question ID: 3638, Top Sentence IDs: [11]\n",
      "Question ID: 3370, Top Sentence IDs: [1]\n",
      "Question ID: 2208, Top Sentence IDs: [2]\n",
      "Question ID: 1779, Top Sentence IDs: [16]\n",
      "Question ID: 607, Top Sentence IDs: [2]\n",
      "Question ID: 3090, Top Sentence IDs: [15]\n",
      "Question ID: 4179, Top Sentence IDs: [9]\n",
      "Question ID: 1043, Top Sentence IDs: [0]\n",
      "Question ID: 3589, Top Sentence IDs: [8]\n",
      "Question ID: 1964, Top Sentence IDs: [3]\n",
      "Question ID: 3279, Top Sentence IDs: [4]\n",
      "Question ID: 2415, Top Sentence IDs: [16]\n",
      "Question ID: 1522, Top Sentence IDs: [6]\n",
      "Question ID: 2506, Top Sentence IDs: [44]\n",
      "Question ID: 4220, Top Sentence IDs: [0]\n",
      "Question ID: 2835, Top Sentence IDs: [6]\n",
      "Question ID: 4198, Top Sentence IDs: [5]\n",
      "Question ID: 3797, Top Sentence IDs: [3]\n",
      "Question ID: 1460, Top Sentence IDs: [4]\n",
      "Question ID: 122, Top Sentence IDs: [1]\n",
      "Question ID: 3533, Top Sentence IDs: [11]\n",
      "Question ID: 493, Top Sentence IDs: [5]\n",
      "Question ID: 725, Top Sentence IDs: [24]\n",
      "Question ID: 681, Top Sentence IDs: [0]\n",
      "Question ID: 1556, Top Sentence IDs: [10]\n",
      "Question ID: 1873, Top Sentence IDs: [3]\n",
      "Question ID: 45, Top Sentence IDs: [31]\n",
      "Question ID: 1957, Top Sentence IDs: [3]\n",
      "Question ID: 3469, Top Sentence IDs: [0]\n",
      "Question ID: 1358, Top Sentence IDs: [17]\n",
      "Question ID: 2764, Top Sentence IDs: [1]\n",
      "Question ID: 1141, Top Sentence IDs: [6]\n",
      "Question ID: 263, Top Sentence IDs: [9]\n",
      "Question ID: 2360, Top Sentence IDs: [3]\n",
      "Question ID: 2485, Top Sentence IDs: [12]\n",
      "Question ID: 2082, Top Sentence IDs: [3]\n",
      "Question ID: 4015, Top Sentence IDs: [7]\n",
      "Question ID: 2321, Top Sentence IDs: [5]\n",
      "Question ID: 1063, Top Sentence IDs: [13]\n",
      "Question ID: 3543, Top Sentence IDs: [0]\n",
      "Question ID: 3042, Top Sentence IDs: [5]\n",
      "Question ID: 2101, Top Sentence IDs: [15]\n",
      "Question ID: 3154, Top Sentence IDs: [3]\n",
      "Question ID: 270, Top Sentence IDs: [7]\n",
      "Question ID: 624, Top Sentence IDs: [2]\n",
      "Question ID: 848, Top Sentence IDs: [8]\n",
      "Question ID: 1287, Top Sentence IDs: [3]\n",
      "Question ID: 2836, Top Sentence IDs: [4]\n",
      "Question ID: 718, Top Sentence IDs: [8]\n",
      "Question ID: 2319, Top Sentence IDs: [7]\n",
      "Question ID: 2181, Top Sentence IDs: [2]\n",
      "Question ID: 1343, Top Sentence IDs: [6]\n",
      "Question ID: 1152, Top Sentence IDs: [0]\n",
      "Question ID: 1799, Top Sentence IDs: [7]\n",
      "Question ID: 2214, Top Sentence IDs: [5]\n",
      "Question ID: 3014, Top Sentence IDs: [3]\n",
      "Question ID: 163, Top Sentence IDs: [44]\n",
      "Question ID: 1162, Top Sentence IDs: [7]\n",
      "Question ID: 465, Top Sentence IDs: [12]\n",
      "Question ID: 180, Top Sentence IDs: [1]\n",
      "Question ID: 3878, Top Sentence IDs: [1]\n",
      "Question ID: 1736, Top Sentence IDs: [7]\n",
      "Question ID: 1513, Top Sentence IDs: [13]\n",
      "Question ID: 228, Top Sentence IDs: [12]\n",
      "Question ID: 2846, Top Sentence IDs: [0]\n",
      "Question ID: 2431, Top Sentence IDs: [2]\n",
      "Question ID: 2667, Top Sentence IDs: [3]\n",
      "Question ID: 1292, Top Sentence IDs: [7]\n",
      "Question ID: 1125, Top Sentence IDs: [3]\n",
      "Question ID: 2538, Top Sentence IDs: [5]\n",
      "Question ID: 888, Top Sentence IDs: [8]\n",
      "Question ID: 3971, Top Sentence IDs: [1]\n",
      "Question ID: 2783, Top Sentence IDs: [0]\n",
      "Question ID: 933, Top Sentence IDs: [0]\n",
      "Question ID: 2135, Top Sentence IDs: [4]\n",
      "Question ID: 4080, Top Sentence IDs: [21]\n",
      "Question ID: 3593, Top Sentence IDs: [7]\n",
      "Question ID: 1184, Top Sentence IDs: [1]\n",
      "Question ID: 4000, Top Sentence IDs: [0]\n",
      "Question ID: 376, Top Sentence IDs: [14]\n",
      "Question ID: 3725, Top Sentence IDs: [2]\n",
      "Question ID: 1268, Top Sentence IDs: [7]\n",
      "Question ID: 3044, Top Sentence IDs: [0]\n",
      "Question ID: 3321, Top Sentence IDs: [2]\n",
      "Question ID: 1624, Top Sentence IDs: [5]\n",
      "Question ID: 573, Top Sentence IDs: [1]\n",
      "Question ID: 3218, Top Sentence IDs: [16]\n",
      "Question ID: 2252, Top Sentence IDs: [13]\n",
      "Question ID: 2034, Top Sentence IDs: [0]\n",
      "Question ID: 2225, Top Sentence IDs: [8]\n",
      "Question ID: 811, Top Sentence IDs: [0]\n",
      "Question ID: 4035, Top Sentence IDs: [18]\n",
      "Question ID: 3289, Top Sentence IDs: [14]\n",
      "Question ID: 817, Top Sentence IDs: [5]\n",
      "Question ID: 106, Top Sentence IDs: [3]\n",
      "Question ID: 2656, Top Sentence IDs: [0]\n",
      "Question ID: 3450, Top Sentence IDs: [4]\n",
      "Question ID: 281, Top Sentence IDs: [2]\n",
      "Question ID: 3752, Top Sentence IDs: [11]\n",
      "Question ID: 3980, Top Sentence IDs: [7]\n",
      "Question ID: 2127, Top Sentence IDs: [2]\n",
      "Question ID: 4162, Top Sentence IDs: [5]\n",
      "Question ID: 783, Top Sentence IDs: [5]\n",
      "Question ID: 3410, Top Sentence IDs: [0]\n",
      "Question ID: 1682, Top Sentence IDs: [11]\n",
      "Question ID: 2532, Top Sentence IDs: [0]\n",
      "Question ID: 1286, Top Sentence IDs: [4]\n",
      "Question ID: 1290, Top Sentence IDs: [1]\n",
      "Question ID: 540, Top Sentence IDs: [15]\n",
      "Question ID: 1453, Top Sentence IDs: [18]\n",
      "Question ID: 1098, Top Sentence IDs: [22]\n",
      "Question ID: 2498, Top Sentence IDs: [8]\n",
      "Question ID: 1917, Top Sentence IDs: [9]\n",
      "Question ID: 2791, Top Sentence IDs: [1]\n",
      "Question ID: 3940, Top Sentence IDs: [0]\n",
      "Question ID: 1147, Top Sentence IDs: [13]\n",
      "Question ID: 2454, Top Sentence IDs: [1]\n",
      "Question ID: 35, Top Sentence IDs: [0]\n",
      "Question ID: 1424, Top Sentence IDs: [5]\n",
      "Question ID: 1523, Top Sentence IDs: [4]\n",
      "Question ID: 3409, Top Sentence IDs: [4]\n",
      "Question ID: 1692, Top Sentence IDs: [4]\n",
      "Question ID: 808, Top Sentence IDs: [4]\n",
      "Question ID: 1855, Top Sentence IDs: [1]\n",
      "Question ID: 1631, Top Sentence IDs: [5]\n",
      "Question ID: 3275, Top Sentence IDs: [1]\n",
      "Question ID: 1089, Top Sentence IDs: [3]\n",
      "Question ID: 296, Top Sentence IDs: [0]\n",
      "Question ID: 3317, Top Sentence IDs: [3]\n",
      "Question ID: 1092, Top Sentence IDs: [6]\n",
      "Question ID: 1871, Top Sentence IDs: [4]\n",
      "Question ID: 3870, Top Sentence IDs: [4]\n",
      "Question ID: 2422, Top Sentence IDs: [1]\n",
      "Question ID: 1354, Top Sentence IDs: [4]\n",
      "Question ID: 202, Top Sentence IDs: [3]\n",
      "Question ID: 2138, Top Sentence IDs: [13]\n",
      "Question ID: 3884, Top Sentence IDs: [2]\n",
      "Question ID: 3274, Top Sentence IDs: [9]\n",
      "Question ID: 2466, Top Sentence IDs: [15]\n",
      "Question ID: 1501, Top Sentence IDs: [20]\n",
      "Question ID: 1455, Top Sentence IDs: [2]\n",
      "Question ID: 2822, Top Sentence IDs: [1]\n",
      "Question ID: 3808, Top Sentence IDs: [1]\n",
      "Question ID: 258, Top Sentence IDs: [3]\n",
      "Question ID: 2339, Top Sentence IDs: [1]\n",
      "Question ID: 3492, Top Sentence IDs: [0]\n",
      "Question ID: 2182, Top Sentence IDs: [0]\n",
      "Question ID: 358, Top Sentence IDs: [19]\n",
      "Question ID: 2295, Top Sentence IDs: [1]\n",
      "Question ID: 4092, Top Sentence IDs: [4]\n",
      "Question ID: 2396, Top Sentence IDs: [1]\n",
      "Question ID: 1168, Top Sentence IDs: [21]\n",
      "Question ID: 2366, Top Sentence IDs: [9]\n",
      "Question ID: 3773, Top Sentence IDs: [5]\n",
      "Question ID: 2489, Top Sentence IDs: [5]\n",
      "Question ID: 1261, Top Sentence IDs: [2]\n",
      "Question ID: 3630, Top Sentence IDs: [1]\n",
      "Question ID: 2887, Top Sentence IDs: [2]\n",
      "Question ID: 2403, Top Sentence IDs: [0]\n",
      "Question ID: 2242, Top Sentence IDs: [1]\n",
      "Question ID: 3674, Top Sentence IDs: [3]\n",
      "Question ID: 2457, Top Sentence IDs: [4]\n",
      "Question ID: 2284, Top Sentence IDs: [0]\n",
      "Question ID: 393, Top Sentence IDs: [5]\n",
      "Question ID: 494, Top Sentence IDs: [8]\n",
      "Question ID: 3898, Top Sentence IDs: [3]\n",
      "Question ID: 2863, Top Sentence IDs: [17]\n",
      "Question ID: 1690, Top Sentence IDs: [21]\n",
      "Question ID: 1142, Top Sentence IDs: [3]\n",
      "Question ID: 2443, Top Sentence IDs: [7]\n",
      "Question ID: 3192, Top Sentence IDs: [2]\n",
      "Question ID: 547, Top Sentence IDs: [9]\n",
      "Question ID: 4018, Top Sentence IDs: [6]\n",
      "Question ID: 1561, Top Sentence IDs: [15]\n",
      "Question ID: 4155, Top Sentence IDs: [0]\n",
      "Question ID: 1251, Top Sentence IDs: [2]\n",
      "Question ID: 185, Top Sentence IDs: [4]\n",
      "Question ID: 3801, Top Sentence IDs: [3]\n",
      "Question ID: 3452, Top Sentence IDs: [2]\n",
      "Question ID: 1181, Top Sentence IDs: [0]\n",
      "Question ID: 167, Top Sentence IDs: [1]\n",
      "Question ID: 516, Top Sentence IDs: [18]\n",
      "Question ID: 2300, Top Sentence IDs: [6]\n",
      "Question ID: 3335, Top Sentence IDs: [7]\n",
      "Question ID: 1419, Top Sentence IDs: [1]\n",
      "Question ID: 1003, Top Sentence IDs: [7]\n",
      "Question ID: 1547, Top Sentence IDs: [7]\n",
      "Question ID: 1660, Top Sentence IDs: [16]\n",
      "Question ID: 3458, Top Sentence IDs: [5]\n",
      "Question ID: 434, Top Sentence IDs: [13]\n",
      "Question ID: 1920, Top Sentence IDs: [0]\n",
      "Question ID: 1393, Top Sentence IDs: [7]\n",
      "Question ID: 2180, Top Sentence IDs: [0]\n",
      "Question ID: 598, Top Sentence IDs: [11]\n",
      "Question ID: 916, Top Sentence IDs: [12]\n",
      "Question ID: 1391, Top Sentence IDs: [2]\n",
      "Question ID: 3790, Top Sentence IDs: [15]\n",
      "Question ID: 2110, Top Sentence IDs: [2]\n",
      "Question ID: 3607, Top Sentence IDs: [4]\n",
      "Question ID: 1053, Top Sentence IDs: [1]\n",
      "Question ID: 2405, Top Sentence IDs: [3]\n",
      "Question ID: 3782, Top Sentence IDs: [3]\n",
      "Question ID: 3304, Top Sentence IDs: [1]\n",
      "Question ID: 2046, Top Sentence IDs: [2]\n",
      "Question ID: 4067, Top Sentence IDs: [11]\n",
      "Question ID: 283, Top Sentence IDs: [2]\n",
      "Question ID: 3175, Top Sentence IDs: [1]\n",
      "Question ID: 3415, Top Sentence IDs: [6]\n",
      "Question ID: 3104, Top Sentence IDs: [1]\n",
      "Question ID: 579, Top Sentence IDs: [0]\n",
      "Question ID: 1072, Top Sentence IDs: [0]\n",
      "Question ID: 2734, Top Sentence IDs: [2]\n",
      "Question ID: 1123, Top Sentence IDs: [6]\n",
      "Question ID: 2065, Top Sentence IDs: [3]\n",
      "Question ID: 339, Top Sentence IDs: [12]\n",
      "Question ID: 1804, Top Sentence IDs: [1]\n",
      "Question ID: 2710, Top Sentence IDs: [2]\n",
      "Question ID: 3244, Top Sentence IDs: [2]\n",
      "Question ID: 1671, Top Sentence IDs: [3]\n",
      "Question ID: 2665, Top Sentence IDs: [2]\n",
      "Question ID: 2873, Top Sentence IDs: [0]\n",
      "Question ID: 2701, Top Sentence IDs: [3]\n",
      "Question ID: 3062, Top Sentence IDs: [2]\n",
      "Question ID: 2991, Top Sentence IDs: [6]\n",
      "Question ID: 739, Top Sentence IDs: [2]\n",
      "Question ID: 3737, Top Sentence IDs: [7]\n",
      "Question ID: 729, Top Sentence IDs: [0]\n",
      "Question ID: 3292, Top Sentence IDs: [1]\n",
      "Question ID: 3243, Top Sentence IDs: [10]\n",
      "Question ID: 3702, Top Sentence IDs: [2]\n",
      "Question ID: 3819, Top Sentence IDs: [0]\n",
      "Question ID: 3017, Top Sentence IDs: [4]\n",
      "Question ID: 1939, Top Sentence IDs: [2]\n",
      "Question ID: 3920, Top Sentence IDs: [7]\n",
      "Question ID: 141, Top Sentence IDs: [0]\n",
      "Question ID: 2406, Top Sentence IDs: [0]\n",
      "Question ID: 1403, Top Sentence IDs: [20]\n",
      "Question ID: 3743, Top Sentence IDs: [5]\n",
      "Question ID: 620, Top Sentence IDs: [7]\n",
      "Question ID: 1943, Top Sentence IDs: [15]\n",
      "Question ID: 2362, Top Sentence IDs: [3]\n",
      "Question ID: 578, Top Sentence IDs: [4]\n",
      "Question ID: 3595, Top Sentence IDs: [0]\n",
      "Question ID: 2541, Top Sentence IDs: [10]\n",
      "Question ID: 3688, Top Sentence IDs: [2]\n",
      "Question ID: 3111, Top Sentence IDs: [4]\n",
      "Question ID: 4010, Top Sentence IDs: [2]\n",
      "Question ID: 3497, Top Sentence IDs: [3]\n",
      "Question ID: 742, Top Sentence IDs: [2]\n",
      "Question ID: 3109, Top Sentence IDs: [3]\n",
      "Question ID: 4084, Top Sentence IDs: [0]\n",
      "Question ID: 3471, Top Sentence IDs: [2]\n",
      "Question ID: 3587, Top Sentence IDs: [2]\n",
      "Question ID: 3718, Top Sentence IDs: [18]\n",
      "Question ID: 2227, Top Sentence IDs: [4]\n",
      "Question ID: 2085, Top Sentence IDs: [0]\n",
      "Question ID: 2977, Top Sentence IDs: [2]\n",
      "Question ID: 3874, Top Sentence IDs: [1]\n",
      "Question ID: 4046, Top Sentence IDs: [6]\n",
      "Question ID: 2171, Top Sentence IDs: [7]\n",
      "Question ID: 1239, Top Sentence IDs: [4]\n",
      "Question ID: 380, Top Sentence IDs: [7]\n",
      "Question ID: 3610, Top Sentence IDs: [2]\n",
      "Question ID: 3966, Top Sentence IDs: [7]\n",
      "Question ID: 2477, Top Sentence IDs: [1]\n",
      "Question ID: 1196, Top Sentence IDs: [13]\n",
      "Question ID: 753, Top Sentence IDs: [10]\n",
      "Question ID: 2167, Top Sentence IDs: [2]\n",
      "Question ID: 1240, Top Sentence IDs: [0]\n",
      "Question ID: 2160, Top Sentence IDs: [8]\n",
      "Question ID: 4089, Top Sentence IDs: [1]\n",
      "Question ID: 2266, Top Sentence IDs: [1]\n",
      "Question ID: 120, Top Sentence IDs: [1]\n",
      "Question ID: 142, Top Sentence IDs: [1]\n",
      "Question ID: 3581, Top Sentence IDs: [0]\n",
      "Question ID: 279, Top Sentence IDs: [0]\n",
      "Question ID: 3728, Top Sentence IDs: [6]\n",
      "Question ID: 3722, Top Sentence IDs: [12]\n",
      "Question ID: 2838, Top Sentence IDs: [4]\n",
      "Question ID: 2388, Top Sentence IDs: [0]\n",
      "Question ID: 1413, Top Sentence IDs: [1]\n",
      "Question ID: 3418, Top Sentence IDs: [3]\n",
      "Question ID: 3327, Top Sentence IDs: [0]\n",
      "Question ID: 3633, Top Sentence IDs: [4]\n",
      "Question ID: 347, Top Sentence IDs: [3]\n",
      "Question ID: 4050, Top Sentence IDs: [16]\n",
      "Question ID: 3299, Top Sentence IDs: [3]\n",
      "Question ID: 4066, Top Sentence IDs: [2]\n",
      "Question ID: 2419, Top Sentence IDs: [8]\n",
      "Question ID: 3974, Top Sentence IDs: [4]\n",
      "Question ID: 665, Top Sentence IDs: [1]\n",
      "Question ID: 2018, Top Sentence IDs: [3]\n",
      "Question ID: 2767, Top Sentence IDs: [2]\n",
      "Question ID: 153, Top Sentence IDs: [9]\n",
      "Question ID: 4181, Top Sentence IDs: [0]\n",
      "Question ID: 2503, Top Sentence IDs: [6]\n",
      "Question ID: 542, Top Sentence IDs: [8]\n",
      "Question ID: 3996, Top Sentence IDs: [1]\n",
      "Question ID: 3482, Top Sentence IDs: [1]\n",
      "Question ID: 3835, Top Sentence IDs: [6]\n",
      "Question ID: 2106, Top Sentence IDs: [5]\n",
      "Question ID: 1048, Top Sentence IDs: [9]\n",
      "Question ID: 3152, Top Sentence IDs: [0]\n",
      "Question ID: 1452, Top Sentence IDs: [2]\n",
      "Question ID: 2052, Top Sentence IDs: [6]\n",
      "Question ID: 3807, Top Sentence IDs: [1]\n",
      "Question ID: 3514, Top Sentence IDs: [4]\n",
      "Question ID: 2650, Top Sentence IDs: [4]\n",
      "Question ID: 3282, Top Sentence IDs: [0]\n",
      "Question ID: 2159, Top Sentence IDs: [7]\n",
      "Question ID: 3463, Top Sentence IDs: [0]\n",
      "Question ID: 1521, Top Sentence IDs: [0]\n",
      "Question ID: 13, Top Sentence IDs: [5]\n",
      "Question ID: 892, Top Sentence IDs: [7]\n",
      "Question ID: 3933, Top Sentence IDs: [3]\n",
      "Question ID: 2730, Top Sentence IDs: [3]\n",
      "Question ID: 2794, Top Sentence IDs: [0]\n",
      "Question ID: 282, Top Sentence IDs: [2]\n",
      "Question ID: 1970, Top Sentence IDs: [5]\n",
      "Question ID: 560, Top Sentence IDs: [5]\n",
      "Question ID: 3086, Top Sentence IDs: [0]\n",
      "Question ID: 2206, Top Sentence IDs: [1]\n",
      "Question ID: 908, Top Sentence IDs: [0]\n",
      "Question ID: 2176, Top Sentence IDs: [0]\n",
      "Question ID: 1618, Top Sentence IDs: [5]\n",
      "Question ID: 835, Top Sentence IDs: [2]\n",
      "Question ID: 1782, Top Sentence IDs: [0]\n",
      "Question ID: 2808, Top Sentence IDs: [0]\n",
      "Question ID: 2588, Top Sentence IDs: [2]\n",
      "Question ID: 1851, Top Sentence IDs: [1]\n",
      "Question ID: 2702, Top Sentence IDs: [1]\n",
      "Question ID: 1710, Top Sentence IDs: [2]\n",
      "Question ID: 2305, Top Sentence IDs: [0]\n",
      "Question ID: 3281, Top Sentence IDs: [0]\n",
      "Question ID: 2142, Top Sentence IDs: [1]\n",
      "Question ID: 3163, Top Sentence IDs: [0]\n",
      "Question ID: 3468, Top Sentence IDs: [1]\n",
      "Question ID: 3381, Top Sentence IDs: [6]\n",
      "Question ID: 2722, Top Sentence IDs: [6]\n",
      "Question ID: 1247, Top Sentence IDs: [5]\n",
      "Question ID: 645, Top Sentence IDs: [3]\n",
      "Question ID: 3708, Top Sentence IDs: [0]\n",
      "Question ID: 3291, Top Sentence IDs: [0]\n",
      "Question ID: 3646, Top Sentence IDs: [2]\n",
      "Question ID: 2882, Top Sentence IDs: [2]\n",
      "Question ID: 2578, Top Sentence IDs: [4]\n",
      "Question ID: 1680, Top Sentence IDs: [2]\n",
      "Question ID: 2619, Top Sentence IDs: [0]\n",
      "Question ID: 3391, Top Sentence IDs: [1]\n",
      "Question ID: 1610, Top Sentence IDs: [13]\n",
      "Question ID: 3038, Top Sentence IDs: [4]\n",
      "Question ID: 3946, Top Sentence IDs: [1]\n",
      "Question ID: 3775, Top Sentence IDs: [0]\n",
      "Question ID: 3384, Top Sentence IDs: [5]\n",
      "Question ID: 2413, Top Sentence IDs: [1]\n",
      "Question ID: 1136, Top Sentence IDs: [3]\n",
      "Question ID: 1756, Top Sentence IDs: [4]\n",
      "Question ID: 3601, Top Sentence IDs: [3]\n",
      "Question ID: 3857, Top Sentence IDs: [3]\n",
      "Question ID: 857, Top Sentence IDs: [1]\n",
      "Question ID: 2099, Top Sentence IDs: [3]\n",
      "Question ID: 2768, Top Sentence IDs: [0]\n",
      "Question ID: 561, Top Sentence IDs: [6]\n",
      "Question ID: 3865, Top Sentence IDs: [0]\n",
      "Question ID: 2108, Top Sentence IDs: [0]\n",
      "Question ID: 3750, Top Sentence IDs: [0]\n",
      "Question ID: 2799, Top Sentence IDs: [0]\n",
      "Question ID: 2614, Top Sentence IDs: [3]\n",
      "Question ID: 1897, Top Sentence IDs: [0]\n",
      "Question ID: 722, Top Sentence IDs: [0]\n",
      "Question ID: 795, Top Sentence IDs: [0]\n",
      "Question ID: 4168, Top Sentence IDs: [1]\n",
      "Question ID: 391, Top Sentence IDs: [0]\n",
      "Question ID: 728, Top Sentence IDs: [3]\n",
      "Question ID: 2344, Top Sentence IDs: [2]\n",
      "Question ID: 1819, Top Sentence IDs: [4]\n",
      "Question ID: 311, Top Sentence IDs: [0]\n",
      "Question ID: 2849, Top Sentence IDs: [0]\n",
      "Question ID: 1673, Top Sentence IDs: [0]\n",
      "Question ID: 328, Top Sentence IDs: [3]\n",
      "Question ID: 2831, Top Sentence IDs: [1]\n",
      "Question ID: 3605, Top Sentence IDs: [5]\n",
      "Question ID: 1762, Top Sentence IDs: [1]\n",
      "Question ID: 1369, Top Sentence IDs: [6]\n",
      "Question ID: 2408, Top Sentence IDs: [3]\n",
      "Question ID: 2795, Top Sentence IDs: [0]\n",
      "Question ID: 1019, Top Sentence IDs: [9]\n",
      "Question ID: 2952, Top Sentence IDs: [2]\n",
      "Question ID: 2900, Top Sentence IDs: [2]\n",
      "Question ID: 844, Top Sentence IDs: [3]\n",
      "Question ID: 3700, Top Sentence IDs: [3]\n",
      "Question ID: 2550, Top Sentence IDs: [2]\n",
      "Question ID: 3222, Top Sentence IDs: [1]\n",
      "Question ID: 1315, Top Sentence IDs: [2]\n",
      "Question ID: 1249, Top Sentence IDs: [11]\n",
      "Question ID: 1066, Top Sentence IDs: [3]\n",
      "Question ID: 4146, Top Sentence IDs: [2]\n",
      "Question ID: 3498, Top Sentence IDs: [1]\n",
      "Question ID: 1328, Top Sentence IDs: [7]\n",
      "Question ID: 3416, Top Sentence IDs: [0]\n",
      "Question ID: 2310, Top Sentence IDs: [4]\n",
      "Question ID: 1948, Top Sentence IDs: [3]\n",
      "Question ID: 3411, Top Sentence IDs: [1]\n",
      "Question ID: 214, Top Sentence IDs: [0]\n",
      "Question ID: 1604, Top Sentence IDs: [0]\n",
      "Question ID: 3932, Top Sentence IDs: [4]\n",
      "Question ID: 869, Top Sentence IDs: [0]\n",
      "Question ID: 1558, Top Sentence IDs: [5]\n",
      "Question ID: 1985, Top Sentence IDs: [4]\n",
      "Question ID: 3457, Top Sentence IDs: [1]\n",
      "Question ID: 2905, Top Sentence IDs: [0]\n",
      "Question ID: 3694, Top Sentence IDs: [1]\n",
      "Question ID: 1093, Top Sentence IDs: [3]\n",
      "Question ID: 2916, Top Sentence IDs: [1]\n",
      "Question ID: 2314, Top Sentence IDs: [0]\n",
      "Question ID: 3345, Top Sentence IDs: [3]\n",
      "Question ID: 3021, Top Sentence IDs: [6]\n",
      "Question ID: 1772, Top Sentence IDs: [1]\n",
      "Question ID: 886, Top Sentence IDs: [1]\n",
      "Question ID: 2914, Top Sentence IDs: [0]\n",
      "Question ID: 2923, Top Sentence IDs: [0]\n",
      "Question ID: 2200, Top Sentence IDs: [0]\n",
      "Question ID: 3196, Top Sentence IDs: [0]\n",
      "Question ID: 3124, Top Sentence IDs: [3]\n",
      "Question ID: 4005, Top Sentence IDs: [1]\n",
      "Question ID: 117, Top Sentence IDs: [0]\n",
      "Question ID: 4208, Top Sentence IDs: [1]\n",
      "Question ID: 424, Top Sentence IDs: [10]\n",
      "Question ID: 1616, Top Sentence IDs: [1]\n",
      "Question ID: 1266, Top Sentence IDs: [1]\n",
      "Question ID: 4100, Top Sentence IDs: [1]\n",
      "Question ID: 4108, Top Sentence IDs: [0]\n",
      "Question ID: 3343, Top Sentence IDs: [3]\n",
      "Question ID: 528, Top Sentence IDs: [4]\n",
      "Question ID: 249, Top Sentence IDs: [3]\n",
      "Question ID: 2859, Top Sentence IDs: [0]\n",
      "Question ID: 3897, Top Sentence IDs: [0]\n",
      "Question ID: 3419, Top Sentence IDs: [0]\n",
      "Question ID: 2922, Top Sentence IDs: [0]\n",
      "Question ID: 3195, Top Sentence IDs: [1]\n",
      "Question ID: 2927, Top Sentence IDs: [1]\n",
      "Question ID: 3092, Top Sentence IDs: [0]\n",
      "Question ID: 3091, Top Sentence IDs: [0]\n",
      "Question ID: 2074, Top Sentence IDs: [1]\n",
      "Question ID: 2732, Top Sentence IDs: [0]\n",
      "Question ID: 3341, Top Sentence IDs: [3]\n",
      "Question ID: 3301, Top Sentence IDs: [3]\n",
      "Question ID: 1133, Top Sentence IDs: [1]\n",
      "Question ID: 1377, Top Sentence IDs: [1]\n",
      "Question ID: 1356, Top Sentence IDs: [2]\n",
      "Question ID: 940, Top Sentence IDs: [3]\n",
      "Question ID: 3901, Top Sentence IDs: [2]\n",
      "Question ID: 2990, Top Sentence IDs: [1]\n",
      "Question ID: 3194, Top Sentence IDs: [0]\n",
      "Question ID: 698, Top Sentence IDs: [4]\n",
      "Question ID: 1638, Top Sentence IDs: [1]\n",
      "Question ID: 3539, Top Sentence IDs: [1]\n",
      "Question ID: 1004, Top Sentence IDs: [6]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def nn_summariser(csvfile, questionids, n=1):\n",
    "    df = pd.read_csv(csvfile)\n",
    "\n",
    "    #tokenizing and padding sequences\n",
    "    def vectorize_texts(texts, tokenizer, max_length):\n",
    "        sequences = tokenizer.texts_to_sequences(texts)\n",
    "        padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_length)\n",
    "        return np.array(padded_sequences)\n",
    "\n",
    "    #preparing the tokenizer\n",
    "    all_text = df['question'].tolist() + df['sentence text'].tolist()\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(all_text)\n",
    "\n",
    "    results = []\n",
    "    for qid in questionids:\n",
    "        sub_df = df[df['qid'] == qid]\n",
    "        question = sub_df['question'].iloc[0]\n",
    "        sentences = sub_df['sentence text'].tolist()\n",
    "        sent_ids = sub_df['sentid'].tolist()\n",
    "\n",
    "        question_vec = vectorize_texts([question], tokenizer, max_length)\n",
    "        sentence_vecs = vectorize_texts(sentences, tokenizer, max_length)\n",
    "\n",
    "        #ensuring the question vector is tiled to match the number of sentences\n",
    "        question_vec = np.tile(question_vec, (len(sentences), 1))\n",
    "\n",
    "        ap_distance, _ = siamese_model([question_vec, sentence_vecs, sentence_vecs])\n",
    "        scores = ap_distance.numpy().flatten()\n",
    "\n",
    "        top_n_indices = np.argsort(scores)[:n]\n",
    "        top_n_ids = [sent_ids[i] for i in top_n_indices]\n",
    "        results.append(top_n_ids)\n",
    "\n",
    "    return results\n",
    "\n",
    "#reporting final results using the test set\n",
    "test_question_ids = test_df['qid'].unique()\n",
    "test_results = nn_summariser('test.csv', test_question_ids, n=1)\n",
    "\n",
    "#printing the test results\n",
    "for qid, top_ids in zip(test_question_ids, test_results):\n",
    "    print(f\"Question ID: {qid}, Top Sentence IDs: {top_ids}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDs of the  n  sentences that have the highest prediction score in the given question are printed in the cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of the first two models\n",
    "\n",
    "The second model has a slightly lower F1 score in both datasets compared to the first model which is a simpler one, however despite the more complex nature of the second model the results are not guaranteed to elavate when using a more complex model. the performance of the models varies based on the compatiblity of the models with the dataset and the adequacy of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZi6cYi2wAXS"
   },
   "source": [
    "# Transformer\n",
    "\n",
    "Implement a simple Transformer neural network that is composed of the following layers:\n",
    "\n",
    "* Use BERT as feature extractor for each token.\n",
    "* A few of transformer encoder layers, hidden dimension 768. You need to determine how many layers to use between 1~3.\n",
    "* A few of transformer decoder layers, hidden dimension 768. You need to determine how many layers to use between 1~3.\n",
    "* 1 hidden layer with size 512.\n",
    "* The final output layer with one cell for binary classification to predict whether two inputs are related or not.\n",
    "\n",
    "Note that each input for this model should be a concatenation of a positive pair (i.e. question + one answer) or a negative pair (i.e. question + not related sentence). The format is usually like [CLS]+ question + [SEP] + a positive/negative sentence.\n",
    "\n",
    "Train the model with the training data, use the dev_test set to determine a good size of the transformer layers, and report the final results using the test set. Again, remember to use the test set only after you have determined the optimal parameters of the transformer layers.\n",
    "\n",
    "At last based on my experiments, I will comment on whether this system is better than the systems developed in the previous tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### explaination how to handle length difference for a batch of data\n",
    "1. **Tokenizing with Padding and Attention Masks:** Ensuring all sequences in a batch are of the same length using padding and generate attention masks.\n",
    "2. **Prepare TensorFlow Datasets:** Converting tokenized inputs and attention masks into TensorFlow datasets.\n",
    "3. **Modify the Model:** Updating the model to use attention masks during the forward pass.\n",
    "4. **Train and Evaluate:** Training and evaluating the model using the prepared datasets.\n",
    "\n",
    "This approach ensures that the Transformer model can handle varying sequence lengths within batches effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310,
     "referenced_widgets": [
      "4d12dfae5e1a4763b97177b357910fa2",
      "56c66a0babf640fd8154a57117900b35",
      "d6bb559db51c4b7cae4bbf614770a998",
      "054d4181c75c4f2591bade77c59d85bc",
      "75cae6e71fe7401ca6bd7ad2a7f29898",
      "3abba6091c2d41d0a13fe90e7f3f4856",
      "b88d5ca5a93e4347bd2478daa29eaf57",
      "0444416527884e00b90c86c101b091f5",
      "6290aa3c8fd54e31a4882f310fa79a13",
      "3425e611797246c68f89a6814c2530c8",
      "ee2e186046ba4e60bfda72ca591d1eda",
      "5792530fe37d4b98b0086ca7615de3d0",
      "a02fb90a9a7e4cb28054ade611e0bd0a",
      "bc1c255da4da427b9dde13d03b521723",
      "bef88ee8751742e09428cd4e5eda2858",
      "5b9a6e8edaa64eabb7a52531af050057",
      "9e3155e8e2d148439d3beb067185fb15",
      "85f0cb8c7e6e4caa90e71387e92c6eae",
      "4b8569d2771a424eb3719913cfcabb1b",
      "a2f439cebf784a53945df264ec06de36",
      "af4622f7bf2e480b98e64a7a4617d8c4",
      "3e1c79df1c0e46a48320c55a9f6cee2d",
      "2dba21bb5a9946528c5d69e6eb1b1e1b",
      "0db4a2a2bf094dee9e002e9dae147309",
      "f2a91b41d5fa4fe190a67eedf9727d4b",
      "cceb9b7b02dd47bd9079f190981be812",
      "3d893880073446cbba37ffea1d977d0b",
      "94601e97d81a4e3da9542a9a6c72c2e8",
      "e3c622f85d644a739152f3853e253e23",
      "e9ea0cf2949a444a95e3621c44bee6e6",
      "90cce431b57340da868ac01871b0562e",
      "53970f77d3ac4cbaa7e3053350fcaf47",
      "d6a8e0b2db0c4de88bb3f6e4e15e0a4f",
      "ee02368a4fe84bd89dce8d849e6337ce",
      "48eb4c6fe1a04a8e9a710941d15bfc4b",
      "0ef74e3985a942668a3c44169ef85028",
      "870adf7941e6482e98e99cc2f8fe9959",
      "fa56be3896e442b6acdd09efdee53311",
      "4248a9a715054851afa1e60459e98245",
      "6f90cbff14bd45249cec792e62e8ec2f",
      "f27be381960045d1aedbcfdc485154f0",
      "9faf922e288d4fe5b641aa4b141774ca",
      "f36601979613464cbceffae0377b7c3c",
      "c63ec2bd9bae4ec79b301cb3cc5a1aa9"
     ]
    },
    "id": "qGFLphxRwAXS",
    "outputId": "39152e38-980c-485a-c79a-cd2a42efa521"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d12dfae5e1a4763b97177b357910fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5792530fe37d4b98b0086ca7615de3d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dba21bb5a9946528c5d69e6eb1b1e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee02368a4fe84bd89dce8d849e6337ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = pd.read_csv('training.csv')\n",
    "dev_test_df = pd.read_csv('dev_test.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "#function to sample a fraction of the dataset\n",
    "def sample_fraction(df, fraction):\n",
    "    return df.sample(frac=fraction, random_state=42)\n",
    "\n",
    "#sampling a quarter of each dataset\n",
    "fraction = 0.25\n",
    "train_df = sample_fraction(train_df, fraction)\n",
    "dev_test_df = sample_fraction(dev_test_df, fraction)\n",
    "test_df = sample_fraction(test_df, fraction)\n",
    "\n",
    "#function to prepare concatenated pairs\n",
    "def prepare_pairs(df):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    questions = df['question'].unique()\n",
    "    for q in questions:\n",
    "        sub_df = df[df['question'] == q]\n",
    "        positives = sub_df[sub_df['label'] == 1]['sentence text'].tolist()\n",
    "        negatives = sub_df[sub_df['label'] == 0]['sentence text'].tolist()\n",
    "\n",
    "        for pos in positives:\n",
    "            pairs.append(f\"[CLS] {q} [SEP] {pos} [SEP]\")\n",
    "            labels.append(1)  #positive pair\n",
    "        for neg in negatives:\n",
    "            pairs.append(f\"[CLS] {q} [SEP] {neg} [SEP]\")\n",
    "            labels.append(0)  #negative pair\n",
    "    return pairs, labels\n",
    "\n",
    "#preparing pairs for training, dev_test, and test datasets\n",
    "train_pairs, train_labels = prepare_pairs(train_df)\n",
    "dev_test_pairs, dev_test_labels = prepare_pairs(dev_test_df)\n",
    "test_pairs, test_labels = prepare_pairs(test_df)\n",
    "\n",
    "#tokenizing and padding sequences\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_length = 128  #based on BERT's max length\n",
    "\n",
    "def tokenize_pairs(pairs, max_length):\n",
    "    inputs = tokenizer(pairs, return_tensors='tf', max_length=max_length, padding=True, truncation=True)\n",
    "    return inputs\n",
    "\n",
    "train_inputs = tokenize_pairs(train_pairs, max_length)\n",
    "dev_test_inputs = tokenize_pairs(dev_test_pairs, max_length)\n",
    "test_inputs = tokenize_pairs(test_pairs, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above i've defined the function that prepares pairs of questions and sentences, along with their labels. For each unique question, it creates pairs with positive and negative sentences. [CLS] and [SEP] are special tokens used by BERT to indicate the start of the sequence and the separation between question and sentence, respectively. Moreover i've used the BertTokenizer function to tokenize the data and prepare it for training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWMr5ltgwAXS"
   },
   "outputs": [],
   "source": [
    "#custom Transformer layers\n",
    "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=hidden_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
    "            tf.keras.layers.Dense(hidden_dim),\n",
    "        ])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.attention(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TransformerDecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.attention1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=hidden_dim)\n",
    "        self.attention2 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=hidden_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
    "            tf.keras.layers.Dense(hidden_dim),\n",
    "        ])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, enc_outputs, training):\n",
    "        attn1 = self.attention1(inputs, inputs)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn1)\n",
    "        attn2 = self.attention2(out1, enc_outputs)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(out1 + attn2)\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        return self.layernorm3(out2 + ffn_output)\n",
    "\n",
    "class TransformerModel(tf.keras.Model):\n",
    "    def __init__(self, num_encoder_layers, num_decoder_layers, hidden_dim, ff_dim, hidden_layer_size, **kwargs):\n",
    "        super(TransformerModel, self).__init__(**kwargs)\n",
    "        self.bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "        self.encoder_layers = [TransformerEncoderLayer(hidden_dim, 8, ff_dim) for _ in range(num_encoder_layers)]\n",
    "        self.decoder_layers = [TransformerDecoderLayer(hidden_dim, 8, ff_dim) for _ in range(num_decoder_layers)]\n",
    "        self.hidden_layer = tf.keras.layers.Dense(hidden_layer_size, activation='relu')\n",
    "        self.output_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        bert_outputs = self.bert(inputs)[0]\n",
    "\n",
    "        enc_output = bert_outputs\n",
    "        for encoder in self.encoder_layers:\n",
    "            enc_output = encoder(enc_output, training=training)\n",
    "\n",
    "        dec_output = enc_output\n",
    "        for decoder in self.decoder_layers:\n",
    "            dec_output = decoder(dec_output, enc_output, training=training)\n",
    "\n",
    "        hidden_output = self.hidden_layer(tf.reduce_mean(dec_output, axis=1))\n",
    "        output = self.output_layer(hidden_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i've defined a custom Transformer model in TensorFlow using Keras. The model includes custom Transformer encoder and decoder layers, as well as a complete Transformer model that integrates a pre-trained BERT model.\n",
    "\n",
    "\n",
    "### Custom Transformer Encoder Layer\n",
    "\n",
    "- **MultiHeadAttention:** Applies multi-head self-attention to the input.\n",
    "- **Feed-Forward Network (FFN):** A two-layer feed-forward network with ReLU activation.\n",
    "- **Layer Normalization:** Normalizes the output of the previous layers.\n",
    "- **Dropout:** Adds regularization to prevent overfitting.\n",
    "- **Call Method:** Defines the forward pass. Applies multi-head attention, dropout, layer normalization, feed-forward network, another dropout, and layer normalization again.\n",
    "\n",
    "### Custom Transformer Decoder Layer\n",
    "\n",
    "- **Attention1:** Applies self-attention to the decoder input.\n",
    "- **Attention2:** Applies attention to the encoder outputs.\n",
    "- **Feed-Forward Network (FFN):** Same as in the encoder layer.\n",
    "- **Layer Normalization:** Normalizes the output of the previous layers.\n",
    "- **Dropout:** Adds regularization.\n",
    "- **Call Method:** Defines the forward pass. Applies self-attention, cross-attention with encoder outputs, feed-forward network, and layer normalization.\n",
    "\n",
    "### Custom Transformer Model\n",
    "\n",
    "- **BERT Model:** Initializes a pre-trained BERT model from Hugging Face.\n",
    "- **Encoder Layers:** A list of custom encoder layers.\n",
    "- **Decoder Layers:** A list of custom decoder layers.\n",
    "- **Hidden Layer:** A dense layer with ReLU activation.\n",
    "- **Output Layer:** A dense layer with sigmoid activation to produce the final output.\n",
    "- **Call Method:** Defines the forward pass. It first gets the outputs from the BERT model, passes them through the encoder layers, then through the decoder layers, averages the outputs, and finally passes them through the hidden and output layers to get the final result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704,
     "referenced_widgets": [
      "4e4f46138cfd48e281666c4e985e5c8c",
      "d88c0750a3a84736a6563020b29748e2",
      "b2a12af650824dd2b766663d0c041613",
      "fe14f2e7f80f4180b36e776bed173624",
      "d599664c058b4bf99f1b55c3effe9a6d",
      "c8f5f6b22eda41ffa9ee7127b1be8242",
      "f10c0682865f4d88b0e1c6372b1044b6",
      "1c68634d238e46008570190a02665c9e",
      "677c9336bd474401b93901b9ce441783",
      "b3682ea32ea94578bf96dfd70ccb9698",
      "77ad1a43314c4764b692318319240771"
     ]
    },
    "id": "g64WVmjNwAXS",
    "outputId": "3c02021e-813f-47de-83b0-4258e070d90b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4f46138cfd48e281666c4e985e5c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297/297 [==============================] - 744s 2s/step - loss: 0.6020 - accuracy: 0.7151 - val_loss: 0.9562 - val_accuracy: 0.3014\n",
      "Epoch 2/10\n",
      "297/297 [==============================] - 652s 2s/step - loss: 0.5890 - accuracy: 0.7172 - val_loss: 0.9712 - val_accuracy: 0.3598\n",
      "Epoch 3/10\n",
      "297/297 [==============================] - 651s 2s/step - loss: 0.5260 - accuracy: 0.7570 - val_loss: 1.2705 - val_accuracy: 0.4091\n",
      "Epoch 4/10\n",
      "297/297 [==============================] - 651s 2s/step - loss: 0.3944 - accuracy: 0.8294 - val_loss: 1.5147 - val_accuracy: 0.4911\n",
      "Epoch 5/10\n",
      "297/297 [==============================] - 666s 2s/step - loss: 0.3392 - accuracy: 0.8627 - val_loss: 2.0118 - val_accuracy: 0.4895\n",
      "Epoch 6/10\n",
      "297/297 [==============================] - 652s 2s/step - loss: 0.2792 - accuracy: 0.8880 - val_loss: 2.8032 - val_accuracy: 0.4524\n",
      "Epoch 7/10\n",
      "297/297 [==============================] - 651s 2s/step - loss: 0.2277 - accuracy: 0.9187 - val_loss: 2.2286 - val_accuracy: 0.5052\n",
      "Epoch 8/10\n",
      "297/297 [==============================] - 651s 2s/step - loss: 0.1360 - accuracy: 0.9536 - val_loss: 2.3600 - val_accuracy: 0.5203\n",
      "Epoch 9/10\n",
      "297/297 [==============================] - 651s 2s/step - loss: 0.1182 - accuracy: 0.9638 - val_loss: 2.4278 - val_accuracy: 0.4961\n",
      "Epoch 10/10\n",
      "297/297 [==============================] - 650s 2s/step - loss: 0.0755 - accuracy: 0.9772 - val_loss: 3.1710 - val_accuracy: 0.4179\n",
      "105/105 [==============================] - 72s 689ms/step - loss: 3.1236 - accuracy: 0.4292\n",
      "Test Accuracy: 0.4292101263999939\n",
      "105/105 [==============================] - 81s 688ms/step\n",
      "Precision: 0.3217291507268554\n",
      "Recall: 0.8555442522889115\n",
      "F1 Score: 0.4676118988045593\n"
     ]
    }
   ],
   "source": [
    "#defining the parameters for the layers\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2\n",
    "hidden_dim = 768\n",
    "ff_dim = 2048\n",
    "hidden_layer_size = 512\n",
    "\n",
    "# Instantiate and compile the model\n",
    "transformer_model = TransformerModel(num_encoder_layers, num_decoder_layers, hidden_dim, ff_dim, hidden_layer_size)\n",
    "transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#preparing data for training and validation\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs['input_ids'], train_labels)).batch(32)\n",
    "dev_test_dataset = tf.data.Dataset.from_tensor_slices((dev_test_inputs['input_ids'], dev_test_labels)).batch(32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_inputs['input_ids'], test_labels)).batch(32)\n",
    "\n",
    "#training the model\n",
    "epochs = 10\n",
    "transformer_model.fit(train_dataset, validation_data=dev_test_dataset, epochs=epochs)\n",
    "\n",
    "#evaluating on the test set\n",
    "test_loss, test_accuracy = transformer_model.evaluate(test_dataset)\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "#computing predictions\n",
    "test_predictions = transformer_model.predict(test_inputs['input_ids']).flatten()\n",
    "test_predictions = (test_predictions > 0.5).astype(int)\n",
    "\n",
    "#ground truth labels\n",
    "y_true = np.array(test_labels)\n",
    "\n",
    "#calculating precision, recall, and F1 score\n",
    "precision = precision_score(y_true, test_predictions)\n",
    "recall = recall_score(y_true, test_predictions)\n",
    "f1 = f1_score(y_true, test_predictions)\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htv7w3p5MnLY"
   },
   "source": [
    "In the cell above i've defined Parameters for the layers, keep that in mind that this model needed a lot of computing power and it was impossible for me to run the model many times. it took about 6 hours each time i wanted to train the model, so i've decided to chose the optimal hyperparameters after two trials and these are the values i've come up with. Moreover after model training i've evaluated the model using the test.csv file provided. The results of the model evaluation on the test set is low but it was expected since this model has a high level of complexity and the datasets provided are imbalance and not large enough to train this transformer model adequately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iaVpRb7MnaN"
   },
   "source": [
    "the final results of the model's performance on the test set is:\n",
    "\n",
    "Precision: 0.3217291507268554\n",
    "\n",
    "Recall: 0.8555442522889115\n",
    "\n",
    "F1 Score: 0.4676118988045593"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8zKTtwBMgEN"
   },
   "source": [
    "### nn_summariser function\n",
    "the function below returns the IDs of the n sentences that have the highest prediction score in the given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51RBby3owAXT",
    "outputId": "e454b1ee-0ef6-46dd-b897-340c3ba603a8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 9s 74ms/step\n",
      "2/2 [==============================] - 1s 110ms/step\n",
      "2/2 [==============================] - 1s 548ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "3/3 [==============================] - 1s 376ms/step\n",
      "1/1 [==============================] - 0s 313ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "2/2 [==============================] - 1s 413ms/step\n",
      "1/1 [==============================] - 0s 436ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "2/2 [==============================] - 1s 156ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "3/3 [==============================] - 1s 361ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 1s 530ms/step\n",
      "1/1 [==============================] - 0s 428ms/step\n",
      "2/2 [==============================] - 1s 178ms/step\n",
      "1/1 [==============================] - 0s 395ms/step\n",
      "2/2 [==============================] - 1s 295ms/step\n",
      "1/1 [==============================] - 0s 430ms/step\n",
      "1/1 [==============================] - 1s 515ms/step\n",
      "1/1 [==============================] - 0s 437ms/step\n",
      "2/2 [==============================] - 1s 90ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "1/1 [==============================] - 0s 283ms/step\n",
      "1/1 [==============================] - 0s 295ms/step\n",
      "5/5 [==============================] - 3s 593ms/step\n",
      "1/1 [==============================] - 0s 426ms/step\n",
      "1/1 [==============================] - 0s 459ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "2/2 [==============================] - 1s 79ms/step\n",
      "2/2 [==============================] - 1s 414ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "2/2 [==============================] - 1s 342ms/step\n",
      "2/2 [==============================] - 1s 417ms/step\n",
      "1/1 [==============================] - 0s 343ms/step\n",
      "1/1 [==============================] - 0s 259ms/step\n",
      "1/1 [==============================] - 0s 436ms/step\n",
      "2/2 [==============================] - 1s 498ms/step\n",
      "2/2 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 1s 540ms/step\n",
      "3/3 [==============================] - 1s 335ms/step\n",
      "2/2 [==============================] - 1s 149ms/step\n",
      "1/1 [==============================] - 0s 360ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "4/4 [==============================] - 2s 571ms/step\n",
      "2/2 [==============================] - 1s 133ms/step\n",
      "1/1 [==============================] - 1s 594ms/step\n",
      "1/1 [==============================] - 1s 562ms/step\n",
      "2/2 [==============================] - 1s 57ms/step\n",
      "5/5 [==============================] - 3s 553ms/step\n",
      "1/1 [==============================] - 1s 740ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "2/2 [==============================] - 1s 435ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 438ms/step\n",
      "1/1 [==============================] - 0s 252ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 448ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 484ms/step\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "2/2 [==============================] - 1s 324ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 404ms/step\n",
      "2/2 [==============================] - 1s 92ms/step\n",
      "1/1 [==============================] - 0s 312ms/step\n",
      "2/2 [==============================] - 1s 118ms/step\n",
      "1/1 [==============================] - 0s 398ms/step\n",
      "1/1 [==============================] - 0s 363ms/step\n",
      "2/2 [==============================] - 1s 54ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 393ms/step\n",
      "1/1 [==============================] - 0s 267ms/step\n",
      "1/1 [==============================] - 0s 283ms/step\n",
      "2/2 [==============================] - 1s 50ms/step\n",
      "2/2 [==============================] - 1s 316ms/step\n",
      "1/1 [==============================] - 0s 252ms/step\n",
      "1/1 [==============================] - 0s 360ms/step\n",
      "1/1 [==============================] - 0s 291ms/step\n",
      "1/1 [==============================] - 0s 332ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 1s 602ms/step\n",
      "1/1 [==============================] - 0s 359ms/step\n",
      "4/4 [==============================] - 2s 514ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 418ms/step\n",
      "1/1 [==============================] - 1s 532ms/step\n",
      "2/2 [==============================] - 1s 122ms/step\n",
      "2/2 [==============================] - 1s 245ms/step\n",
      "2/2 [==============================] - 1s 129ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "2/2 [==============================] - 1s 48ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 319ms/step\n",
      "2/2 [==============================] - 1s 78ms/step\n",
      "1/1 [==============================] - 0s 273ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 295ms/step\n",
      "1/1 [==============================] - 0s 274ms/step\n",
      "1/1 [==============================] - 0s 248ms/step\n",
      "1/1 [==============================] - 0s 437ms/step\n",
      "2/2 [==============================] - 1s 47ms/step\n",
      "1/1 [==============================] - 0s 315ms/step\n",
      "1/1 [==============================] - 0s 402ms/step\n",
      "1/1 [==============================] - 0s 247ms/step\n",
      "1/1 [==============================] - 0s 496ms/step\n",
      "1/1 [==============================] - 0s 362ms/step\n",
      "1/1 [==============================] - 0s 387ms/step\n",
      "2/2 [==============================] - 1s 269ms/step\n",
      "2/2 [==============================] - 1s 229ms/step\n",
      "1/1 [==============================] - 0s 290ms/step\n",
      "2/2 [==============================] - 1s 467ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 235ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2/2 [==============================] - 1s 213ms/step\n",
      "1/1 [==============================] - 0s 248ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 320ms/step\n",
      "1/1 [==============================] - 0s 334ms/step\n",
      "2/2 [==============================] - 1s 314ms/step\n",
      "2/2 [==============================] - 1s 54ms/step\n",
      "1/1 [==============================] - 0s 247ms/step\n",
      "2/2 [==============================] - 1s 218ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 274ms/step\n",
      "1/1 [==============================] - 0s 452ms/step\n",
      "2/2 [==============================] - 1s 295ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 314ms/step\n",
      "1/1 [==============================] - 0s 475ms/step\n",
      "1/1 [==============================] - 0s 227ms/step\n",
      "1/1 [==============================] - 1s 519ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 315ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 245ms/step\n",
      "1/1 [==============================] - 1s 585ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 296ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 473ms/step\n",
      "1/1 [==============================] - 0s 200ms/step\n",
      "3/3 [==============================] - 1s 334ms/step\n",
      "2/2 [==============================] - 1s 491ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "2/2 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 226ms/step\n",
      "1/1 [==============================] - 0s 201ms/step\n",
      "1/1 [==============================] - 0s 336ms/step\n",
      "1/1 [==============================] - 0s 362ms/step\n",
      "1/1 [==============================] - 1s 551ms/step\n",
      "1/1 [==============================] - 0s 489ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 268ms/step\n",
      "1/1 [==============================] - 0s 321ms/step\n",
      "1/1 [==============================] - 0s 246ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "2/2 [==============================] - 1s 319ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 419ms/step\n",
      "1/1 [==============================] - 0s 468ms/step\n",
      "1/1 [==============================] - 0s 322ms/step\n",
      "1/1 [==============================] - 0s 267ms/step\n",
      "1/1 [==============================] - 0s 233ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 282ms/step\n",
      "2/2 [==============================] - 1s 436ms/step\n",
      "1/1 [==============================] - 0s 214ms/step\n",
      "1/1 [==============================] - 0s 252ms/step\n",
      "1/1 [==============================] - 0s 295ms/step\n",
      "1/1 [==============================] - 0s 305ms/step\n",
      "2/2 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 243ms/step\n",
      "2/2 [==============================] - 1s 118ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "2/2 [==============================] - 1s 98ms/step\n",
      "1/1 [==============================] - 0s 272ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 420ms/step\n",
      "1/1 [==============================] - 0s 393ms/step\n",
      "2/2 [==============================] - 1s 144ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "2/2 [==============================] - 1s 206ms/step\n",
      "2/2 [==============================] - 1s 67ms/step\n",
      "1/1 [==============================] - 1s 570ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "2/2 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 403ms/step\n",
      "2/2 [==============================] - 1s 69ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 396ms/step\n",
      "1/1 [==============================] - 0s 374ms/step\n",
      "1/1 [==============================] - 1s 502ms/step\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 0s 322ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 409ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "2/2 [==============================] - 1s 211ms/step\n",
      "1/1 [==============================] - 1s 602ms/step\n",
      "2/2 [==============================] - 1s 234ms/step\n",
      "1/1 [==============================] - 0s 439ms/step\n",
      "1/1 [==============================] - 0s 249ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 439ms/step\n",
      "2/2 [==============================] - 1s 355ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "2/2 [==============================] - 1s 252ms/step\n",
      "2/2 [==============================] - 1s 359ms/step\n",
      "1/1 [==============================] - 1s 651ms/step\n",
      "2/2 [==============================] - 1s 386ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 483ms/step\n",
      "2/2 [==============================] - 1s 156ms/step\n",
      "2/2 [==============================] - 1s 514ms/step\n",
      "1/1 [==============================] - 0s 296ms/step\n",
      "1/1 [==============================] - 0s 365ms/step\n",
      "1/1 [==============================] - 1s 585ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 1s 622ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 290ms/step\n",
      "1/1 [==============================] - 0s 242ms/step\n",
      "1/1 [==============================] - 0s 349ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 402ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "2/2 [==============================] - 1s 67ms/step\n",
      "2/2 [==============================] - 1s 64ms/step\n",
      "1/1 [==============================] - 0s 417ms/step\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 442ms/step\n",
      "1/1 [==============================] - 0s 232ms/step\n",
      "2/2 [==============================] - 1s 94ms/step\n",
      "1/1 [==============================] - 0s 329ms/step\n",
      "2/2 [==============================] - 1s 71ms/step\n",
      "1/1 [==============================] - 1s 619ms/step\n",
      "1/1 [==============================] - 0s 456ms/step\n",
      "1/1 [==============================] - 1s 513ms/step\n",
      "1/1 [==============================] - 0s 248ms/step\n",
      "1/1 [==============================] - 0s 385ms/step\n",
      "2/2 [==============================] - 1s 538ms/step\n",
      "2/2 [==============================] - 1s 145ms/step\n",
      "2/2 [==============================] - 1s 357ms/step\n",
      "1/1 [==============================] - 1s 522ms/step\n",
      "1/1 [==============================] - 0s 310ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 362ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "2/2 [==============================] - 1s 419ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 307ms/step\n",
      "1/1 [==============================] - 0s 257ms/step\n",
      "1/1 [==============================] - 0s 477ms/step\n",
      "1/1 [==============================] - 0s 421ms/step\n",
      "1/1 [==============================] - 0s 318ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 438ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "2/2 [==============================] - 1s 132ms/step\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "1/1 [==============================] - 0s 396ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 1s 563ms/step\n",
      "1/1 [==============================] - 0s 442ms/step\n",
      "1/1 [==============================] - 0s 247ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 403ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 248ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "2/2 [==============================] - 1s 128ms/step\n",
      "1/1 [==============================] - 0s 270ms/step\n",
      "1/1 [==============================] - 0s 253ms/step\n",
      "1/1 [==============================] - 0s 362ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 1s 518ms/step\n",
      "1/1 [==============================] - 0s 241ms/step\n",
      "1/1 [==============================] - 1s 537ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 228ms/step\n",
      "1/1 [==============================] - 0s 390ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "2/2 [==============================] - 1s 99ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 214ms/step\n",
      "2/2 [==============================] - 1s 422ms/step\n",
      "1/1 [==============================] - 0s 398ms/step\n",
      "1/1 [==============================] - 1s 564ms/step\n",
      "1/1 [==============================] - 0s 334ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "2/2 [==============================] - 1s 130ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 1s 615ms/step\n",
      "1/1 [==============================] - 0s 375ms/step\n",
      "1/1 [==============================] - 0s 248ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 477ms/step\n",
      "1/1 [==============================] - 0s 333ms/step\n",
      "1/1 [==============================] - 0s 455ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 234ms/step\n",
      "1/1 [==============================] - 0s 269ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 379ms/step\n",
      "1/1 [==============================] - 0s 230ms/step\n",
      "1/1 [==============================] - 1s 522ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 396ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "2/2 [==============================] - 1s 248ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "1/1 [==============================] - 0s 421ms/step\n",
      "1/1 [==============================] - 0s 291ms/step\n",
      "1/1 [==============================] - 0s 359ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 268ms/step\n",
      "2/2 [==============================] - 1s 198ms/step\n",
      "2/2 [==============================] - 1s 358ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 187ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "2/2 [==============================] - 1s 86ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 361ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 291ms/step\n",
      "1/1 [==============================] - 0s 381ms/step\n",
      "1/1 [==============================] - 0s 338ms/step\n",
      "1/1 [==============================] - 0s 451ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 247ms/step\n",
      "1/1 [==============================] - 0s 441ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 270ms/step\n",
      "1/1 [==============================] - 0s 416ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 376ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 477ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 295ms/step\n",
      "2/2 [==============================] - 1s 240ms/step\n",
      "1/1 [==============================] - 0s 377ms/step\n",
      "1/1 [==============================] - 0s 349ms/step\n",
      "1/1 [==============================] - 1s 603ms/step\n",
      "1/1 [==============================] - 0s 397ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 247ms/step\n",
      "2/2 [==============================] - 1s 51ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 241ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 343ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "3/3 [==============================] - 1s 373ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 485ms/step\n",
      "1/1 [==============================] - 0s 228ms/step\n",
      "1/1 [==============================] - 0s 287ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "3/3 [==============================] - 1s 232ms/step\n",
      "1/1 [==============================] - 0s 393ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 444ms/step\n",
      "1/1 [==============================] - 0s 468ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 387ms/step\n",
      "1/1 [==============================] - 0s 280ms/step\n",
      "2/2 [==============================] - 1s 216ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 228ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 270ms/step\n",
      "1/1 [==============================] - 0s 249ms/step\n",
      "1/1 [==============================] - 0s 380ms/step\n",
      "1/1 [==============================] - 0s 254ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 247ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 394ms/step\n",
      "2/2 [==============================] - 1s 232ms/step\n",
      "2/2 [==============================] - 1s 130ms/step\n",
      "1/1 [==============================] - 1s 505ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 292ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 238ms/step\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "1/1 [==============================] - 0s 333ms/step\n",
      "1/1 [==============================] - 0s 378ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 228ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "2/2 [==============================] - 1s 72ms/step\n",
      "1/1 [==============================] - 0s 387ms/step\n",
      "1/1 [==============================] - 0s 280ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 258ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 440ms/step\n",
      "1/1 [==============================] - 0s 453ms/step\n",
      "1/1 [==============================] - 0s 442ms/step\n",
      "1/1 [==============================] - 0s 315ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 433ms/step\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "1/1 [==============================] - 0s 302ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "2/2 [==============================] - 1s 108ms/step\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 251ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 228ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 0s 237ms/step\n",
      "1/1 [==============================] - 0s 250ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 249ms/step\n",
      "1/1 [==============================] - 0s 439ms/step\n",
      "1/1 [==============================] - 0s 485ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 446ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 248ms/step\n",
      "1/1 [==============================] - 0s 233ms/step\n",
      "1/1 [==============================] - 0s 248ms/step\n",
      "1/1 [==============================] - 0s 246ms/step\n",
      "1/1 [==============================] - 0s 299ms/step\n",
      "1/1 [==============================] - 0s 311ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 317ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 321ms/step\n",
      "1/1 [==============================] - 1s 505ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 269ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 204ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 327ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 219ms/step\n",
      "1/1 [==============================] - 1s 525ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 379ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 369ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 203ms/step\n",
      "1/1 [==============================] - 0s 331ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 318ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 318ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 270ms/step\n",
      "1/1 [==============================] - 0s 350ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 437ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 450ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 283ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 1s 594ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 233ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 351ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 199ms/step\n",
      "1/1 [==============================] - 0s 311ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 234ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 222ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 379ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 240ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 314ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 357ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 324ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 1s 567ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 318ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 269ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 417ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "Question ID: 4109, Top Sentence IDs: [25]\n",
      "Question ID: 584, Top Sentence IDs: [4]\n",
      "Question ID: 1644, Top Sentence IDs: [2]\n",
      "Question ID: 3764, Top Sentence IDs: [5]\n",
      "Question ID: 3508, Top Sentence IDs: [3]\n",
      "Question ID: 991, Top Sentence IDs: [0]\n",
      "Question ID: 2401, Top Sentence IDs: [2]\n",
      "Question ID: 1999, Top Sentence IDs: [1]\n",
      "Question ID: 937, Top Sentence IDs: [2]\n",
      "Question ID: 2125, Top Sentence IDs: [2]\n",
      "Question ID: 1606, Top Sentence IDs: [46]\n",
      "Question ID: 2531, Top Sentence IDs: [13]\n",
      "Question ID: 2938, Top Sentence IDs: [3]\n",
      "Question ID: 671, Top Sentence IDs: [37]\n",
      "Question ID: 1922, Top Sentence IDs: [5]\n",
      "Question ID: 1754, Top Sentence IDs: [17]\n",
      "Question ID: 1449, Top Sentence IDs: [8]\n",
      "Question ID: 1852, Top Sentence IDs: [8]\n",
      "Question ID: 2092, Top Sentence IDs: [8]\n",
      "Question ID: 3905, Top Sentence IDs: [7]\n",
      "Question ID: 643, Top Sentence IDs: [13]\n",
      "Question ID: 3987, Top Sentence IDs: [27]\n",
      "Question ID: 520, Top Sentence IDs: [9]\n",
      "Question ID: 1014, Top Sentence IDs: [21]\n",
      "Question ID: 1253, Top Sentence IDs: [9]\n",
      "Question ID: 1907, Top Sentence IDs: [30]\n",
      "Question ID: 605, Top Sentence IDs: [0]\n",
      "Question ID: 1480, Top Sentence IDs: [1]\n",
      "Question ID: 2772, Top Sentence IDs: [13]\n",
      "Question ID: 290, Top Sentence IDs: [17]\n",
      "Question ID: 329, Top Sentence IDs: [33]\n",
      "Question ID: 3719, Top Sentence IDs: [4]\n",
      "Question ID: 1766, Top Sentence IDs: [18]\n",
      "Question ID: 1537, Top Sentence IDs: [4]\n",
      "Question ID: 72, Top Sentence IDs: [28]\n",
      "Question ID: 4185, Top Sentence IDs: [49]\n",
      "Question ID: 1177, Top Sentence IDs: [6]\n",
      "Question ID: 978, Top Sentence IDs: [4]\n",
      "Question ID: 2786, Top Sentence IDs: [6]\n",
      "Question ID: 259, Top Sentence IDs: [3]\n",
      "Question ID: 1417, Top Sentence IDs: [24]\n",
      "Question ID: 396, Top Sentence IDs: [1]\n",
      "Question ID: 273, Top Sentence IDs: [16]\n",
      "Question ID: 1022, Top Sentence IDs: [14]\n",
      "Question ID: 3748, Top Sentence IDs: [0]\n",
      "Question ID: 806, Top Sentence IDs: [52]\n",
      "Question ID: 1527, Top Sentence IDs: [37]\n",
      "Question ID: 10, Top Sentence IDs: [23]\n",
      "Question ID: 4107, Top Sentence IDs: [52]\n",
      "Question ID: 1083, Top Sentence IDs: [32]\n",
      "Question ID: 371, Top Sentence IDs: [13]\n",
      "Question ID: 300, Top Sentence IDs: [5]\n",
      "Question ID: 785, Top Sentence IDs: [1]\n",
      "Question ID: 3401, Top Sentence IDs: [0]\n",
      "Question ID: 972, Top Sentence IDs: [72]\n",
      "Question ID: 1001, Top Sentence IDs: [18]\n",
      "Question ID: 3850, Top Sentence IDs: [13]\n",
      "Question ID: 2971, Top Sentence IDs: [28]\n",
      "Question ID: 4133, Top Sentence IDs: [2]\n",
      "Question ID: 1539, Top Sentence IDs: [15]\n",
      "Question ID: 3307, Top Sentence IDs: [20]\n",
      "Question ID: 854, Top Sentence IDs: [11]\n",
      "Question ID: 3943, Top Sentence IDs: [5]\n",
      "Question ID: 2021, Top Sentence IDs: [8]\n",
      "Question ID: 441, Top Sentence IDs: [1]\n",
      "Question ID: 1431, Top Sentence IDs: [1]\n",
      "Question ID: 3168, Top Sentence IDs: [10]\n",
      "Question ID: 1119, Top Sentence IDs: [11]\n",
      "Question ID: 1511, Top Sentence IDs: [0]\n",
      "Question ID: 1614, Top Sentence IDs: [9]\n",
      "Question ID: 3182, Top Sentence IDs: [7]\n",
      "Question ID: 1811, Top Sentence IDs: [14]\n",
      "Question ID: 2683, Top Sentence IDs: [0]\n",
      "Question ID: 1520, Top Sentence IDs: [24]\n",
      "Question ID: 1308, Top Sentence IDs: [9]\n",
      "Question ID: 129, Top Sentence IDs: [11]\n",
      "Question ID: 672, Top Sentence IDs: [31]\n",
      "Question ID: 1202, Top Sentence IDs: [0]\n",
      "Question ID: 895, Top Sentence IDs: [5]\n",
      "Question ID: 4085, Top Sentence IDs: [16]\n",
      "Question ID: 1112, Top Sentence IDs: [4]\n",
      "Question ID: 2966, Top Sentence IDs: [2]\n",
      "Question ID: 2472, Top Sentence IDs: [0]\n",
      "Question ID: 2501, Top Sentence IDs: [18]\n",
      "Question ID: 4127, Top Sentence IDs: [14]\n",
      "Question ID: 443, Top Sentence IDs: [1]\n",
      "Question ID: 505, Top Sentence IDs: [5]\n",
      "Question ID: 1282, Top Sentence IDs: [14]\n",
      "Question ID: 1373, Top Sentence IDs: [17]\n",
      "Question ID: 456, Top Sentence IDs: [11]\n",
      "Question ID: 4121, Top Sentence IDs: [6]\n",
      "Question ID: 2009, Top Sentence IDs: [11]\n",
      "Question ID: 2226, Top Sentence IDs: [12]\n",
      "Question ID: 82, Top Sentence IDs: [0]\n",
      "Question ID: 1296, Top Sentence IDs: [12]\n",
      "Question ID: 1716, Top Sentence IDs: [7]\n",
      "Question ID: 2253, Top Sentence IDs: [2]\n",
      "Question ID: 3565, Top Sentence IDs: [0]\n",
      "Question ID: 1603, Top Sentence IDs: [20]\n",
      "Question ID: 2478, Top Sentence IDs: [2]\n",
      "Question ID: 351, Top Sentence IDs: [29]\n",
      "Question ID: 3809, Top Sentence IDs: [13]\n",
      "Question ID: 1594, Top Sentence IDs: [0]\n",
      "Question ID: 2933, Top Sentence IDs: [2]\n",
      "Question ID: 2055, Top Sentence IDs: [0]\n",
      "Question ID: 1761, Top Sentence IDs: [12]\n",
      "Question ID: 3237, Top Sentence IDs: [1]\n",
      "Question ID: 1697, Top Sentence IDs: [3]\n",
      "Question ID: 3572, Top Sentence IDs: [0]\n",
      "Question ID: 1294, Top Sentence IDs: [17]\n",
      "Question ID: 384, Top Sentence IDs: [21]\n",
      "Question ID: 1665, Top Sentence IDs: [2]\n",
      "Question ID: 952, Top Sentence IDs: [2]\n",
      "Question ID: 3659, Top Sentence IDs: [2]\n",
      "Question ID: 1729, Top Sentence IDs: [16]\n",
      "Question ID: 135, Top Sentence IDs: [1]\n",
      "Question ID: 2552, Top Sentence IDs: [19]\n",
      "Question ID: 2474, Top Sentence IDs: [0]\n",
      "Question ID: 1828, Top Sentence IDs: [29]\n",
      "Question ID: 700, Top Sentence IDs: [22]\n",
      "Question ID: 47, Top Sentence IDs: [17]\n",
      "Question ID: 2386, Top Sentence IDs: [10]\n",
      "Question ID: 3160, Top Sentence IDs: [13]\n",
      "Question ID: 771, Top Sentence IDs: [2]\n",
      "Question ID: 1117, Top Sentence IDs: [13]\n",
      "Question ID: 1368, Top Sentence IDs: [11]\n",
      "Question ID: 3938, Top Sentence IDs: [1]\n",
      "Question ID: 2632, Top Sentence IDs: [10]\n",
      "Question ID: 1916, Top Sentence IDs: [48]\n",
      "Question ID: 3729, Top Sentence IDs: [0]\n",
      "Question ID: 2626, Top Sentence IDs: [3]\n",
      "Question ID: 2045, Top Sentence IDs: [6]\n",
      "Question ID: 1223, Top Sentence IDs: [1]\n",
      "Question ID: 3150, Top Sentence IDs: [2]\n",
      "Question ID: 4210, Top Sentence IDs: [3]\n",
      "Question ID: 2452, Top Sentence IDs: [7]\n",
      "Question ID: 2763, Top Sentence IDs: [7]\n",
      "Question ID: 2041, Top Sentence IDs: [11]\n",
      "Question ID: 285, Top Sentence IDs: [0]\n",
      "Question ID: 1823, Top Sentence IDs: [9]\n",
      "Question ID: 2229, Top Sentence IDs: [21]\n",
      "Question ID: 1222, Top Sentence IDs: [7]\n",
      "Question ID: 693, Top Sentence IDs: [39]\n",
      "Question ID: 713, Top Sentence IDs: [0]\n",
      "Question ID: 1486, Top Sentence IDs: [8]\n",
      "Question ID: 3596, Top Sentence IDs: [14]\n",
      "Question ID: 957, Top Sentence IDs: [29]\n",
      "Question ID: 686, Top Sentence IDs: [13]\n",
      "Question ID: 2342, Top Sentence IDs: [0]\n",
      "Question ID: 1568, Top Sentence IDs: [5]\n",
      "Question ID: 3249, Top Sentence IDs: [24]\n",
      "Question ID: 3433, Top Sentence IDs: [0]\n",
      "Question ID: 596, Top Sentence IDs: [25]\n",
      "Question ID: 145, Top Sentence IDs: [4]\n",
      "Question ID: 1203, Top Sentence IDs: [6]\n",
      "Question ID: 3969, Top Sentence IDs: [0]\n",
      "Question ID: 1337, Top Sentence IDs: [2]\n",
      "Question ID: 617, Top Sentence IDs: [9]\n",
      "Question ID: 2462, Top Sentence IDs: [14]\n",
      "Question ID: 593, Top Sentence IDs: [23]\n",
      "Question ID: 1429, Top Sentence IDs: [1]\n",
      "Question ID: 868, Top Sentence IDs: [13]\n",
      "Question ID: 2596, Top Sentence IDs: [4]\n",
      "Question ID: 3126, Top Sentence IDs: [22]\n",
      "Question ID: 2201, Top Sentence IDs: [9]\n",
      "Question ID: 4004, Top Sentence IDs: [50]\n",
      "Question ID: 1817, Top Sentence IDs: [40]\n",
      "Question ID: 1281, Top Sentence IDs: [4]\n",
      "Question ID: 720, Top Sentence IDs: [0]\n",
      "Question ID: 4132, Top Sentence IDs: [1]\n",
      "Question ID: 829, Top Sentence IDs: [8]\n",
      "Question ID: 408, Top Sentence IDs: [6]\n",
      "Question ID: 3714, Top Sentence IDs: [0]\n",
      "Question ID: 1323, Top Sentence IDs: [10]\n",
      "Question ID: 6, Top Sentence IDs: [24]\n",
      "Question ID: 1933, Top Sentence IDs: [3]\n",
      "Question ID: 3399, Top Sentence IDs: [3]\n",
      "Question ID: 2675, Top Sentence IDs: [3]\n",
      "Question ID: 2840, Top Sentence IDs: [5]\n",
      "Question ID: 2191, Top Sentence IDs: [13]\n",
      "Question ID: 1765, Top Sentence IDs: [6]\n",
      "Question ID: 115, Top Sentence IDs: [44]\n",
      "Question ID: 1218, Top Sentence IDs: [14]\n",
      "Question ID: 2448, Top Sentence IDs: [7]\n",
      "Question ID: 2480, Top Sentence IDs: [15]\n",
      "Question ID: 1617, Top Sentence IDs: [3]\n",
      "Question ID: 707, Top Sentence IDs: [0]\n",
      "Question ID: 3823, Top Sentence IDs: [3]\n",
      "Question ID: 1375, Top Sentence IDs: [1]\n",
      "Question ID: 1506, Top Sentence IDs: [3]\n",
      "Question ID: 876, Top Sentence IDs: [8]\n",
      "Question ID: 3845, Top Sentence IDs: [4]\n",
      "Question ID: 324, Top Sentence IDs: [18]\n",
      "Question ID: 2304, Top Sentence IDs: [12]\n",
      "Question ID: 119, Top Sentence IDs: [16]\n",
      "Question ID: 1340, Top Sentence IDs: [0]\n",
      "Question ID: 3772, Top Sentence IDs: [13]\n",
      "Question ID: 4167, Top Sentence IDs: [21]\n",
      "Question ID: 245, Top Sentence IDs: [5]\n",
      "Question ID: 110, Top Sentence IDs: [7]\n",
      "Question ID: 2245, Top Sentence IDs: [0]\n",
      "Question ID: 74, Top Sentence IDs: [5]\n",
      "Question ID: 1966, Top Sentence IDs: [10]\n",
      "Question ID: 2647, Top Sentence IDs: [0]\n",
      "Question ID: 1138, Top Sentence IDs: [1]\n",
      "Question ID: 677, Top Sentence IDs: [12]\n",
      "Question ID: 1418, Top Sentence IDs: [22]\n",
      "Question ID: 3390, Top Sentence IDs: [4]\n",
      "Question ID: 2090, Top Sentence IDs: [7]\n",
      "Question ID: 2496, Top Sentence IDs: [1]\n",
      "Question ID: 2375, Top Sentence IDs: [7]\n",
      "Question ID: 1200, Top Sentence IDs: [2]\n",
      "Question ID: 3805, Top Sentence IDs: [1]\n",
      "Question ID: 1653, Top Sentence IDs: [17]\n",
      "Question ID: 158, Top Sentence IDs: [10]\n",
      "Question ID: 3667, Top Sentence IDs: [10]\n",
      "Question ID: 1172, Top Sentence IDs: [1]\n",
      "Question ID: 1797, Top Sentence IDs: [26]\n",
      "Question ID: 1676, Top Sentence IDs: [6]\n",
      "Question ID: 2153, Top Sentence IDs: [13]\n",
      "Question ID: 215, Top Sentence IDs: [2]\n",
      "Question ID: 796, Top Sentence IDs: [3]\n",
      "Question ID: 996, Top Sentence IDs: [9]\n",
      "Question ID: 69, Top Sentence IDs: [21]\n",
      "Question ID: 3442, Top Sentence IDs: [1]\n",
      "Question ID: 1640, Top Sentence IDs: [18]\n",
      "Question ID: 2149, Top Sentence IDs: [0]\n",
      "Question ID: 4143, Top Sentence IDs: [38]\n",
      "Question ID: 2499, Top Sentence IDs: [17]\n",
      "Question ID: 2805, Top Sentence IDs: [11]\n",
      "Question ID: 2573, Top Sentence IDs: [1]\n",
      "Question ID: 116, Top Sentence IDs: [10]\n",
      "Question ID: 489, Top Sentence IDs: [9]\n",
      "Question ID: 3900, Top Sentence IDs: [3]\n",
      "Question ID: 760, Top Sentence IDs: [13]\n",
      "Question ID: 4119, Top Sentence IDs: [9]\n",
      "Question ID: 1728, Top Sentence IDs: [0]\n",
      "Question ID: 3574, Top Sentence IDs: [16]\n",
      "Question ID: 968, Top Sentence IDs: [9]\n",
      "Question ID: 601, Top Sentence IDs: [21]\n",
      "Question ID: 1672, Top Sentence IDs: [27]\n",
      "Question ID: 647, Top Sentence IDs: [40]\n",
      "Question ID: 252, Top Sentence IDs: [10]\n",
      "Question ID: 3254, Top Sentence IDs: [25]\n",
      "Question ID: 639, Top Sentence IDs: [5]\n",
      "Question ID: 2959, Top Sentence IDs: [0]\n",
      "Question ID: 1991, Top Sentence IDs: [19]\n",
      "Question ID: 1186, Top Sentence IDs: [8]\n",
      "Question ID: 2326, Top Sentence IDs: [14]\n",
      "Question ID: 3554, Top Sentence IDs: [3]\n",
      "Question ID: 3837, Top Sentence IDs: [0]\n",
      "Question ID: 2941, Top Sentence IDs: [5]\n",
      "Question ID: 2861, Top Sentence IDs: [11]\n",
      "Question ID: 3873, Top Sentence IDs: [2]\n",
      "Question ID: 3683, Top Sentence IDs: [0]\n",
      "Question ID: 1332, Top Sentence IDs: [9]\n",
      "Question ID: 1021, Top Sentence IDs: [8]\n",
      "Question ID: 3348, Top Sentence IDs: [8]\n",
      "Question ID: 3428, Top Sentence IDs: [1]\n",
      "Question ID: 2737, Top Sentence IDs: [4]\n",
      "Question ID: 3100, Top Sentence IDs: [6]\n",
      "Question ID: 36, Top Sentence IDs: [1]\n",
      "Question ID: 2123, Top Sentence IDs: [9]\n",
      "Question ID: 1727, Top Sentence IDs: [9]\n",
      "Question ID: 747, Top Sentence IDs: [3]\n",
      "Question ID: 4062, Top Sentence IDs: [0]\n",
      "Question ID: 823, Top Sentence IDs: [11]\n",
      "Question ID: 99, Top Sentence IDs: [18]\n",
      "Question ID: 2244, Top Sentence IDs: [18]\n",
      "Question ID: 1776, Top Sentence IDs: [41]\n",
      "Question ID: 687, Top Sentence IDs: [2]\n",
      "Question ID: 2139, Top Sentence IDs: [45]\n",
      "Question ID: 4150, Top Sentence IDs: [7]\n",
      "Question ID: 3392, Top Sentence IDs: [15]\n",
      "Question ID: 2267, Top Sentence IDs: [9]\n",
      "Question ID: 2237, Top Sentence IDs: [8]\n",
      "Question ID: 2426, Top Sentence IDs: [17]\n",
      "Question ID: 222, Top Sentence IDs: [13]\n",
      "Question ID: 61, Top Sentence IDs: [38]\n",
      "Question ID: 1392, Top Sentence IDs: [6]\n",
      "Question ID: 1835, Top Sentence IDs: [8]\n",
      "Question ID: 420, Top Sentence IDs: [6]\n",
      "Question ID: 899, Top Sentence IDs: [8]\n",
      "Question ID: 1352, Top Sentence IDs: [18]\n",
      "Question ID: 2236, Top Sentence IDs: [7]\n",
      "Question ID: 1100, Top Sentence IDs: [0]\n",
      "Question ID: 2992, Top Sentence IDs: [4]\n",
      "Question ID: 959, Top Sentence IDs: [10]\n",
      "Question ID: 3806, Top Sentence IDs: [33]\n",
      "Question ID: 692, Top Sentence IDs: [7]\n",
      "Question ID: 1199, Top Sentence IDs: [12]\n",
      "Question ID: 3513, Top Sentence IDs: [5]\n",
      "Question ID: 1803, Top Sentence IDs: [5]\n",
      "Question ID: 2187, Top Sentence IDs: [16]\n",
      "Question ID: 2374, Top Sentence IDs: [2]\n",
      "Question ID: 646, Top Sentence IDs: [7]\n",
      "Question ID: 1876, Top Sentence IDs: [0]\n",
      "Question ID: 2761, Top Sentence IDs: [2]\n",
      "Question ID: 1499, Top Sentence IDs: [12]\n",
      "Question ID: 1699, Top Sentence IDs: [9]\n",
      "Question ID: 1487, Top Sentence IDs: [15]\n",
      "Question ID: 2299, Top Sentence IDs: [1]\n",
      "Question ID: 3375, Top Sentence IDs: [4]\n",
      "Question ID: 2233, Top Sentence IDs: [0]\n",
      "Question ID: 1319, Top Sentence IDs: [1]\n",
      "Question ID: 3945, Top Sentence IDs: [8]\n",
      "Question ID: 286, Top Sentence IDs: [16]\n",
      "Question ID: 2000, Top Sentence IDs: [5]\n",
      "Question ID: 1234, Top Sentence IDs: [10]\n",
      "Question ID: 2434, Top Sentence IDs: [12]\n",
      "Question ID: 1509, Top Sentence IDs: [5]\n",
      "Question ID: 336, Top Sentence IDs: [21]\n",
      "Question ID: 182, Top Sentence IDs: [1]\n",
      "Question ID: 3494, Top Sentence IDs: [6]\n",
      "Question ID: 7, Top Sentence IDs: [10]\n",
      "Question ID: 2910, Top Sentence IDs: [5]\n",
      "Question ID: 2806, Top Sentence IDs: [4]\n",
      "Question ID: 1746, Top Sentence IDs: [1]\n",
      "Question ID: 1988, Top Sentence IDs: [6]\n",
      "Question ID: 634, Top Sentence IDs: [25]\n",
      "Question ID: 712, Top Sentence IDs: [7]\n",
      "Question ID: 4130, Top Sentence IDs: [0]\n",
      "Question ID: 2459, Top Sentence IDs: [8]\n",
      "Question ID: 979, Top Sentence IDs: [27]\n",
      "Question ID: 1216, Top Sentence IDs: [9]\n",
      "Question ID: 305, Top Sentence IDs: [2]\n",
      "Question ID: 1555, Top Sentence IDs: [14]\n",
      "Question ID: 3332, Top Sentence IDs: [1]\n",
      "Question ID: 3308, Top Sentence IDs: [2]\n",
      "Question ID: 1937, Top Sentence IDs: [0]\n",
      "Question ID: 1070, Top Sentence IDs: [5]\n",
      "Question ID: 4036, Top Sentence IDs: [2]\n",
      "Question ID: 4189, Top Sentence IDs: [17]\n",
      "Question ID: 1306, Top Sentence IDs: [4]\n",
      "Question ID: 1935, Top Sentence IDs: [13]\n",
      "Question ID: 804, Top Sentence IDs: [13]\n",
      "Question ID: 2120, Top Sentence IDs: [3]\n",
      "Question ID: 1656, Top Sentence IDs: [6]\n",
      "Question ID: 1156, Top Sentence IDs: [15]\n",
      "Question ID: 3638, Top Sentence IDs: [7]\n",
      "Question ID: 3370, Top Sentence IDs: [1]\n",
      "Question ID: 2208, Top Sentence IDs: [7]\n",
      "Question ID: 1779, Top Sentence IDs: [12]\n",
      "Question ID: 607, Top Sentence IDs: [2]\n",
      "Question ID: 3090, Top Sentence IDs: [14]\n",
      "Question ID: 4179, Top Sentence IDs: [3]\n",
      "Question ID: 1043, Top Sentence IDs: [3]\n",
      "Question ID: 3589, Top Sentence IDs: [4]\n",
      "Question ID: 1964, Top Sentence IDs: [6]\n",
      "Question ID: 3279, Top Sentence IDs: [9]\n",
      "Question ID: 2415, Top Sentence IDs: [0]\n",
      "Question ID: 1522, Top Sentence IDs: [6]\n",
      "Question ID: 2506, Top Sentence IDs: [5]\n",
      "Question ID: 4220, Top Sentence IDs: [2]\n",
      "Question ID: 2835, Top Sentence IDs: [3]\n",
      "Question ID: 4198, Top Sentence IDs: [11]\n",
      "Question ID: 3797, Top Sentence IDs: [4]\n",
      "Question ID: 1460, Top Sentence IDs: [16]\n",
      "Question ID: 122, Top Sentence IDs: [0]\n",
      "Question ID: 3533, Top Sentence IDs: [9]\n",
      "Question ID: 493, Top Sentence IDs: [3]\n",
      "Question ID: 725, Top Sentence IDs: [50]\n",
      "Question ID: 681, Top Sentence IDs: [1]\n",
      "Question ID: 1556, Top Sentence IDs: [10]\n",
      "Question ID: 1873, Top Sentence IDs: [3]\n",
      "Question ID: 45, Top Sentence IDs: [29]\n",
      "Question ID: 1957, Top Sentence IDs: [11]\n",
      "Question ID: 3469, Top Sentence IDs: [2]\n",
      "Question ID: 1358, Top Sentence IDs: [20]\n",
      "Question ID: 2764, Top Sentence IDs: [1]\n",
      "Question ID: 1141, Top Sentence IDs: [2]\n",
      "Question ID: 263, Top Sentence IDs: [16]\n",
      "Question ID: 2360, Top Sentence IDs: [0]\n",
      "Question ID: 2485, Top Sentence IDs: [12]\n",
      "Question ID: 2082, Top Sentence IDs: [16]\n",
      "Question ID: 4015, Top Sentence IDs: [7]\n",
      "Question ID: 2321, Top Sentence IDs: [9]\n",
      "Question ID: 1063, Top Sentence IDs: [21]\n",
      "Question ID: 3543, Top Sentence IDs: [8]\n",
      "Question ID: 3042, Top Sentence IDs: [5]\n",
      "Question ID: 2101, Top Sentence IDs: [15]\n",
      "Question ID: 3154, Top Sentence IDs: [3]\n",
      "Question ID: 270, Top Sentence IDs: [9]\n",
      "Question ID: 624, Top Sentence IDs: [2]\n",
      "Question ID: 848, Top Sentence IDs: [9]\n",
      "Question ID: 1287, Top Sentence IDs: [4]\n",
      "Question ID: 2836, Top Sentence IDs: [2]\n",
      "Question ID: 718, Top Sentence IDs: [0]\n",
      "Question ID: 2319, Top Sentence IDs: [3]\n",
      "Question ID: 2181, Top Sentence IDs: [18]\n",
      "Question ID: 1343, Top Sentence IDs: [6]\n",
      "Question ID: 1152, Top Sentence IDs: [3]\n",
      "Question ID: 1799, Top Sentence IDs: [7]\n",
      "Question ID: 2214, Top Sentence IDs: [2]\n",
      "Question ID: 3014, Top Sentence IDs: [3]\n",
      "Question ID: 163, Top Sentence IDs: [28]\n",
      "Question ID: 1162, Top Sentence IDs: [0]\n",
      "Question ID: 465, Top Sentence IDs: [5]\n",
      "Question ID: 180, Top Sentence IDs: [16]\n",
      "Question ID: 3878, Top Sentence IDs: [11]\n",
      "Question ID: 1736, Top Sentence IDs: [5]\n",
      "Question ID: 1513, Top Sentence IDs: [10]\n",
      "Question ID: 228, Top Sentence IDs: [21]\n",
      "Question ID: 2846, Top Sentence IDs: [1]\n",
      "Question ID: 2431, Top Sentence IDs: [4]\n",
      "Question ID: 2667, Top Sentence IDs: [12]\n",
      "Question ID: 1292, Top Sentence IDs: [2]\n",
      "Question ID: 1125, Top Sentence IDs: [3]\n",
      "Question ID: 2538, Top Sentence IDs: [0]\n",
      "Question ID: 888, Top Sentence IDs: [9]\n",
      "Question ID: 3971, Top Sentence IDs: [1]\n",
      "Question ID: 2783, Top Sentence IDs: [4]\n",
      "Question ID: 933, Top Sentence IDs: [6]\n",
      "Question ID: 2135, Top Sentence IDs: [8]\n",
      "Question ID: 4080, Top Sentence IDs: [21]\n",
      "Question ID: 3593, Top Sentence IDs: [4]\n",
      "Question ID: 1184, Top Sentence IDs: [17]\n",
      "Question ID: 4000, Top Sentence IDs: [0]\n",
      "Question ID: 376, Top Sentence IDs: [1]\n",
      "Question ID: 3725, Top Sentence IDs: [1]\n",
      "Question ID: 1268, Top Sentence IDs: [15]\n",
      "Question ID: 3044, Top Sentence IDs: [0]\n",
      "Question ID: 3321, Top Sentence IDs: [1]\n",
      "Question ID: 1624, Top Sentence IDs: [12]\n",
      "Question ID: 573, Top Sentence IDs: [5]\n",
      "Question ID: 3218, Top Sentence IDs: [5]\n",
      "Question ID: 2252, Top Sentence IDs: [25]\n",
      "Question ID: 2034, Top Sentence IDs: [6]\n",
      "Question ID: 2225, Top Sentence IDs: [1]\n",
      "Question ID: 811, Top Sentence IDs: [12]\n",
      "Question ID: 4035, Top Sentence IDs: [39]\n",
      "Question ID: 3289, Top Sentence IDs: [8]\n",
      "Question ID: 817, Top Sentence IDs: [3]\n",
      "Question ID: 106, Top Sentence IDs: [0]\n",
      "Question ID: 2656, Top Sentence IDs: [0]\n",
      "Question ID: 3450, Top Sentence IDs: [6]\n",
      "Question ID: 281, Top Sentence IDs: [12]\n",
      "Question ID: 3752, Top Sentence IDs: [13]\n",
      "Question ID: 3980, Top Sentence IDs: [0]\n",
      "Question ID: 2127, Top Sentence IDs: [4]\n",
      "Question ID: 4162, Top Sentence IDs: [1]\n",
      "Question ID: 783, Top Sentence IDs: [9]\n",
      "Question ID: 3410, Top Sentence IDs: [0]\n",
      "Question ID: 1682, Top Sentence IDs: [6]\n",
      "Question ID: 2532, Top Sentence IDs: [0]\n",
      "Question ID: 1286, Top Sentence IDs: [7]\n",
      "Question ID: 1290, Top Sentence IDs: [1]\n",
      "Question ID: 540, Top Sentence IDs: [0]\n",
      "Question ID: 1453, Top Sentence IDs: [7]\n",
      "Question ID: 1098, Top Sentence IDs: [22]\n",
      "Question ID: 2498, Top Sentence IDs: [9]\n",
      "Question ID: 1917, Top Sentence IDs: [7]\n",
      "Question ID: 2791, Top Sentence IDs: [0]\n",
      "Question ID: 3940, Top Sentence IDs: [0]\n",
      "Question ID: 1147, Top Sentence IDs: [16]\n",
      "Question ID: 2454, Top Sentence IDs: [12]\n",
      "Question ID: 35, Top Sentence IDs: [9]\n",
      "Question ID: 1424, Top Sentence IDs: [15]\n",
      "Question ID: 1523, Top Sentence IDs: [4]\n",
      "Question ID: 3409, Top Sentence IDs: [1]\n",
      "Question ID: 1692, Top Sentence IDs: [4]\n",
      "Question ID: 808, Top Sentence IDs: [2]\n",
      "Question ID: 1855, Top Sentence IDs: [10]\n",
      "Question ID: 1631, Top Sentence IDs: [10]\n",
      "Question ID: 3275, Top Sentence IDs: [2]\n",
      "Question ID: 1089, Top Sentence IDs: [5]\n",
      "Question ID: 296, Top Sentence IDs: [11]\n",
      "Question ID: 3317, Top Sentence IDs: [8]\n",
      "Question ID: 1092, Top Sentence IDs: [6]\n",
      "Question ID: 1871, Top Sentence IDs: [14]\n",
      "Question ID: 3870, Top Sentence IDs: [0]\n",
      "Question ID: 2422, Top Sentence IDs: [1]\n",
      "Question ID: 1354, Top Sentence IDs: [0]\n",
      "Question ID: 202, Top Sentence IDs: [5]\n",
      "Question ID: 2138, Top Sentence IDs: [3]\n",
      "Question ID: 3884, Top Sentence IDs: [0]\n",
      "Question ID: 3274, Top Sentence IDs: [7]\n",
      "Question ID: 2466, Top Sentence IDs: [9]\n",
      "Question ID: 1501, Top Sentence IDs: [17]\n",
      "Question ID: 1455, Top Sentence IDs: [0]\n",
      "Question ID: 2822, Top Sentence IDs: [0]\n",
      "Question ID: 3808, Top Sentence IDs: [0]\n",
      "Question ID: 258, Top Sentence IDs: [3]\n",
      "Question ID: 2339, Top Sentence IDs: [5]\n",
      "Question ID: 3492, Top Sentence IDs: [0]\n",
      "Question ID: 2182, Top Sentence IDs: [8]\n",
      "Question ID: 358, Top Sentence IDs: [19]\n",
      "Question ID: 2295, Top Sentence IDs: [3]\n",
      "Question ID: 4092, Top Sentence IDs: [2]\n",
      "Question ID: 2396, Top Sentence IDs: [1]\n",
      "Question ID: 1168, Top Sentence IDs: [13]\n",
      "Question ID: 2366, Top Sentence IDs: [24]\n",
      "Question ID: 3773, Top Sentence IDs: [3]\n",
      "Question ID: 2489, Top Sentence IDs: [10]\n",
      "Question ID: 1261, Top Sentence IDs: [11]\n",
      "Question ID: 3630, Top Sentence IDs: [6]\n",
      "Question ID: 2887, Top Sentence IDs: [2]\n",
      "Question ID: 2403, Top Sentence IDs: [8]\n",
      "Question ID: 2242, Top Sentence IDs: [8]\n",
      "Question ID: 3674, Top Sentence IDs: [2]\n",
      "Question ID: 2457, Top Sentence IDs: [4]\n",
      "Question ID: 2284, Top Sentence IDs: [0]\n",
      "Question ID: 393, Top Sentence IDs: [0]\n",
      "Question ID: 494, Top Sentence IDs: [9]\n",
      "Question ID: 3898, Top Sentence IDs: [0]\n",
      "Question ID: 2863, Top Sentence IDs: [11]\n",
      "Question ID: 1690, Top Sentence IDs: [8]\n",
      "Question ID: 1142, Top Sentence IDs: [3]\n",
      "Question ID: 2443, Top Sentence IDs: [3]\n",
      "Question ID: 3192, Top Sentence IDs: [3]\n",
      "Question ID: 547, Top Sentence IDs: [6]\n",
      "Question ID: 4018, Top Sentence IDs: [5]\n",
      "Question ID: 1561, Top Sentence IDs: [7]\n",
      "Question ID: 4155, Top Sentence IDs: [8]\n",
      "Question ID: 1251, Top Sentence IDs: [9]\n",
      "Question ID: 185, Top Sentence IDs: [4]\n",
      "Question ID: 3801, Top Sentence IDs: [3]\n",
      "Question ID: 3452, Top Sentence IDs: [0]\n",
      "Question ID: 1181, Top Sentence IDs: [2]\n",
      "Question ID: 167, Top Sentence IDs: [1]\n",
      "Question ID: 516, Top Sentence IDs: [0]\n",
      "Question ID: 2300, Top Sentence IDs: [3]\n",
      "Question ID: 3335, Top Sentence IDs: [2]\n",
      "Question ID: 1419, Top Sentence IDs: [0]\n",
      "Question ID: 1003, Top Sentence IDs: [8]\n",
      "Question ID: 1547, Top Sentence IDs: [11]\n",
      "Question ID: 1660, Top Sentence IDs: [0]\n",
      "Question ID: 3458, Top Sentence IDs: [12]\n",
      "Question ID: 434, Top Sentence IDs: [17]\n",
      "Question ID: 1920, Top Sentence IDs: [9]\n",
      "Question ID: 1393, Top Sentence IDs: [15]\n",
      "Question ID: 2180, Top Sentence IDs: [6]\n",
      "Question ID: 598, Top Sentence IDs: [10]\n",
      "Question ID: 916, Top Sentence IDs: [1]\n",
      "Question ID: 1391, Top Sentence IDs: [1]\n",
      "Question ID: 3790, Top Sentence IDs: [9]\n",
      "Question ID: 2110, Top Sentence IDs: [3]\n",
      "Question ID: 3607, Top Sentence IDs: [0]\n",
      "Question ID: 1053, Top Sentence IDs: [1]\n",
      "Question ID: 2405, Top Sentence IDs: [0]\n",
      "Question ID: 3782, Top Sentence IDs: [4]\n",
      "Question ID: 3304, Top Sentence IDs: [2]\n",
      "Question ID: 2046, Top Sentence IDs: [1]\n",
      "Question ID: 4067, Top Sentence IDs: [9]\n",
      "Question ID: 283, Top Sentence IDs: [7]\n",
      "Question ID: 3175, Top Sentence IDs: [1]\n",
      "Question ID: 3415, Top Sentence IDs: [6]\n",
      "Question ID: 3104, Top Sentence IDs: [6]\n",
      "Question ID: 579, Top Sentence IDs: [0]\n",
      "Question ID: 1072, Top Sentence IDs: [0]\n",
      "Question ID: 2734, Top Sentence IDs: [0]\n",
      "Question ID: 1123, Top Sentence IDs: [7]\n",
      "Question ID: 2065, Top Sentence IDs: [10]\n",
      "Question ID: 339, Top Sentence IDs: [6]\n",
      "Question ID: 1804, Top Sentence IDs: [2]\n",
      "Question ID: 2710, Top Sentence IDs: [4]\n",
      "Question ID: 3244, Top Sentence IDs: [0]\n",
      "Question ID: 1671, Top Sentence IDs: [0]\n",
      "Question ID: 2665, Top Sentence IDs: [3]\n",
      "Question ID: 2873, Top Sentence IDs: [2]\n",
      "Question ID: 2701, Top Sentence IDs: [8]\n",
      "Question ID: 3062, Top Sentence IDs: [0]\n",
      "Question ID: 2991, Top Sentence IDs: [9]\n",
      "Question ID: 739, Top Sentence IDs: [14]\n",
      "Question ID: 3737, Top Sentence IDs: [0]\n",
      "Question ID: 729, Top Sentence IDs: [3]\n",
      "Question ID: 3292, Top Sentence IDs: [1]\n",
      "Question ID: 3243, Top Sentence IDs: [10]\n",
      "Question ID: 3702, Top Sentence IDs: [20]\n",
      "Question ID: 3819, Top Sentence IDs: [1]\n",
      "Question ID: 3017, Top Sentence IDs: [0]\n",
      "Question ID: 1939, Top Sentence IDs: [9]\n",
      "Question ID: 3920, Top Sentence IDs: [0]\n",
      "Question ID: 141, Top Sentence IDs: [5]\n",
      "Question ID: 2406, Top Sentence IDs: [3]\n",
      "Question ID: 1403, Top Sentence IDs: [20]\n",
      "Question ID: 3743, Top Sentence IDs: [3]\n",
      "Question ID: 620, Top Sentence IDs: [2]\n",
      "Question ID: 1943, Top Sentence IDs: [15]\n",
      "Question ID: 2362, Top Sentence IDs: [1]\n",
      "Question ID: 578, Top Sentence IDs: [8]\n",
      "Question ID: 3595, Top Sentence IDs: [1]\n",
      "Question ID: 2541, Top Sentence IDs: [4]\n",
      "Question ID: 3688, Top Sentence IDs: [8]\n",
      "Question ID: 3111, Top Sentence IDs: [12]\n",
      "Question ID: 4010, Top Sentence IDs: [1]\n",
      "Question ID: 3497, Top Sentence IDs: [0]\n",
      "Question ID: 742, Top Sentence IDs: [6]\n",
      "Question ID: 3109, Top Sentence IDs: [2]\n",
      "Question ID: 4084, Top Sentence IDs: [0]\n",
      "Question ID: 3471, Top Sentence IDs: [6]\n",
      "Question ID: 3587, Top Sentence IDs: [2]\n",
      "Question ID: 3718, Top Sentence IDs: [4]\n",
      "Question ID: 2227, Top Sentence IDs: [1]\n",
      "Question ID: 2085, Top Sentence IDs: [0]\n",
      "Question ID: 2977, Top Sentence IDs: [1]\n",
      "Question ID: 3874, Top Sentence IDs: [1]\n",
      "Question ID: 4046, Top Sentence IDs: [1]\n",
      "Question ID: 2171, Top Sentence IDs: [6]\n",
      "Question ID: 1239, Top Sentence IDs: [22]\n",
      "Question ID: 380, Top Sentence IDs: [3]\n",
      "Question ID: 3610, Top Sentence IDs: [2]\n",
      "Question ID: 3966, Top Sentence IDs: [1]\n",
      "Question ID: 2477, Top Sentence IDs: [2]\n",
      "Question ID: 1196, Top Sentence IDs: [2]\n",
      "Question ID: 753, Top Sentence IDs: [0]\n",
      "Question ID: 2167, Top Sentence IDs: [6]\n",
      "Question ID: 1240, Top Sentence IDs: [5]\n",
      "Question ID: 2160, Top Sentence IDs: [9]\n",
      "Question ID: 4089, Top Sentence IDs: [5]\n",
      "Question ID: 2266, Top Sentence IDs: [1]\n",
      "Question ID: 120, Top Sentence IDs: [0]\n",
      "Question ID: 142, Top Sentence IDs: [10]\n",
      "Question ID: 3581, Top Sentence IDs: [4]\n",
      "Question ID: 279, Top Sentence IDs: [2]\n",
      "Question ID: 3728, Top Sentence IDs: [11]\n",
      "Question ID: 3722, Top Sentence IDs: [11]\n",
      "Question ID: 2838, Top Sentence IDs: [4]\n",
      "Question ID: 2388, Top Sentence IDs: [0]\n",
      "Question ID: 1413, Top Sentence IDs: [5]\n",
      "Question ID: 3418, Top Sentence IDs: [2]\n",
      "Question ID: 3327, Top Sentence IDs: [5]\n",
      "Question ID: 3633, Top Sentence IDs: [1]\n",
      "Question ID: 347, Top Sentence IDs: [6]\n",
      "Question ID: 4050, Top Sentence IDs: [21]\n",
      "Question ID: 3299, Top Sentence IDs: [3]\n",
      "Question ID: 4066, Top Sentence IDs: [2]\n",
      "Question ID: 2419, Top Sentence IDs: [6]\n",
      "Question ID: 3974, Top Sentence IDs: [0]\n",
      "Question ID: 665, Top Sentence IDs: [1]\n",
      "Question ID: 2018, Top Sentence IDs: [7]\n",
      "Question ID: 2767, Top Sentence IDs: [5]\n",
      "Question ID: 153, Top Sentence IDs: [0]\n",
      "Question ID: 4181, Top Sentence IDs: [0]\n",
      "Question ID: 2503, Top Sentence IDs: [6]\n",
      "Question ID: 542, Top Sentence IDs: [7]\n",
      "Question ID: 3996, Top Sentence IDs: [2]\n",
      "Question ID: 3482, Top Sentence IDs: [1]\n",
      "Question ID: 3835, Top Sentence IDs: [0]\n",
      "Question ID: 2106, Top Sentence IDs: [4]\n",
      "Question ID: 1048, Top Sentence IDs: [9]\n",
      "Question ID: 3152, Top Sentence IDs: [0]\n",
      "Question ID: 1452, Top Sentence IDs: [0]\n",
      "Question ID: 2052, Top Sentence IDs: [1]\n",
      "Question ID: 3807, Top Sentence IDs: [1]\n",
      "Question ID: 3514, Top Sentence IDs: [2]\n",
      "Question ID: 2650, Top Sentence IDs: [6]\n",
      "Question ID: 3282, Top Sentence IDs: [1]\n",
      "Question ID: 2159, Top Sentence IDs: [6]\n",
      "Question ID: 3463, Top Sentence IDs: [0]\n",
      "Question ID: 1521, Top Sentence IDs: [4]\n",
      "Question ID: 13, Top Sentence IDs: [0]\n",
      "Question ID: 892, Top Sentence IDs: [6]\n",
      "Question ID: 3933, Top Sentence IDs: [2]\n",
      "Question ID: 2730, Top Sentence IDs: [12]\n",
      "Question ID: 2794, Top Sentence IDs: [0]\n",
      "Question ID: 282, Top Sentence IDs: [0]\n",
      "Question ID: 1970, Top Sentence IDs: [0]\n",
      "Question ID: 560, Top Sentence IDs: [3]\n",
      "Question ID: 3086, Top Sentence IDs: [2]\n",
      "Question ID: 2206, Top Sentence IDs: [6]\n",
      "Question ID: 908, Top Sentence IDs: [1]\n",
      "Question ID: 2176, Top Sentence IDs: [0]\n",
      "Question ID: 1618, Top Sentence IDs: [4]\n",
      "Question ID: 835, Top Sentence IDs: [3]\n",
      "Question ID: 1782, Top Sentence IDs: [0]\n",
      "Question ID: 2808, Top Sentence IDs: [1]\n",
      "Question ID: 2588, Top Sentence IDs: [0]\n",
      "Question ID: 1851, Top Sentence IDs: [5]\n",
      "Question ID: 2702, Top Sentence IDs: [2]\n",
      "Question ID: 1710, Top Sentence IDs: [4]\n",
      "Question ID: 2305, Top Sentence IDs: [0]\n",
      "Question ID: 3281, Top Sentence IDs: [2]\n",
      "Question ID: 2142, Top Sentence IDs: [1]\n",
      "Question ID: 3163, Top Sentence IDs: [0]\n",
      "Question ID: 3468, Top Sentence IDs: [11]\n",
      "Question ID: 3381, Top Sentence IDs: [4]\n",
      "Question ID: 2722, Top Sentence IDs: [4]\n",
      "Question ID: 1247, Top Sentence IDs: [7]\n",
      "Question ID: 645, Top Sentence IDs: [2]\n",
      "Question ID: 3708, Top Sentence IDs: [0]\n",
      "Question ID: 3291, Top Sentence IDs: [0]\n",
      "Question ID: 3646, Top Sentence IDs: [1]\n",
      "Question ID: 2882, Top Sentence IDs: [1]\n",
      "Question ID: 2578, Top Sentence IDs: [3]\n",
      "Question ID: 1680, Top Sentence IDs: [1]\n",
      "Question ID: 2619, Top Sentence IDs: [0]\n",
      "Question ID: 3391, Top Sentence IDs: [1]\n",
      "Question ID: 1610, Top Sentence IDs: [10]\n",
      "Question ID: 3038, Top Sentence IDs: [0]\n",
      "Question ID: 3946, Top Sentence IDs: [1]\n",
      "Question ID: 3775, Top Sentence IDs: [0]\n",
      "Question ID: 3384, Top Sentence IDs: [0]\n",
      "Question ID: 2413, Top Sentence IDs: [1]\n",
      "Question ID: 1136, Top Sentence IDs: [0]\n",
      "Question ID: 1756, Top Sentence IDs: [3]\n",
      "Question ID: 3601, Top Sentence IDs: [2]\n",
      "Question ID: 3857, Top Sentence IDs: [5]\n",
      "Question ID: 857, Top Sentence IDs: [2]\n",
      "Question ID: 2099, Top Sentence IDs: [0]\n",
      "Question ID: 2768, Top Sentence IDs: [2]\n",
      "Question ID: 561, Top Sentence IDs: [6]\n",
      "Question ID: 3865, Top Sentence IDs: [0]\n",
      "Question ID: 2108, Top Sentence IDs: [1]\n",
      "Question ID: 3750, Top Sentence IDs: [1]\n",
      "Question ID: 2799, Top Sentence IDs: [2]\n",
      "Question ID: 2614, Top Sentence IDs: [5]\n",
      "Question ID: 1897, Top Sentence IDs: [11]\n",
      "Question ID: 722, Top Sentence IDs: [1]\n",
      "Question ID: 795, Top Sentence IDs: [0]\n",
      "Question ID: 4168, Top Sentence IDs: [0]\n",
      "Question ID: 391, Top Sentence IDs: [1]\n",
      "Question ID: 728, Top Sentence IDs: [9]\n",
      "Question ID: 2344, Top Sentence IDs: [2]\n",
      "Question ID: 1819, Top Sentence IDs: [4]\n",
      "Question ID: 311, Top Sentence IDs: [1]\n",
      "Question ID: 2849, Top Sentence IDs: [0]\n",
      "Question ID: 1673, Top Sentence IDs: [5]\n",
      "Question ID: 328, Top Sentence IDs: [3]\n",
      "Question ID: 2831, Top Sentence IDs: [1]\n",
      "Question ID: 3605, Top Sentence IDs: [1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#defining the nn_summariser function\n",
    "def nn_summariser(csvfile, questionids, n=1):\n",
    "    df = pd.read_csv(csvfile)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    results = []\n",
    "    for qid in questionids:\n",
    "        sub_df = df[df['qid'] == qid]\n",
    "        question = sub_df['question'].iloc[0]\n",
    "        sentences = sub_df['sentence text'].tolist()\n",
    "        sent_ids = sub_df['sentid'].tolist()\n",
    "\n",
    "        #preparing inputs\n",
    "        pairs = [f\"[CLS] {question} [SEP] {sent} [SEP]\" for sent in sentences]\n",
    "        inputs = tokenizer(pairs, return_tensors='tf', max_length=128, padding=True, truncation=True)\n",
    "\n",
    "        #predicting scores\n",
    "        predictions = transformer_model.predict(inputs['input_ids'])\n",
    "        scores = predictions.flatten()\n",
    "\n",
    "        #getting top n sentences\n",
    "        top_n_indices = np.argsort(scores)[-n:]\n",
    "        top_n_ids = [sent_ids[i] for i in top_n_indices]\n",
    "        results.append(top_n_ids)\n",
    "\n",
    "    return results\n",
    "\n",
    "#reporting final results using the test set\n",
    "test_question_ids = test_df['qid'].unique()\n",
    "test_results = nn_summariser('test.csv', test_question_ids, n=1)\n",
    "\n",
    "#printing the test results\n",
    "for qid, top_ids in zip(test_question_ids, test_results):\n",
    "    print(f\"Question ID: {qid}, Top Sentence IDs: {top_ids}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDs of the  n  sentences that have the highest prediction score in the given question are printed in the cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of this model with the previous models\n",
    "\n",
    "as stated in the comparison of the first two models before, more complexity is not equal to better performance and when having an inadequate amount of data with imbalance label distribution it could lead to worse performance and results, hence this model is performing worse than the two before."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "a7b63e7410c98f344f02082f10d790581d1dba1eeb1c8fe30f342f6109f0429e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0444416527884e00b90c86c101b091f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "054d4181c75c4f2591bade77c59d85bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3425e611797246c68f89a6814c2530c8",
      "placeholder": "​",
      "style": "IPY_MODEL_ee2e186046ba4e60bfda72ca591d1eda",
      "value": " 48.0/48.0 [00:00&lt;00:00, 3.04kB/s]"
     }
    },
    "0db4a2a2bf094dee9e002e9dae147309": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94601e97d81a4e3da9542a9a6c72c2e8",
      "placeholder": "​",
      "style": "IPY_MODEL_e3c622f85d644a739152f3853e253e23",
      "value": "tokenizer.json: 100%"
     }
    },
    "0ef74e3985a942668a3c44169ef85028": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f27be381960045d1aedbcfdc485154f0",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9faf922e288d4fe5b641aa4b141774ca",
      "value": 570
     }
    },
    "1c68634d238e46008570190a02665c9e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2dba21bb5a9946528c5d69e6eb1b1e1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0db4a2a2bf094dee9e002e9dae147309",
       "IPY_MODEL_f2a91b41d5fa4fe190a67eedf9727d4b",
       "IPY_MODEL_cceb9b7b02dd47bd9079f190981be812"
      ],
      "layout": "IPY_MODEL_3d893880073446cbba37ffea1d977d0b"
     }
    },
    "3425e611797246c68f89a6814c2530c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3abba6091c2d41d0a13fe90e7f3f4856": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d893880073446cbba37ffea1d977d0b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e1c79df1c0e46a48320c55a9f6cee2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4248a9a715054851afa1e60459e98245": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48eb4c6fe1a04a8e9a710941d15bfc4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4248a9a715054851afa1e60459e98245",
      "placeholder": "​",
      "style": "IPY_MODEL_6f90cbff14bd45249cec792e62e8ec2f",
      "value": "config.json: 100%"
     }
    },
    "4b8569d2771a424eb3719913cfcabb1b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d12dfae5e1a4763b97177b357910fa2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_56c66a0babf640fd8154a57117900b35",
       "IPY_MODEL_d6bb559db51c4b7cae4bbf614770a998",
       "IPY_MODEL_054d4181c75c4f2591bade77c59d85bc"
      ],
      "layout": "IPY_MODEL_75cae6e71fe7401ca6bd7ad2a7f29898"
     }
    },
    "4e4f46138cfd48e281666c4e985e5c8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d88c0750a3a84736a6563020b29748e2",
       "IPY_MODEL_b2a12af650824dd2b766663d0c041613",
       "IPY_MODEL_fe14f2e7f80f4180b36e776bed173624"
      ],
      "layout": "IPY_MODEL_d599664c058b4bf99f1b55c3effe9a6d"
     }
    },
    "53970f77d3ac4cbaa7e3053350fcaf47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56c66a0babf640fd8154a57117900b35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3abba6091c2d41d0a13fe90e7f3f4856",
      "placeholder": "​",
      "style": "IPY_MODEL_b88d5ca5a93e4347bd2478daa29eaf57",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "5792530fe37d4b98b0086ca7615de3d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a02fb90a9a7e4cb28054ade611e0bd0a",
       "IPY_MODEL_bc1c255da4da427b9dde13d03b521723",
       "IPY_MODEL_bef88ee8751742e09428cd4e5eda2858"
      ],
      "layout": "IPY_MODEL_5b9a6e8edaa64eabb7a52531af050057"
     }
    },
    "5b9a6e8edaa64eabb7a52531af050057": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6290aa3c8fd54e31a4882f310fa79a13": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "677c9336bd474401b93901b9ce441783": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6f90cbff14bd45249cec792e62e8ec2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "75cae6e71fe7401ca6bd7ad2a7f29898": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77ad1a43314c4764b692318319240771": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "85f0cb8c7e6e4caa90e71387e92c6eae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "870adf7941e6482e98e99cc2f8fe9959": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f36601979613464cbceffae0377b7c3c",
      "placeholder": "​",
      "style": "IPY_MODEL_c63ec2bd9bae4ec79b301cb3cc5a1aa9",
      "value": " 570/570 [00:00&lt;00:00, 34.7kB/s]"
     }
    },
    "90cce431b57340da868ac01871b0562e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "94601e97d81a4e3da9542a9a6c72c2e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e3155e8e2d148439d3beb067185fb15": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9faf922e288d4fe5b641aa4b141774ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a02fb90a9a7e4cb28054ade611e0bd0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e3155e8e2d148439d3beb067185fb15",
      "placeholder": "​",
      "style": "IPY_MODEL_85f0cb8c7e6e4caa90e71387e92c6eae",
      "value": "vocab.txt: 100%"
     }
    },
    "a2f439cebf784a53945df264ec06de36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "af4622f7bf2e480b98e64a7a4617d8c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2a12af650824dd2b766663d0c041613": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c68634d238e46008570190a02665c9e",
      "max": 440449768,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_677c9336bd474401b93901b9ce441783",
      "value": 440449768
     }
    },
    "b3682ea32ea94578bf96dfd70ccb9698": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b88d5ca5a93e4347bd2478daa29eaf57": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bc1c255da4da427b9dde13d03b521723": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b8569d2771a424eb3719913cfcabb1b",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a2f439cebf784a53945df264ec06de36",
      "value": 231508
     }
    },
    "bef88ee8751742e09428cd4e5eda2858": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af4622f7bf2e480b98e64a7a4617d8c4",
      "placeholder": "​",
      "style": "IPY_MODEL_3e1c79df1c0e46a48320c55a9f6cee2d",
      "value": " 232k/232k [00:00&lt;00:00, 5.19MB/s]"
     }
    },
    "c63ec2bd9bae4ec79b301cb3cc5a1aa9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c8f5f6b22eda41ffa9ee7127b1be8242": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cceb9b7b02dd47bd9079f190981be812": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_53970f77d3ac4cbaa7e3053350fcaf47",
      "placeholder": "​",
      "style": "IPY_MODEL_d6a8e0b2db0c4de88bb3f6e4e15e0a4f",
      "value": " 466k/466k [00:00&lt;00:00, 6.87MB/s]"
     }
    },
    "d599664c058b4bf99f1b55c3effe9a6d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6a8e0b2db0c4de88bb3f6e4e15e0a4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d6bb559db51c4b7cae4bbf614770a998": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0444416527884e00b90c86c101b091f5",
      "max": 48,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6290aa3c8fd54e31a4882f310fa79a13",
      "value": 48
     }
    },
    "d88c0750a3a84736a6563020b29748e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8f5f6b22eda41ffa9ee7127b1be8242",
      "placeholder": "​",
      "style": "IPY_MODEL_f10c0682865f4d88b0e1c6372b1044b6",
      "value": "model.safetensors: 100%"
     }
    },
    "e3c622f85d644a739152f3853e253e23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e9ea0cf2949a444a95e3621c44bee6e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee02368a4fe84bd89dce8d849e6337ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_48eb4c6fe1a04a8e9a710941d15bfc4b",
       "IPY_MODEL_0ef74e3985a942668a3c44169ef85028",
       "IPY_MODEL_870adf7941e6482e98e99cc2f8fe9959"
      ],
      "layout": "IPY_MODEL_fa56be3896e442b6acdd09efdee53311"
     }
    },
    "ee2e186046ba4e60bfda72ca591d1eda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f10c0682865f4d88b0e1c6372b1044b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f27be381960045d1aedbcfdc485154f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2a91b41d5fa4fe190a67eedf9727d4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9ea0cf2949a444a95e3621c44bee6e6",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_90cce431b57340da868ac01871b0562e",
      "value": 466062
     }
    },
    "f36601979613464cbceffae0377b7c3c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa56be3896e442b6acdd09efdee53311": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe14f2e7f80f4180b36e776bed173624": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3682ea32ea94578bf96dfd70ccb9698",
      "placeholder": "​",
      "style": "IPY_MODEL_77ad1a43314c4764b692318319240771",
      "value": " 440M/440M [00:05&lt;00:00, 35.0MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
